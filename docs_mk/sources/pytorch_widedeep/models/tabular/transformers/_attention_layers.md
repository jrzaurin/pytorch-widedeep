#


## AdditiveAttention
[source](https://github.com/jrzaurin/pytorch-widedeep/blob/master/pytorch_widedeep/models/tabular/transformers/_attention_layers.py/#L178)
```python 
AdditiveAttention(
   input_dim: int, n_heads: int, use_bias: bool, dropout: float,
   share_qv_weights: bool
)
```




**Methods:**


### .__init__
[source](https://github.com/jrzaurin/pytorch-widedeep/blob/master/pytorch_widedeep/models/tabular/transformers/_attention_layers.py/#L179)
```python
.__init__(
   input_dim: int, n_heads: int, use_bias: bool, dropout: float,
   share_qv_weights: bool
)
```


### .forward
[source](https://github.com/jrzaurin/pytorch-widedeep/blob/master/pytorch_widedeep/models/tabular/transformers/_attention_layers.py/#L211)
```python
.forward(
   X: Tensor
)
```

