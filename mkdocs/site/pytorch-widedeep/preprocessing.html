
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch-widedeep.readthedocs.io/pytorch-widedeep/preprocessing.html">
      
      
        <link rel="prev" href="utils/text_utils.html">
      
      
        <link rel="next" href="load_from_folder.html">
      
      
      <link rel="icon" href="../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.35">
    
    
      
        <title>Preprocessing - pytorch_widedeep</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.35f28582.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-preprocessing-module" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="pytorch_widedeep" class="md-header__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            pytorch_widedeep
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Preprocessing
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../installation.html" class="md-tabs__link">
        
  
    
  
  Installation

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../quick_start.html" class="md-tabs__link">
        
  
    
  
  Quick Start

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="utils/index.html" class="md-tabs__link">
          
  
    
  
  Pytorch-widedeep

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../examples/01_preprocessors_and_utils.html" class="md-tabs__link">
          
  
    
  
  Examples

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../contributing.html" class="md-tabs__link">
        
  
    
  
  Contributing

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="pytorch_widedeep" class="md-nav__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    pytorch_widedeep
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quick_start.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Pytorch-widedeep
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Pytorch-widedeep
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="utils/index.html" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/deeptabular_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deeptabular utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/fastai_transforms.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fastai transforms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/image_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/text_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Preprocessing
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="preprocessing.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Preprocessing
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      WidePreprocessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      TabPreprocessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.tab_preprocessor.Quantizer" class="md-nav__link">
    <span class="md-ellipsis">
      Quantizer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      TextPreprocessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      HFPreprocessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      ImagePreprocessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.din_preprocessor.DINPreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      DINPreprocessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chunked-versions" class="md-nav__link">
    <span class="md-ellipsis">
      Chunked versions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      ChunkWidePreprocessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      ChunkTabPreprocessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.text_preprocessor.ChunkTextPreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      ChunkTextPreprocessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.preprocessing.hf_preprocessor.ChunkHFPreprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      ChunkHFPreprocessor
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="load_from_folder.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Load From Folder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="model_components.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Components
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="the_rec_module.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Rec Module
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bayesian models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="losses.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Losses
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Metrics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="dataloaders.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataloaders
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="callbacks.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="trainer.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_trainer.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bayesian Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="self_supervised_pretraining.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Self Supervised Pretraining
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="tab2vec.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tab2Vec
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/01_preprocessors_and_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01_preprocessors_and_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/02_model_components.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    02_model_components
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/03_binary_classification_with_defaults.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    03_binary_classification_with_defaults
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/04_regression_with_images_and_text.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    04_regression_with_images_and_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/05_save_and_load_model_and_artifacts.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    05_save_and_load_model_and_artifacts
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/06_finetune_and_warmup.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06_finetune_and_warmup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/07_custom_components.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    07_custom_components
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/08_custom_dataLoader_imbalanced_dataset.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    08_custom_dataLoader_imbalanced_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/09_extracting_embeddings.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    09_extracting_embeddings
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/10_3rd_party_integration-RayTune_WnB.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10_3rd_party_integration-RayTune_WnB
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/11_auc_multiclass.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11_auc_multiclass
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/12_ZILNLoss_origkeras_vs_pytorch_widedeep.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12_ZILNLoss_origkeras_vs_pytorch_widedeep
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/13_model_uncertainty_prediction.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13_model_uncertainty_prediction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/14_bayesian_models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14_bayesian_models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/16_Self_Supervised_Pretraning_pt1.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15_Self-Supervised Pre-Training pt 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/16_Self_Supervised_Pretraning_pt2.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15_Self-Supervised Pre-Training pt 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/17_Usign_a_custom_hugging_face_model.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16_Usign-a-custom-hugging-face-model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/18_feature_importance_via_attention_weights.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17_feature_importance_via_attention_weights
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/19_wide_and_deep_for_recsys_pt1.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18_wide_and_deep_for_recsys_pt1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/19_wide_and_deep_for_recsys_pt2.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18_wide_and_deep_for_recsys_pt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/20_load_from_folder_functionality.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    19_load_from_folder_functionality
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/21_Using_huggingface_within_widedeep.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    20-Using-huggingface-within-widedeep
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="the-preprocessing-module">The <code>preprocessing</code> module<a class="headerlink" href="#the-preprocessing-module" title="Permanent link">&para;</a></h1>
<p>This module contains the classes that are used to prepare the data before
being passed to the models. There is one Preprocessor per data mode or model
component (<code>wide</code>, <code>deeptabular</code>, <code>deepimage</code> and <code>deeptext</code>) with
the exception of the <code>deeptext</code> component. In this case, two processors are
available: one for the case when no Hugging Face model is used
(<code>TextPreprocessor</code>) and another one when a Hugging Face model is used
(<code>HFPreprocessor</code>).</p>


<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">WidePreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">WidePreprocessor</span><span class="p">(</span><span class="n">wide_cols</span><span class="p">,</span> <span class="n">crossed_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.preprocessing.base_preprocessor.BasePreprocessor">BasePreprocessor</span></code></p>


        <p>Preprocessor to prepare the wide input dataset</p>
<p>This Preprocessor prepares the data for the wide, linear component.
This linear model is implemented via an Embedding layer that is
connected to the output neuron. <code>WidePreprocessor</code> numerically
encodes all the unique values of all categorical columns <code>wide_cols +
crossed_cols</code>. See the Example below.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>wide_cols</code></b>
              (<code><span title="typing.List">List</span>[str]</code>)
          –
          <div class="doc-md-description">
            <p>List of strings with the name of the columns that will label
encoded and passed through the <code>wide</code> component</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>crossed_cols</code></b>
              (<code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="typing.Tuple">Tuple</span>[str, str]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the name of the columns that will be <code>'crossed'</code>
and then label encoded. e.g. <em>[('education', 'occupation'), ...]</em>. For
binary features, a cross-product transformation is 1 if and only if
the constituent features are all 1, and 0 otherwise.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.wide_crossed_cols">wide_crossed_cols</span></code></b>
              (<code><span title="typing.List">List</span></code>)
          –
          <div class="doc-md-description">
            <p>List with the names of all columns that will be label encoded</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.encoding_dict">encoding_dict</span></code></b>
              (<code>Dict</code>)
          –
          <div class="doc-md-description">
            <p>Dictionary where the keys are the result of pasting <code>colname + '_' +
column value</code> and the values are the corresponding mapped integer.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.inverse_encoding_dict">inverse_encoding_dict</span></code></b>
              (<code>Dict</code>)
          –
          <div class="doc-md-description">
            <p>the inverse encoding dictionary</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.wide_dim">wide_dim</span></code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Dimension of the wide model (i.e. dim of the linear layer)</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">WidePreprocessor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;l&#39;</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">crossed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;color&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide_preprocessor</span> <span class="o">=</span> <span class="n">WidePreprocessor</span><span class="p">(</span><span class="n">wide_cols</span><span class="o">=</span><span class="n">wide_cols</span><span class="p">,</span> <span class="n">crossed_cols</span><span class="o">=</span><span class="n">crossed_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_wide</span> <span class="o">=</span> <span class="n">wide_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_wide</span>
<span class="go">array([[1, 4],</span>
<span class="go">       [2, 5],</span>
<span class="go">       [3, 6]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide_preprocessor</span><span class="o">.</span><span class="n">encoding_dict</span>
<span class="go">{&#39;color_r&#39;: 1, &#39;color_b&#39;: 2, &#39;color_g&#39;: 3, &#39;color_size_r-s&#39;: 4, &#39;color_size_b-n&#39;: 5, &#39;color_size_g-l&#39;: 6}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide_preprocessor</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_wide</span><span class="p">)</span>
<span class="go">  color color_size</span>
<span class="go">0     r        r-s</span>
<span class="go">1     b        b-n</span>
<span class="go">2     g        g-l</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/wide_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">wide_cols</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">crossed_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">WidePreprocessor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">wide_cols</span> <span class="o">=</span> <span class="n">wide_cols</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">crossed_cols</span> <span class="o">=</span> <span class="n">crossed_cols</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.fit" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit</span>


<a href="#pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Fits the Preprocessor and creates required attributes</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor" href="#pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor">WidePreprocessor</a></code>
          –
          <div class="doc-md-description">
            <p><code>WidePreprocessor</code> fitted object</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/wide_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;WidePreprocessor&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Fits the Preprocessor and creates required attributes</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    WidePreprocessor</span>
<span class="sd">        `WidePreprocessor` fitted object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">df_wide</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_wide</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wide_crossed_cols</span> <span class="o">=</span> <span class="n">df_wide</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">glob_feature_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_global_feature_list</span><span class="p">(</span>
        <span class="n">df_wide</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wide_crossed_cols</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># leave 0 for padding/&quot;unseen&quot; categories</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">glob_feature_list</span><span class="p">)}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wide_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoding_dict</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inverse_encoding_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inverse_encoding_dict</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;unseen&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">transform</span>


<a href="#pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.ndarray">ndarray</span></code>
          –
          <div class="doc-md-description">
            <p>transformed input dataframe</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/wide_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        transformed input dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attributes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;encoding_dict&quot;</span><span class="p">])</span>
    <span class="n">df_wide</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_wide</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">df_wide</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wide_crossed_cols</span><span class="p">)])</span>
    <span class="k">for</span> <span class="n">col_i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wide_crossed_cols</span><span class="p">):</span>
        <span class="n">encoded</span><span class="p">[:,</span> <span class="n">col_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_wide</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dict</span><span class="p">[</span><span class="n">col</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>
                <span class="k">if</span> <span class="n">col</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dict</span>
                <span class="k">else</span> <span class="mi">0</span>
            <span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">encoded</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.inverse_transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">inverse_transform</span>


<a href="#pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.inverse_transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">inverse_transform</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Takes as input the output from the <code>transform</code> method and it will
return the original values.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>encoded</code></b>
              (<code><span title="numpy.ndarray">ndarray</span></code>)
          –
          <div class="doc-md-description">
            <p>numpy array with the encoded values that are the output from the
<code>transform</code> method</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="pandas.DataFrame">DataFrame</span></code>
          –
          <div class="doc-md-description">
            <p>Pandas dataframe with the original values</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/wide_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoded</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Takes as input the output from the `transform` method and it will</span>
<span class="sd">    return the original values.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    encoded: np.ndarray</span>
<span class="sd">        numpy array with the encoded values that are the output from the</span>
<span class="sd">        `transform` method</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pd.DataFrame</span>
<span class="sd">        Pandas dataframe with the original values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">wide_crossed_cols</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pd</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&gt;=</span> <span class="s2">&quot;2.1.0&quot;</span><span class="p">:</span>
        <span class="n">decoded</span> <span class="o">=</span> <span class="n">decoded</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_encoding_dict</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">decoded</span> <span class="o">=</span> <span class="n">decoded</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_encoding_dict</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">decoded</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">rm_str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">col</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">])</span>
        <span class="n">decoded</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoded</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">rm_str</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">decoded</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.fit_transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit_transform</span>


<a href="#pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor.fit_transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Combines <code>fit</code> and <code>transform</code></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.ndarray">ndarray</span></code>
          –
          <div class="doc-md-description">
            <p>transformed input dataframe</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/wide_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Combines `fit` and `transform`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        transformed input dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabPreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabPreprocessor</span><span class="p">(</span>
    <span class="n">cat_embed_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cols_to_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">auto_embed_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">embedding_rule</span><span class="o">=</span><span class="s2">&quot;fastai_new&quot;</span><span class="p">,</span>
    <span class="n">default_embed_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">with_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">already_standard</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.preprocessing.base_preprocessor.BasePreprocessor">BasePreprocessor</span></code></p>


        <p>Preprocessor to prepare the <code>deeptabular</code> component input dataset</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str], <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List containing the name of the categorical columns that will be
represented by embeddings (e.g. <em>['education', 'relationship', ...]</em>) or
a Tuple with the name and the embedding dimension (e.g.: <em>[
('education',32), ('relationship',16), ...]</em>)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the continuous cols</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Continuous columns can be turned into categorical via <code>pd.cut</code>. If
<code>quantization_setup</code> is an <code>int</code>, all continuous columns will be
quantized using this value as the number of bins. Alternatively, a
dictionary where the keys are the column names to quantize and the
values are the either integers indicating the number of bins or a
list of scalars indicating the bin edges can also be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cols_to_scale</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str], str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the names of the columns that will be standarised via
sklearn's <code>StandardScaler</code>. It can also be the string <code>'all'</code> in
which case all the continuous cols will be scaled.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>auto_embed_dim</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the embedding dimensions will be
automatically defined via rule of thumb. See <code>embedding_rule</code>
below.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embedding_rule</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[google, fastai_old, fastai_new]</code>, default:
                  <code>&#39;fastai_new&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>auto_embed_dim=True</code>, this is the choice of embedding rule of
thumb. Choices are:</p>
<ul>
<li>
<p><em>fastai_new</em>: <span class="arithmatex">\(min(600, round(1.6 \times n_{cat}^{0.56}))\)</span></p>
</li>
<li>
<p><em>fastai_old</em>: <span class="arithmatex">\(min(50, (n_{cat}//{2})+1)\)</span></p>
</li>
<li>
<p><em>google</em>: <span class="arithmatex">\(min(600, round(n_{cat}^{0.24}))\)</span></p>
</li>
</ul>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>default_embed_dim</code></b>
              (<code>int</code>, default:
                  <code>16</code>
)
          –
          <div class="doc-md-description">
            <p>Dimension for the embeddings if the embedding dimension is not
provided in the <code>cat_embed_cols</code> parameter and <code>auto_embed_dim</code> is
set to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>with_attention</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the preprocessed data will be passed to an
attention-based model (more precisely a model where all embeddings
must have the same dimensions). If <code>True</code>, the param <code>cat_embed_cols</code>
must just be a list containing just the categorical column names:
e.g.
<em>['education', 'relationship', ...]</em>. This is because they will all be
 encoded using embeddings of the same dim, which will be specified
 later when the model is defined. <br/> Param alias:
 <code>for_transformer</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>with_cls_token</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if a <code>'[CLS]'</code> token will be added to the dataset
when using attention-based models. The final hidden state
corresponding to this token is used as the aggregated representation
for classification and regression tasks. If not, the categorical
and/or continuous embeddings will be concatenated before being passed
to the final MLP (if present).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>shared_embed</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the embeddings will be "shared" when using
attention-based models. The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>verbose</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>scale</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>note</strong>: this arg will be removed in upcoming
 releases. Please use <code>cols_to_scale</code> instead. <br/> Bool indicating
 whether or not to scale/standarise continuous cols. It is important
 to emphasize that all the DL models for tabular data in the library
 also include the possibility of normalising the input continuous
 features via a <code>BatchNorm</code> or a <code>LayerNorm</code>. <br/> Param alias:
 <code>scale_cont_cols</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>already_standard</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>note</strong>: this arg will be removed in upcoming
 releases. Please use <code>cols_to_scale</code> instead. <br/> List with the
 name of the continuous cols that do not need to be
 scaled/standarised.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Other Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>**kwargs</code></b>
          –
          <div class="doc-md-description">
            <p><code>pd.cut</code> and <code>StandardScaler</code> related args</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.embed_dim">embed_dim</span></code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span></code>)
          –
          <div class="doc-md-description">
            <p>Dictionary where keys are the embed cols and values are the embedding
dimensions. If <code>with_attention</code> is set to <code>True</code> this attribute
is not generated during the <code>fit</code> process</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.label_encoder">label_encoder</span></code></b>
              (<code><a class="autorefs autorefs-internal" title="pytorch_widedeep.utils.deeptabular_utils.LabelEncoder" href="utils/deeptabular_utils.html#pytorch_widedeep.utils.deeptabular_utils.LabelEncoder">LabelEncoder</a></code>)
          –
          <div class="doc-md-description">
            <p>see <code>pytorch_widedeep.utils.dense_utils.LabelEncder</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.cat_embed_input">cat_embed_input</span></code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span></code>)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name, number of individual values for
that column and, If <code>with_attention</code> is set to <code>False</code>, the
corresponding embeddings dim, e.g. <em>[('education', 16, 10),
('relationship', 6, 8), ...]</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.standardize_cols">standardize_cols</span></code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span></code>)
          –
          <div class="doc-md-description">
            <p>List of the columns that will be standarized</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.scaler">scaler</span></code></b>
              (<code><span title="sklearn.preprocessing.StandardScaler">StandardScaler</span></code>)
          –
          <div class="doc-md-description">
            <p>an instance of <code>sklearn.preprocessing.StandardScaler</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.column_idx">column_idx</span></code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span></code>)
          –
          <div class="doc-md-description">
            <p>Dictionary where keys are column names and values are column indexes.
This is neccesary to slice tensors</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.quantizer">quantizer</span></code></b>
              (<code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.tab_preprocessor.Quantizer" href="#pytorch_widedeep.preprocessing.tab_preprocessor.Quantizer">Quantizer</a></code>)
          –
          <div class="doc-md-description">
            <p>an instance of <code>Quantizer</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">TabPreprocessor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;l&#39;</span><span class="p">],</span> <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">55</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;color&#39;</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;size&#39;</span><span class="p">,</span><span class="mi">5</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cont_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deep_preprocessor</span> <span class="o">=</span> <span class="n">TabPreprocessor</span><span class="p">(</span><span class="n">cat_embed_cols</span><span class="o">=</span><span class="n">cat_embed_cols</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">cont_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">deep_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deep_preprocessor</span><span class="o">.</span><span class="n">cat_embed_cols</span>
<span class="go">[(&#39;color&#39;, 5), (&#39;size&#39;, 5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deep_preprocessor</span><span class="o">.</span><span class="n">column_idx</span>
<span class="go">{&#39;color&#39;: 0, &#39;size&#39;: 1, &#39;age&#39;: 2}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cont_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;col1&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="s2">&quot;col2&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cont_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;col1&quot;</span><span class="p">,</span> <span class="s2">&quot;col2&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tab_preprocessor</span> <span class="o">=</span> <span class="n">TabPreprocessor</span><span class="p">(</span><span class="n">continuous_cols</span><span class="o">=</span><span class="n">cont_cols</span><span class="p">,</span> <span class="n">quantization_setup</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ft_cont_df</span> <span class="o">=</span> <span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">cont_df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># or...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantization_setup</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;col1&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="s1">&#39;col2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tab_preprocessor2</span> <span class="o">=</span> <span class="n">TabPreprocessor</span><span class="p">(</span><span class="n">continuous_cols</span><span class="o">=</span><span class="n">cont_cols</span><span class="p">,</span> <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ft_cont_df2</span> <span class="o">=</span> <span class="n">tab_preprocessor2</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">cont_df</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/tab_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;with_attention&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;for_transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;for_matrix_factorization&quot;</span><span class="p">,</span> <span class="s2">&quot;for_mf&quot;</span><span class="p">])</span>
<span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;cat_embed_cols&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;embed_cols&quot;</span><span class="p">])</span>
<span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;scale_cont_cols&quot;</span><span class="p">])</span>
<span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;quantization_setup&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;cols_and_bins&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">cat_embed_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cols_to_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">auto_embed_dim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">embedding_rule</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;google&quot;</span><span class="p">,</span> <span class="s2">&quot;fastai_old&quot;</span><span class="p">,</span> <span class="s2">&quot;fastai_new&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;fastai_new&quot;</span><span class="p">,</span>
    <span class="n">default_embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">with_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">with_cls_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">already_standard</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabPreprocessor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="n">continuous_cols</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">quantization_setup</span> <span class="o">=</span> <span class="n">quantization_setup</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cols_to_scale</span> <span class="o">=</span> <span class="n">cols_to_scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">already_standard</span> <span class="o">=</span> <span class="n">already_standard</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">auto_embed_dim</span> <span class="o">=</span> <span class="n">auto_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding_rule</span> <span class="o">=</span> <span class="n">embedding_rule</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">default_embed_dim</span> <span class="o">=</span> <span class="n">default_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_attention</span> <span class="o">=</span> <span class="n">with_attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="n">with_cls_token</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">shared_embed</span> <span class="o">=</span> <span class="n">shared_embed</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">quant_args</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="o">.</span><span class="vm">__code__</span><span class="o">.</span><span class="n">co_varnames</span>
    <span class="p">}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale_args</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
    <span class="p">}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span><span class="n">cat_embed_cols</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">with_cls_token</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_cols</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="s2">&quot;cls_token&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">cat_embed_cols</span>  <span class="c1"># type: ignore[operator]</span>
            <span class="k">if</span> <span class="n">cat_embed_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="p">[</span><span class="s2">&quot;cls_token&quot;</span><span class="p">]</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_cols</span> <span class="o">=</span> <span class="n">cat_embed_cols</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.fit" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit</span>


<a href="#pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Fits the Preprocessor and creates required attributes</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor" href="#pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor">TabPreprocessor</a></code>
          –
          <div class="doc-md-description">
            <p><code>TabPreprocessor</code> fitted object</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/tab_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BasePreprocessor</span><span class="p">:</span>  <span class="c1"># noqa: C901</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fits the Preprocessor and creates required attributes</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    TabPreprocessor</span>
<span class="sd">        `TabPreprocessor` fitted object</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">df_adj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_insert_cls_token</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Categorical embeddings logic</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantization_setup</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">df_cat</span><span class="p">,</span> <span class="n">cat_embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_categorical</span><span class="p">(</span><span class="n">df_adj</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">(</span>
            <span class="n">columns_to_encode</span><span class="o">=</span><span class="n">df_cat</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
            <span class="n">shared_embed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">shared_embed</span><span class="p">,</span>
            <span class="n">with_attention</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">with_attention</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_cat</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">encoding_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_attention</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_input</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_input</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">cat_embed_dim</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">column_idx</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_cat</span><span class="o">.</span><span class="n">columns</span><span class="p">)})</span>

    <span class="c1"># Continuous columns logic</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">df_cont</span><span class="p">,</span> <span class="n">cont_embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_continuous</span><span class="p">(</span><span class="n">df_adj</span><span class="p">)</span>

        <span class="c1"># Standardization logic</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">standardize_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_args</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">df_cont</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">standardize_cols</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Continuous columns will not be normalised&quot;</span><span class="p">)</span>

        <span class="c1"># Quantization logic</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cols_and_bins</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># we do not run &#39;Quantizer.fit&#39; here since in the wild case</span>
            <span class="c1"># someone wants standardization and quantization for the same</span>
            <span class="c1"># columns, the Quantizer will run on the scaled data</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cols_and_bins</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">quant_args</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_attention</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">n_cat</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">cont_embed_dim</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_input</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">col</span><span class="p">,</span> <span class="n">n_cat</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_input</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">cont_embed_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">column_idx</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">column_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_cont</span><span class="p">)}</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">transform</span>


<a href="#pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Returns the processed <code>dataframe</code> as a np.ndarray</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.ndarray">ndarray</span></code>
          –
          <div class="doc-md-description">
            <p>transformed input dataframe</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/tab_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>  <span class="c1"># noqa: C901</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the processed `dataframe` as a np.ndarray</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        transformed input dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">condition</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span><span class="p">)</span>

    <span class="n">df_adj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_insert_cls_token</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">df_cat</span> <span class="o">=</span> <span class="n">df_adj</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cat_cols</span><span class="p">]</span>
        <span class="n">df_cat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_cat</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">df_cont</span> <span class="o">=</span> <span class="n">df_adj</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">continuous_cols</span><span class="p">]</span>
        <span class="c1"># Standardization logic</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">standardize_cols</span><span class="p">:</span>
            <span class="n">df_cont</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">standardize_cols</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span>
                <span class="n">df_cont</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">standardize_cols</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
            <span class="p">)</span>
        <span class="c1"># Quantization logic</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cols_and_bins</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Adjustment so I don&#39;t have to override the method</span>
            <span class="c1"># in &#39;ChunkTabPreprocessor&#39;</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantizer</span><span class="o">.</span><span class="n">is_fitted</span><span class="p">:</span>
                <span class="n">df_cont</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_cont</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">df_cont</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_cont</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">df_deep</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_cat</span><span class="p">,</span> <span class="n">df_cont</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">df_deep</span> <span class="o">=</span> <span class="n">df_cat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
            <span class="n">df_deep</span> <span class="o">=</span> <span class="n">df_cont</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">df_deep</span><span class="o">.</span><span class="n">values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.inverse_transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">inverse_transform</span>


<a href="#pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.inverse_transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">inverse_transform</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Takes as input the output from the <code>transform</code> method and it will
return the original values.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>encoded</code></b>
              (<code><span title="numpy.ndarray">ndarray</span></code>)
          –
          <div class="doc-md-description">
            <p>array with the output of the <code>transform</code> method</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="pandas.DataFrame">DataFrame</span></code>
          –
          <div class="doc-md-description">
            <p>Pandas dataframe with the original values</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/tab_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoded</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>  <span class="c1"># noqa: C901</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Takes as input the output from the `transform` method and it will</span>
<span class="sd">    return the original values.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    encoded: np.ndarray</span>
<span class="sd">        array with the output of the `transform` method</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pd.DataFrame</span>
<span class="sd">        Pandas dataframe with the original values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">column_idx</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="c1"># embeddings back to original category</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># quantized cols to the mid point</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cols_and_bins</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;Note that quantized cols will be turned into the mid point of &quot;</span>
                    <span class="s2">&quot;the corresponding bin&quot;</span>
                <span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantizer</span><span class="o">.</span><span class="n">inversed_bins</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">decoded</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoded</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="c1"># continuous_cols back to non-standarised</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">decoded</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">standardize_cols</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span>
                <span class="n">decoded</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">standardize_cols</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  <span class="c1"># KeyError:</span>
            <span class="k">pass</span>

    <span class="k">if</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">decoded</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">decoded</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;cls_token&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">decoded</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.fit_transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit_transform</span>


<a href="#pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor.fit_transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Combines <code>fit</code> and <code>transform</code></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.ndarray">ndarray</span></code>
          –
          <div class="doc-md-description">
            <p>transformed input dataframe</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/tab_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Combines `fit` and `transform`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        transformed input dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.tab_preprocessor.Quantizer" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Quantizer</span>


<a href="#pytorch_widedeep.preprocessing.tab_preprocessor.Quantizer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">Quantizer</span><span class="p">(</span><span class="n">quantization_setup</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">


        <p>Helper class to perform the quantization of continuous columns. It is
included in this docs for completion, since depending on the value of the
parameter <code>'quantization_setup'</code> of the <code>TabPreprocessor</code> class, that
class might have an attribute of type <code>Quantizer</code>. However, this class is
designed to always run internally within the <code>TabPreprocessor</code> class.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>)
          –
          <div class="doc-md-description">
            <p>Dictionary where the keys are the column names to quantize and the
values are the either integers indicating the number of bins or a
list of scalars indicating the bin edges.</p>
          </div>
        </li>
    </ul>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/tab_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">quantization_setup</span> <span class="o">=</span> <span class="n">quantization_setup</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">quant_args</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextPreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TextPreprocessor</span><span class="p">(</span>
    <span class="n">text_col</span><span class="p">,</span>
    <span class="n">max_vocab</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span>
    <span class="n">min_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">maxlen</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
    <span class="n">pad_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">pad_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">already_processed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">word_vectors_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_cpus</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.preprocessing.base_preprocessor.BasePreprocessor">BasePreprocessor</span></code></p>


        <p>Preprocessor to prepare the <code>deeptext</code> input dataset</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>text_col</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>column in the input dataframe containing the texts</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>max_vocab</code></b>
              (<code>int</code>, default:
                  <code>30000</code>
)
          –
          <div class="doc-md-description">
            <p>Maximum number of tokens in the vocabulary</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>min_freq</code></b>
              (<code>int</code>, default:
                  <code>5</code>
)
          –
          <div class="doc-md-description">
            <p>Minimum frequency for a token to be part of the vocabulary</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>maxlen</code></b>
              (<code>int</code>, default:
                  <code>80</code>
)
          –
          <div class="doc-md-description">
            <p>Maximum length of the tokenized sequences</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>pad_first</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Indicates whether the padding index will be added at the beginning or the
end of the sequences</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>pad_idx</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>padding index. Fastai's Tokenizer leaves 0 for the 'unknown' token.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>already_processed</code></b>
              (<code><span title="typing.Optional">Optional</span>[bool]</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the sequence of elements is already processed or
prepared. If this is the case, this Preprocessor will simply tokenize
and pad the sequence. <br/></p>
<div class="highlight"><pre><span></span><code>Param aliases: `not_text`. &lt;br/&gt;
</code></pre></div>
<p>This parameter is thought for those cases where the input sequences
are already fully processed or are directly not text (e.g. IDs)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>word_vectors_path</code></b>
              (<code><span title="typing.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Path to the pretrained word vectors</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_cpus</code></b>
              (<code><span title="typing.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>number of CPUs to used during the tokenization process</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>verbose</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>Enable verbose output.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.vocab">vocab</span></code></b>
              (<code><a class="autorefs autorefs-internal" title="pytorch_widedeep.utils.fastai_transforms.Vocab" href="utils/fastai_transforms.html#pytorch_widedeep.utils.fastai_transforms.Vocab">Vocab</a></code>)
          –
          <div class="doc-md-description">
            <p>an instance of <code>pytorch_widedeep.utils.fastai_transforms.Vocab</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.embedding_matrix">embedding_matrix</span></code></b>
              (<code><span title="numpy.ndarray">ndarray</span></code>)
          –
          <div class="doc-md-description">
            <p>Array with the pretrained embeddings</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">TextPreprocessor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text_column&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;life is like a box of chocolates&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="s2">&quot;You never know what you&#39;re gonna get&quot;</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_preprocessor</span> <span class="o">=</span> <span class="n">TextPreprocessor</span><span class="p">(</span><span class="n">text_col</span><span class="o">=</span><span class="s1">&#39;text_column&#39;</span><span class="p">,</span> <span class="n">max_vocab</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
<span class="go">The vocabulary contains 24 tokens</span>
<span class="go">array([[ 1,  1,  1,  1, 10, 11, 12, 13, 14, 15],</span>
<span class="go">       [ 5,  9, 16, 17, 18,  9, 19, 20, 21, 22]], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_te</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text_column&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;you never know what is in the box&#39;</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_te</span><span class="p">)</span>
<span class="go">array([[ 1,  1,  9, 16, 17, 18, 11,  0,  0, 13]], dtype=int32)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/text_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;already_processed&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;not_text&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">text_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">max_vocab</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">30000</span><span class="p">,</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">maxlen</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span>
    <span class="n">pad_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">pad_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">already_processed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">word_vectors_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_cpus</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TextPreprocessor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">text_col</span> <span class="o">=</span> <span class="n">text_col</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_vocab</span> <span class="o">=</span> <span class="n">max_vocab</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_freq</span> <span class="o">=</span> <span class="n">min_freq</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">maxlen</span> <span class="o">=</span> <span class="n">maxlen</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_first</span> <span class="o">=</span> <span class="n">pad_first</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_idx</span> <span class="o">=</span> <span class="n">pad_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">already_processed</span> <span class="o">=</span> <span class="n">already_processed</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_vectors_path</span> <span class="o">=</span> <span class="n">word_vectors_path</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cpus</span> <span class="o">=</span> <span class="n">n_cpus</span> <span class="k">if</span> <span class="n">n_cpus</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.fit" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit</span>


<a href="#pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Builds the vocabulary</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor" href="#pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor">TextPreprocessor</a></code>
          –
          <div class="doc-md-description">
            <p><code>TextPreprocessor</code> fitted object</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/text_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BasePreprocessor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Builds the vocabulary</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    TextPreprocessor</span>
<span class="sd">        `TextPreprocessor` fitted object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_texts</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="n">get_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">already_processed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cpus</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span> <span class="n">TVocab</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span>
        <span class="n">max_vocab</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_vocab</span><span class="p">,</span>
        <span class="n">min_freq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_freq</span><span class="p">,</span>
        <span class="n">pad_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_idx</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">tokens</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The vocabulary contains </span><span class="si">{}</span><span class="s2"> tokens&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">)))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_vectors_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">build_embeddings_matrix</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_vectors_path</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_freq</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">transform</span>


<a href="#pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Returns the padded, <em>'numericalised'</em> sequences</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.ndarray">ndarray</span></code>
          –
          <div class="doc-md-description">
            <p>Padded, <em>'numericalised'</em> sequences</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/text_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the padded, _&#39;numericalised&#39;_ sequences</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        Padded, _&#39;numericalised&#39;_ sequences</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attributes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;vocab&quot;</span><span class="p">])</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_texts</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">get_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">already_processed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cpus</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_sequences</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.transform_sample" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">transform_sample</span>


<a href="#pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.transform_sample" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">transform_sample</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Returns the padded, <em>'numericalised'</em> sequence</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>text</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>text to be tokenized and padded</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.ndarray">ndarray</span></code>
          –
          <div class="doc-md-description">
            <p>Padded, <em>'numericalised'</em> sequence</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/text_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">transform_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the padded, _&#39;numericalised&#39;_ sequence</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    text: str</span>
<span class="sd">        text to be tokenized and padded</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        Padded, _&#39;numericalised&#39;_ sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attributes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;vocab&quot;</span><span class="p">])</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">get_texts</span><span class="p">([</span><span class="n">text</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">already_processed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cpus</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_sequences</span><span class="p">(</span><span class="n">tokens</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.fit_transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit_transform</span>


<a href="#pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.fit_transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Combines <code>fit</code> and <code>transform</code></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.ndarray">ndarray</span></code>
          –
          <div class="doc-md-description">
            <p>Padded, <em>'numericalised'</em> sequences</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/text_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Combines `fit` and `transform`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        Padded, _&#39;numericalised&#39;_ sequences</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.inverse_transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">inverse_transform</span>


<a href="#pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor.inverse_transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">inverse_transform</span><span class="p">(</span><span class="n">padded_seq</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Returns the original text plus the added 'special' tokens</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>padded_seq</code></b>
              (<code><span title="numpy.ndarray">ndarray</span></code>)
          –
          <div class="doc-md-description">
            <p>array with the output of the <code>transform</code> method</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="pandas.DataFrame">DataFrame</span></code>
          –
          <div class="doc-md-description">
            <p>Pandas dataframe with the original text plus the added 'special' tokens</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/text_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padded_seq</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the original text plus the added &#39;special&#39; tokens</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    padded_seq: np.ndarray</span>
<span class="sd">        array with the output of the `transform` method</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pd.DataFrame</span>
<span class="sd">        Pandas dataframe with the original text plus the added &#39;special&#39; tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">num</span><span class="p">)</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">padded_seq</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="bp">self</span><span class="o">.</span><span class="n">text_col</span><span class="p">:</span> <span class="n">texts</span><span class="p">})</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">HFPreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">HFPreprocessor</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">use_fast_tokenizer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">text_col</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">root_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">preprocessing_rules</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">encode_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.preprocessing.base_preprocessor.BasePreprocessor">BasePreprocessor</span></code></p>


        <p>Text processor to prepare the <code>deeptext</code> input dataset that is a
wrapper around HuggingFace's tokenizers.</p>
<p>Following the main phylosophy of the <code>pytorch-widedeep</code> library, this
class is designed to be as flexible as possible. Therefore, it is coded
so that the user can use it as one would use any HuggingFace tokenizers,
or following the API call 'protocol' of the rest of the library.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>model_name</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>The model name from the transformers library e.g. <em>'bert-base-uncased'</em>.
Currently supported models are those from the families: BERT, RoBERTa,
DistilBERT, ALBERT and ELECTRA.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_fast_tokenizer</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Whether to use the fast tokenizer from HuggingFace or not</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>text_col</code></b>
              (<code><span title="typing.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The column in the input dataframe containing the text data. If this
tokenizer is used via the <code>fit</code> and <code>transform</code> methods, this
argument is mandatory. If the tokenizer is used via the <code>encode</code>
method, this argument is not needed since the input text is passed
directly to the <code>encode</code> method.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>num_workers</code></b>
              (<code><span title="typing.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Number of workers to use when preprocessing the text data. If not
None, and <code>use_fast_tokenizer</code> is False, the text data will be
preprocessed in parallel using the number of workers specified. If
<code>use_fast_tokenizer</code> is True, this argument is ignored.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>preprocessing_rules</code></b>
              (<code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="typing.Callable">Callable</span>[[str], str]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>A list of functions to be applied to the text data before encoding.
This can be useful to clean the text data before encoding. For
example, removing html tags, special characters, etc.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>tokenizer_params</code></b>
              (<code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Additional parameters to be passed to the HuggingFace's
<code>PreTrainedTokenizer</code>. Parameters to the <code>PreTrainedTokenizer</code>
can also be passed via the <code>**kwargs</code> argument</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>encode_params</code></b>
              (<code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Additional parameters to be passed to the <code>batch_encode_plus</code> method
of the HuggingFace's <code>PreTrainedTokenizer</code>. If the <code>fit</code> and <code>transform</code>
methods are used, the <code>encode_params</code> dict parameter is mandatory. If
the <code>encode</code> method is used, this parameter is not needed since the
input text is passed directly to the <code>encode</code> method.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>**kwargs</code></b>
          –
          <div class="doc-md-description">
            <p>Additional kwargs to be passed to the model, in particular to the
<code>PreTrainedTokenizer</code> class.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.is_fitted">is_fitted</span></code></b>
              (<code>bool</code>)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the preprocessor has been fitted. This is a
HuggingFacea tokenizer, so it is always considered fitted and this
attribute is manually set to True internally. This parameter exists
for consistency with the rest of the library and because is needed
for some functionality in the library.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">HFPreprocessor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;this is the first text&quot;</span><span class="p">,</span> <span class="s2">&quot;this is the second text&quot;</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hf_processor_1</span> <span class="o">=</span> <span class="n">HFPreprocessor</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">text_col</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text_1</span> <span class="o">=</span> <span class="n">hf_processor_1</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;this is a new text&quot;</span><span class="p">,</span> <span class="s2">&quot;this is another text&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hf_processor_2</span> <span class="o">=</span> <span class="n">HFPreprocessor</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text_2</span> <span class="o">=</span> <span class="n">hf_processor_2</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/hf_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">use_fast_tokenizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">text_col</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">root_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">preprocessing_rules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encode_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_fast_tokenizer</span> <span class="o">=</span> <span class="n">use_fast_tokenizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">text_col</span> <span class="o">=</span> <span class="n">text_col</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span> <span class="o">=</span> <span class="n">root_dir</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">preprocessing_rules</span> <span class="o">=</span> <span class="n">preprocessing_rules</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_params</span> <span class="o">=</span> <span class="n">tokenizer_params</span> <span class="k">if</span> <span class="n">tokenizer_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span> <span class="o">=</span> <span class="n">encode_params</span> <span class="k">if</span> <span class="n">encode_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_multiprocessing</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">num_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">use_fast_tokenizer</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenizer</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">use_fast_tokenizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_fast_tokenizer</span><span class="p">,</span>
        <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_params</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># A HuggingFace tokenizer is already trained, since we need this</span>
    <span class="c1"># attribute elsewhere in the library, we simply set it to True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">True</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.encode" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">encode</span>


<a href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.encode" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">encode</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Encodes a list of texts. The method is a wrapper around the
<code>batch_encode_plus</code> method of the HuggingFace's tokenizer.</p>
<p>if 'use_fast_tokenizer' is True, the method will use the <code>batch_encode_plus</code></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>texts</code></b>
              (<code><span title="typing.List">List</span>[str]</code>)
          –
          <div class="doc-md-description">
            <p>List of texts to be encoded</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>**kwargs</code></b>
          –
          <div class="doc-md-description">
            <p>Additional parameters to be passed to the <code>batch_encode_plus</code> method
of the HuggingFace's tokenizer. If the 'encode_params' dict was passed
when instantiating the class, that dictionaly will be updated with
the kwargs passed here.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.array">array</span></code>
          –
          <div class="doc-md-description">
            <p>The encoded texts</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/hf_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes a list of texts. The method is a wrapper around the</span>
<span class="sd">    `batch_encode_plus` method of the HuggingFace&#39;s tokenizer.</span>

<span class="sd">    if &#39;use_fast_tokenizer&#39; is True, the method will use the `batch_encode_plus`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    texts: List[str]</span>
<span class="sd">        List of texts to be encoded</span>
<span class="sd">    **kwargs</span>
<span class="sd">        Additional parameters to be passed to the `batch_encode_plus` method</span>
<span class="sd">        of the HuggingFace&#39;s tokenizer. If the &#39;encode_params&#39; dict was passed</span>
<span class="sd">        when instantiating the class, that dictionaly will be updated with</span>
<span class="sd">        the kwargs passed here.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.array</span>
<span class="sd">        The encoded texts</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessing_rules</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiprocessing</span><span class="p">:</span>
            <span class="n">texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_text_parallel</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiprocessing</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_paralell</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">encoded_texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">(</span>
            <span class="n">texts</span><span class="p">,</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">encoded_texts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_ids&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Padding and Truncating parameters were not passed and all input arrays &quot;</span>
            <span class="s2">&quot;do not have the same shape. Padding to the longest sequence. &quot;</span>
            <span class="s2">&quot;Padding will be done with the index of the pad token for the model&quot;</span><span class="p">,</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="k">for</span> <span class="n">ids</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)))</span>
                <span class="k">for</span> <span class="n">ids</span> <span class="ow">in</span> <span class="n">input_ids</span>
            <span class="p">]</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.decode" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">decode</span>


<a href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.decode" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">decode</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Decodes a list of input_ids. The method is a wrapper around the
<code>convert_ids_to_tokens</code> and <code>convert_tokens_to_string</code> methods of the
HuggingFace's tokenizer.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>input_ids</code></b>
              (<code><span title="numpy.typing.NDArray">NDArray</span>[<span title="numpy.int64">int64</span>]</code>)
          –
          <div class="doc-md-description">
            <p>The input_ids to be decoded</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>skip_special_tokens</code></b>
              (<code>bool</code>)
          –
          <div class="doc-md-description">
            <p>Whether to skip the special tokens or not</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="typing.List">List</span>[str]</code>
          –
          <div class="doc-md-description">
            <p>The decoded texts</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/hf_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">decode</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decodes a list of input_ids. The method is a wrapper around the</span>
<span class="sd">    `convert_ids_to_tokens` and `convert_tokens_to_string` methods of the</span>
<span class="sd">    HuggingFace&#39;s tokenizer.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_ids: npt.NDArray[np.int64]</span>
<span class="sd">        The input_ids to be decoded</span>
<span class="sd">    skip_special_tokens: bool</span>
<span class="sd">        Whether to skip the special tokens or not</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    List[str]</span>
<span class="sd">        The decoded texts</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_string</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">texts</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.fit" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit</span>


<a href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>This method is included for consistency with the rest of the library
in general and with the <code>BasePreprocessor</code> in particular. HuggingFace's
tokenizers and models are already trained. Therefore, the 'fit' method
here does nothing other than checking that the 'text_col' parameter is
not <code>None</code>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>The dataframe containing the text data in the column specified by
the 'text_col' parameter</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/hf_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;HFPreprocessor&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method is included for consistency with the rest of the library</span>
<span class="sd">    in general and with the `BasePreprocessor` in particular. HuggingFace&#39;s</span>
<span class="sd">    tokenizers and models are already trained. Therefore, the &#39;fit&#39; method</span>
<span class="sd">    here does nothing other than checking that the &#39;text_col&#39; parameter is</span>
<span class="sd">    not `None`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        The dataframe containing the text data in the column specified by</span>
<span class="sd">        the &#39;text_col&#39; parameter</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_col</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;text_col&#39; is None. Please specify the column name containing the text data&quot;</span>
            <span class="s2">&quot; if you want to use the &#39;fit&#39; method&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">transform</span>


<a href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Encodes the text data in the input dataframe. This method simply
calls the <code>encode</code> method under the hood. Similar to the <code>fit</code> method,
this method is included for consistency with the rest of the library
in general and with the <code>BasePreprocessor</code> in particular.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>The dataframe containing the text data in the column specified by
the 'text_col' parameter</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.array">array</span></code>
          –
          <div class="doc-md-description">
            <p>The encoded texts</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/hf_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes the text data in the input dataframe. This method simply</span>
<span class="sd">    calls the `encode` method under the hood. Similar to the `fit` method,</span>
<span class="sd">    this method is included for consistency with the rest of the library</span>
<span class="sd">    in general and with the `BasePreprocessor` in particular.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        The dataframe containing the text data in the column specified by</span>
<span class="sd">        the &#39;text_col&#39; parameter</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.array</span>
<span class="sd">        The encoded texts</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_col</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;text_col&#39; is None. Please specify the column name containing the text data&quot;</span>
            <span class="s2">&quot; if you want to use the &#39;fit&#39; method&quot;</span>
        <span class="p">)</span>

    <span class="n">texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_texts</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.transform_sample" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">transform_sample</span>


<a href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.transform_sample" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">transform_sample</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Encodes a single text sample.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>text</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>The text sample to be encoded</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.array">array</span></code>
          –
          <div class="doc-md-description">
            <p>The encoded text</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/hf_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">transform_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes a single text sample.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    text: str</span>
<span class="sd">        The text sample to be encoded</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.array</span>
<span class="sd">        The encoded text</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The `encode` (or `fit`) method must be called before calling `transform_sample`&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">([</span><span class="n">text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.fit_transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit_transform</span>


<a href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.fit_transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Encodes the text data in the input dataframe.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>The dataframe containing the text data in the column specified by
the 'text_col' parameter</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.array">array</span></code>
          –
          <div class="doc-md-description">
            <p>The encoded texts</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/hf_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes the text data in the input dataframe.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        The dataframe containing the text data in the column specified by</span>
<span class="sd">        the &#39;text_col&#39; parameter</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.array</span>
<span class="sd">        The encoded texts</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.inverse_transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">inverse_transform</span>


<a href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor.inverse_transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">inverse_transform</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Decodes a list of input_ids. The method simply calls the <code>decode</code> method
under the hood.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>input_ids</code></b>
              (<code><span title="numpy.typing.NDArray">NDArray</span>[<span title="numpy.int64">int64</span>]</code>)
          –
          <div class="doc-md-description">
            <p>The input_ids to be decoded</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>skip_special_tokens</code></b>
              (<code>bool</code>)
          –
          <div class="doc-md-description">
            <p>Whether to skip the special tokens or not</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="typing.List">List</span>[str]</code>
          –
          <div class="doc-md-description">
            <p>The decoded texts</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/hf_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decodes a list of input_ids. The method simply calls the `decode` method</span>
<span class="sd">    under the hood.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_ids: npt.NDArray[np.int64]</span>
<span class="sd">        The input_ids to be decoded</span>
<span class="sd">    skip_special_tokens: bool</span>
<span class="sd">        Whether to skip the special tokens or not</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    List[str]</span>
<span class="sd">        The decoded texts</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ImagePreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">ImagePreprocessor</span><span class="p">(</span>
    <span class="n">img_col</span><span class="p">,</span> <span class="n">img_path</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.preprocessing.base_preprocessor.BasePreprocessor">BasePreprocessor</span></code></p>


        <p>Preprocessor to prepare the <code>deepimage</code> input dataset.</p>
<p>The Preprocessing consists simply on resizing according to their
aspect ratio</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>img_col</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>name of the column with the images filenames</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>img_path</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>path to the dicrectory where the images are stored</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>width</code></b>
              (<code>int</code>, default:
                  <code>224</code>
)
          –
          <div class="doc-md-description">
            <p>width of the resulting processed image.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>height</code></b>
              (<code>int</code>, default:
                  <code>224</code>
)
          –
          <div class="doc-md-description">
            <p>width of the resulting processed image.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>verbose</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>Enable verbose output.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor.aap">aap</span></code></b>
              (<code><a class="autorefs autorefs-internal" title="pytorch_widedeep.utils.image_utils.AspectAwarePreprocessor" href="utils/image_utils.html#pytorch_widedeep.utils.image_utils.AspectAwarePreprocessor">AspectAwarePreprocessor</a></code>)
          –
          <div class="doc-md-description">
            <p>an instance of <code>pytorch_widedeep.utils.image_utils.AspectAwarePreprocessor</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor.spp">spp</span></code></b>
              (<code><a class="autorefs autorefs-internal" title="pytorch_widedeep.utils.image_utils.SimplePreprocessor" href="utils/image_utils.html#pytorch_widedeep.utils.image_utils.SimplePreprocessor">SimplePreprocessor</a></code>)
          –
          <div class="doc-md-description">
            <p>an instance of <code>pytorch_widedeep.utils.image_utils.SimplePreprocessor</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor.normalise_metrics">normalise_metrics</span></code></b>
              (<code>Dict</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the normalisation metrics of the image dataset, i.e.
mean and std for the R, G and B channels</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">ImagePreprocessor</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">path_to_image1</span> <span class="o">=</span> <span class="s1">&#39;tests/test_data_utils/images/galaxy1.png&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">path_to_image2</span> <span class="o">=</span> <span class="s1">&#39;tests/test_data_utils/images/galaxy2.png&#39;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;images_column&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">path_to_image1</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;images_column&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">path_to_image2</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img_preprocessor</span> <span class="o">=</span> <span class="n">ImagePreprocessor</span><span class="p">(</span><span class="n">img_col</span><span class="o">=</span><span class="s1">&#39;images_column&#39;</span><span class="p">,</span> <span class="n">img_path</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">resized_images</span> <span class="o">=</span> <span class="n">img_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_resized_images</span> <span class="o">=</span> <span class="n">img_preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
</code></pre></div>
    <p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>:
Normalising metrics will only be computed when the <code>fit_transform</code>
method is run. Running <code>transform</code> only will not change the computed
metrics and running <code>fit</code> only simply instantiates the resizing
functions.</p>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/image_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">img_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">img_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">224</span><span class="p">,</span>
    <span class="n">height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">224</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ImagePreprocessor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">img_col</span> <span class="o">=</span> <span class="n">img_col</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">img_path</span> <span class="o">=</span> <span class="n">img_path</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">width</span> <span class="o">=</span> <span class="n">width</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">height</span> <span class="o">=</span> <span class="n">height</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">aap</span> <span class="o">=</span> <span class="n">AspectAwarePreprocessor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">height</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">spp</span> <span class="o">=</span> <span class="n">SimplePreprocessor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">height</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">compute_normalising_computed</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor.transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">transform</span>


<a href="#pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor.transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Resizes the images to the input height and width.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe with the <code>img_col</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.ndarray">ndarray</span></code>
          –
          <div class="doc-md-description">
            <p>Resized images to the input height and width</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/image_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Resizes the images to the input height and width.</span>


<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe with the `img_col`</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        Resized images to the input height and width</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">image_list</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">img_col</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reading Images from </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">img_path</span><span class="p">))</span>
    <span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">img_path</span><span class="p">,</span> <span class="n">img</span><span class="p">]))</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">]</span>

    <span class="c1"># finding images with different height and width</span>
    <span class="n">aspect</span> <span class="o">=</span> <span class="p">[(</span><span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">im</span> <span class="ow">in</span> <span class="n">imgs</span><span class="p">]</span>
    <span class="n">aspect_r</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">aspect</span><span class="p">]</span>
    <span class="n">diff_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">aspect_r</span><span class="p">)</span> <span class="k">if</span> <span class="n">r</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Resizing&quot;</span><span class="p">)</span>
    <span class="n">resized_imgs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">imgs</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">),</span> <span class="n">disable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">diff_idx</span><span class="p">:</span>
            <span class="n">resized_imgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">aap</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if aspect ratio is 1:1, no need for AspectAwarePreprocessor</span>
            <span class="n">resized_imgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spp</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_normalising_computed</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Computing normalisation metrics&quot;</span><span class="p">)</span>
        <span class="c1"># mean and std deviation will only be computed when the fit method</span>
        <span class="c1"># is called</span>
        <span class="n">mean_R</span><span class="p">,</span> <span class="n">mean_G</span><span class="p">,</span> <span class="n">mean_B</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="n">std_R</span><span class="p">,</span> <span class="n">std_G</span><span class="p">,</span> <span class="n">std_B</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">rsz_img</span> <span class="ow">in</span> <span class="n">resized_imgs</span><span class="p">:</span>
            <span class="p">(</span><span class="n">mean_b</span><span class="p">,</span> <span class="n">mean_g</span><span class="p">,</span> <span class="n">mean_r</span><span class="p">),</span> <span class="p">(</span><span class="n">std_b</span><span class="p">,</span> <span class="n">std_g</span><span class="p">,</span> <span class="n">std_r</span><span class="p">)</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">meanStdDev</span><span class="p">(</span>
                <span class="n">rsz_img</span>
            <span class="p">)</span>
            <span class="n">mean_R</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_r</span><span class="p">)</span>
            <span class="n">mean_G</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_g</span><span class="p">)</span>
            <span class="n">mean_B</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_b</span><span class="p">)</span>
            <span class="n">std_R</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">std_r</span><span class="p">)</span>
            <span class="n">std_G</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">std_g</span><span class="p">)</span>
            <span class="n">std_B</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">std_b</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalise_metrics</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">mean</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;R&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mean_R</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span>
                <span class="s2">&quot;G&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mean_G</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span>
                <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mean_B</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="n">std</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;R&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">std_R</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span>
                <span class="s2">&quot;G&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">std_G</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span>
                <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">std_B</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_normalising_computed</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">resized_imgs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor.fit_transform" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit_transform</span>


<a href="#pytorch_widedeep.preprocessing.image_preprocessor.ImagePreprocessor.fit_transform" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Combines <code>fit</code> and <code>transform</code></p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>df</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="numpy.ndarray">ndarray</span></code>
          –
          <div class="doc-md-description">
            <p>Resized images to the input height and width</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/image_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Combines `fit` and `transform`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        Resized images to the input height and width</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.din_preprocessor.DINPreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">DINPreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.din_preprocessor.DINPreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">DINPreprocessor</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">user_id_col</span><span class="p">,</span>
    <span class="n">item_embed_col</span><span class="p">,</span>
    <span class="n">target_col</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="p">,</span>
    <span class="n">action_col</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">other_seq_embed_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cols_to_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">auto_embed_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">embedding_rule</span><span class="o">=</span><span class="s2">&quot;fastai_new&quot;</span><span class="p">,</span>
    <span class="n">default_embed_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">already_standard</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.preprocessing.base_preprocessor.BasePreprocessor">BasePreprocessor</span></code></p>


        <p>Preprocessor for Deep Interest Network (DIN) models.</p>
<p>This preprocessor handles the preparation of data for DIN models,
including sequence building, label encoding, and handling of various
types of input columns (categorical, continuous, and sequential).</p>


<details class="parameters:" open>
  <summary></summary>
  <p>user_id_col : str
    Name of the column containing user IDs.
item_embed_col : Union[str, Tuple[str, int]]
    Name of the column containing item IDs to be embedded, or a tuple of
    (column_name, embedding_dim).
target_col : str
    Name of the column containing the target variable.
max_seq_length : int
    Maximum length of sequences to be created.
action_col : Optional[str], default=None
    Name of the column containing user actions (if applicable).
other_seq_embed_cols : Optional[List[str] | List[Tuple[str, int]]], default=None
    List of other columns to be treated as sequences.
cat_embed_cols : Optional[Union[List[str], List[Tuple[str, int]]]], default=None
    List of categorical columns to be represented by embeddings.
continuous_cols: List, default = None
    List with the name of the continuous cols.
quantization_setup: int or Dict[str, Union[int, List[float]]], default=None
    Continuous columns can be turned into categorical via <code>pd.cut</code>.
cols_to_scale: List or str, default = None,
    List with the names of the columns that will be standardized via
    sklearn's <code>StandardScaler</code>.
auto_embed_dim: bool, default = True
    Boolean indicating whether the embedding dimensions will be
    automatically defined via rule of thumb.
embedding_rule: str, default = 'fastai_new'
    Rule of thumb for embedding size.
default_embed_dim: int, default=16
    Default dimension for the embeddings.
verbose : int, default=1
    Verbosity level.
scale: bool, default = False
    Boolean indicating whether or not to scale/standardize continuous cols.
already_standard: List, default = None
    List with the name of the continuous cols that do not need to be
    scaled/standardized.
**kwargs :
    Additional keyword arguments to be passed to the TabPreprocessor.</p>
</details>

<details class="attributes:" open>
  <summary></summary>
  <p>is_fitted : bool
    Whether the preprocessor has been fitted.
has_standard_tab_data : bool
    Whether the data includes standard tabular data.
tab_preprocessor : TabPreprocessor
    Preprocessor for standard tabular data.
din_columns_idx : Dict[str, int]
    Dictionary mapping column names to their indices in the processed data.
item_le : LabelEncoder
    Label encoder for item IDs.
n_items : int
    Number of unique items.
user_behaviour_config : Tuple[List[str], int, int]
    Configuration for user behavior sequences.
action_le : LabelEncoder
    Label encoder for action column (if applicable).
n_actions : int
    Number of unique actions (if applicable).
action_seq_config : Tuple[List[str], int]
    Configuration for action sequences (if applicable).
other_seq_le : LabelEncoder
    Label encoder for other sequence columns (if applicable).
n_other_seq_cols : Dict[str, int]
    Number of unique values in each other sequence column.
other_seq_config : List[Tuple[List[str], int, int]]
    Configuration for other sequence columns.</p>
</details>

<details class="examples:" open>
  <summary></summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import pandas as pd
from pytorch_widedeep.preprocessing import DINPreprocessor
data = {
...    'user_id': [1, 1, 1, 2, 2, 2, 3, 3, 3],
...    'item_id': [101, 102, 103, 101, 103, 104, 102, 103, 104],
...    'timestamp': [1, 2, 3, 1, 2, 3, 1, 2, 3],
...    'category': ['A', 'B', 'A', 'B', 'A', 'C', 'B', 'A', 'C'],
...    'price': [10.5, 15.0, 12.0, 10.5, 12.0, 20.0, 15.0, 12.0, 20.0],
...    'rating': [0, 1, 0, 1, 0, 1, 0, 1, 0]
... }
df = pd.DataFrame(data)
din_preprocessor = DINPreprocessor(
...     user_id_col='user_id',
...     item_embed_col='item_id',
...     target_col='rating',
...     max_seq_length=2,
...     action_col='rating',
...     other_seq_embed_cols=['category'],
...     cat_embed_cols=['user_id'],
...     continuous_cols=['price'],
...     cols_to_scale=['price']
... )
X, y = din_preprocessor.fit_transform(df)</p>
</blockquote>
</blockquote>
</blockquote>
</details>
                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/din_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;item_embed_col&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;item_id_col&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">user_id_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">item_embed_col</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
    <span class="n">target_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">action_col</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">other_seq_embed_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cols_to_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">auto_embed_dim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">embedding_rule</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;google&quot;</span><span class="p">,</span> <span class="s2">&quot;fastai_old&quot;</span><span class="p">,</span> <span class="s2">&quot;fastai_new&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;fastai_new&quot;</span><span class="p">,</span>
    <span class="n">default_embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">already_standard</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">user_id_col</span> <span class="o">=</span> <span class="n">user_id_col</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">item_embed_col</span> <span class="o">=</span> <span class="n">item_embed_col</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span> <span class="o">=</span> <span class="n">max_seq_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">target_col</span> <span class="o">=</span> <span class="n">target_col</span> <span class="k">if</span> <span class="n">target_col</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;target&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">action_col</span> <span class="o">=</span> <span class="n">action_col</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">other_seq_embed_cols</span> <span class="o">=</span> <span class="n">other_seq_embed_cols</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_cols</span> <span class="o">=</span> <span class="n">cat_embed_cols</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="n">continuous_cols</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">quantization_setup</span> <span class="o">=</span> <span class="n">quantization_setup</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cols_to_scale</span> <span class="o">=</span> <span class="n">cols_to_scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">auto_embed_dim</span> <span class="o">=</span> <span class="n">auto_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding_rule</span> <span class="o">=</span> <span class="n">embedding_rule</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">default_embed_dim</span> <span class="o">=</span> <span class="n">default_embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">already_standard</span> <span class="o">=</span> <span class="n">already_standard</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">has_standard_tab_data</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_cols</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">continuous_cols</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_standard_tab_data</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tab_preprocessor</span> <span class="o">=</span> <span class="n">TabPreprocessor</span><span class="p">(</span>
            <span class="n">cat_embed_cols</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cat_embed_cols</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">cols_to_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cols_to_scale</span><span class="p">,</span>
            <span class="n">auto_embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">auto_embed_dim</span><span class="p">,</span>
            <span class="n">embedding_rule</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_rule</span><span class="p">,</span>
            <span class="n">default_embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">default_embed_dim</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">already_standard</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">already_standard</span><span class="p">,</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div><h2 id="chunked-versions">Chunked versions<a class="headerlink" href="#chunked-versions" title="Permanent link">&para;</a></h2>
<p>Chunked versions of the preprocessors are also available. These are useful
when the data is too big to fit in memory. See also the <a href="load_from_folder.html"><code>load_from_folder</code></a>
module in the library and the corresponding section here in the documentation.</p>
<p>Note that there is not a <code>ChunkImagePreprocessor</code>. This is because the
processing of the images will occur inside the <code>ImageFromFolder</code> class in
the <a href="load_from_folder.html"><code>load_from_folder</code></a> module.</p>


<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChunkWidePreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">ChunkWidePreprocessor</span><span class="p">(</span>
    <span class="n">wide_cols</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="n">crossed_cols</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor" href="#pytorch_widedeep.preprocessing.wide_preprocessor.WidePreprocessor">WidePreprocessor</a></code></p>


        <p>Preprocessor to prepare the wide input dataset</p>
<p>This Preprocessor prepares the data for the wide, linear component.
This linear model is implemented via an Embedding layer that is
connected to the output neuron. <code>ChunkWidePreprocessor</code> numerically
encodes all the unique values of all categorical columns <code>wide_cols +
crossed_cols</code>. See the Example below.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>wide_cols</code></b>
              (<code><span title="typing.List">List</span>[str]</code>)
          –
          <div class="doc-md-description">
            <p>List of strings with the name of the columns that will label
encoded and passed through the <code>wide</code> component</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>crossed_cols</code></b>
              (<code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="typing.Tuple">Tuple</span>[str, str]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the name of the columns that will be <code>'crossed'</code>
and then label encoded. e.g. <em>[('education', 'occupation'), ...]</em>. For
binary features, a cross-product transformation is 1 if and only if
the constituent features are all 1, and 0 otherwise.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor.wide_crossed_cols">wide_crossed_cols</span></code></b>
              (<code><span title="typing.List">List</span></code>)
          –
          <div class="doc-md-description">
            <p>List with the names of all columns that will be label encoded</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor.encoding_dict">encoding_dict</span></code></b>
              (<code>Dict</code>)
          –
          <div class="doc-md-description">
            <p>Dictionary where the keys are the result of pasting <code>colname + '_' +
column value</code> and the values are the corresponding mapped integer.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor.inverse_encoding_dict">inverse_encoding_dict</span></code></b>
              (<code>Dict</code>)
          –
          <div class="doc-md-description">
            <p>the inverse encoding dictionary</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor.wide_dim">wide_dim</span></code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Dimension of the wide model (i.e. dim of the linear layer)</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">ChunkWidePreprocessor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">chunk</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;l&#39;</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">crossed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;color&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">chunk_wide_preprocessor</span> <span class="o">=</span> <span class="n">ChunkWidePreprocessor</span><span class="p">(</span><span class="n">wide_cols</span><span class="o">=</span><span class="n">wide_cols</span><span class="p">,</span> <span class="n">crossed_cols</span><span class="o">=</span><span class="n">crossed_cols</span><span class="p">,</span>
<span class="gp">... </span><span class="n">n_chunks</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_wide</span> <span class="o">=</span> <span class="n">chunk_wide_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/wide_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">wide_cols</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">n_chunks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">crossed_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ChunkWidePreprocessor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">wide_cols</span><span class="p">,</span> <span class="n">crossed_cols</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_chunks</span> <span class="o">=</span> <span class="n">n_chunks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">chunk_counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor.partial_fit" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">partial_fit</span>


<a href="#pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor.partial_fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">partial_fit</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Fits the Preprocessor and creates required attributes</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>chunk</code></b>
              (<code><span title="pandas.DataFrame">DataFrame</span></code>)
          –
          <div class="doc-md-description">
            <p>Input pandas dataframe</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor" href="#pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor">ChunkWidePreprocessor</a></code>
          –
          <div class="doc-md-description">
            <p><code>ChunkWidePreprocessor</code> fitted object</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/wide_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chunk</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;ChunkWidePreprocessor&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Fits the Preprocessor and creates required attributes</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    chunk: pd.DataFrame</span>
<span class="sd">        Input pandas dataframe</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ChunkWidePreprocessor</span>
<span class="sd">        `ChunkWidePreprocessor` fitted object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">df_wide</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_wide</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wide_crossed_cols</span> <span class="o">=</span> <span class="n">df_wide</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_counter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">glob_feature_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_make_global_feature_list</span><span class="p">(</span><span class="n">df_wide</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wide_crossed_cols</span><span class="p">])</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">glob_feature_set</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_make_global_feature_list</span><span class="p">(</span><span class="n">df_wide</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wide_crossed_cols</span><span class="p">])</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">chunk_counter</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_counter</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_chunks</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">glob_feature_set</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wide_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoding_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inverse_encoding_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inverse_encoding_dict</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;unseen&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor.fit" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">fit</span>


<a href="#pytorch_widedeep.preprocessing.wide_preprocessor.ChunkWidePreprocessor.fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Runs <code>partial_fit</code>. This is just to override the fit method in the base
class. This class is not designed or thought to run fit</p>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/preprocessing/wide_preprocessor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;ChunkWidePreprocessor&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs `partial_fit`. This is just to override the fit method in the base</span>
<span class="sd">    class. This class is not designed or thought to run fit</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChunkTabPreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">ChunkTabPreprocessor</span><span class="p">(</span>
    <span class="n">n_chunks</span><span class="p">,</span>
    <span class="n">cat_embed_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cols_and_bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cols_to_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">default_embed_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">with_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">already_standard</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor" href="#pytorch_widedeep.preprocessing.tab_preprocessor.TabPreprocessor">TabPreprocessor</a></code></p>


        <p>Preprocessor to prepare the <code>deeptabular</code> component input dataset</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>n_chunks</code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Number of chunks that the tabular dataset is divided by.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str], <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List containing the name of the categorical columns that will be
represented by embeddings (e.g. <em>['education', 'relationship', ...]</em>) or
a Tuple with the name and the embedding dimension (e.g.: <em>[
('education',32), ('relationship',16), ...]</em>)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the continuous cols</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cols_and_bins</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Continuous columns can be turned into categorical via
<code>pd.cut</code>. 'cols_and_bins' is dictionary where the keys are the column
names to quantize and the values are a list of scalars indicating the
bin edges.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cols_to_scale</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str], str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the names of the columns that will be standarised via
sklearn's <code>StandardScaler</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>default_embed_dim</code></b>
              (<code>int</code>, default:
                  <code>16</code>
)
          –
          <div class="doc-md-description">
            <p>Dimension for the embeddings if the embed_dim is not provided in the
<code>cat_embed_cols</code> parameter and <code>auto_embed_dim</code> is set to
<code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>with_attention</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the preprocessed data will be passed to an
attention-based model (more precisely a model where all embeddings
must have the same dimensions). If <code>True</code>, the param <code>cat_embed_cols</code>
must just be a list containing just the categorical column names:
e.g.
<em>['education', 'relationship', ...]</em>. This is because they will all be
 encoded using embeddings of the same dim, which will be specified
 later when the model is defined. <br/> Param alias:
 <code>for_transformer</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>with_cls_token</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if a <code>'[CLS]'</code> token will be added to the dataset
when using attention-based models. The final hidden state
corresponding to this token is used as the aggregated representation
for classification and regression tasks. If not, the categorical
(and continuous embeddings if present) will be concatenated before
being passed to the final MLP (if present).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>shared_embed</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the embeddings will be "shared" when using
attention-based models. The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>verbose</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>scale</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>note</strong>: this arg will be removed in upcoming
 releases. Please use <code>cols_to_scale</code> instead. <br/> Bool indicating
 whether or not to scale/standarise continuous cols. It is important
 to emphasize that all the DL models for tabular data in the library
 also include the possibility of normalising the input continuous
 features via a <code>BatchNorm</code> or a <code>LayerNorm</code>. <br/> Param alias:
 <code>scale_cont_cols</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>already_standard</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>note</strong>: this arg will be removed in upcoming
 releases. Please use <code>cols_to_scale</code> instead. <br/> List with the
 name of the continuous cols that do not need to be
 scaled/standarised.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Other Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>**kwargs</code></b>
          –
          <div class="doc-md-description">
            <p><code>pd.cut</code> and <code>StandardScaler</code> related args</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor.embed_dim">embed_dim</span></code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span></code>)
          –
          <div class="doc-md-description">
            <p>Dictionary where keys are the embed cols and values are the embedding
dimensions. If <code>with_attention</code> is set to <code>True</code> this attribute
is not generated during the <code>fit</code> process</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor.label_encoder">label_encoder</span></code></b>
              (<code><a class="autorefs autorefs-internal" title="pytorch_widedeep.utils.deeptabular_utils.LabelEncoder" href="utils/deeptabular_utils.html#pytorch_widedeep.utils.deeptabular_utils.LabelEncoder">LabelEncoder</a></code>)
          –
          <div class="doc-md-description">
            <p>see <code>pytorch_widedeep.utils.dense_utils.LabelEncder</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor.cat_embed_input">cat_embed_input</span></code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span></code>)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name, number of individual values for
that column and, If <code>with_attention</code> is set to <code>False</code>, the
corresponding embeddings dim, e.g. <em>[('education', 16, 10),
('relationship', 6, 8), ...]</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor.standardize_cols">standardize_cols</span></code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span></code>)
          –
          <div class="doc-md-description">
            <p>List of the columns that will be standarized</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor.scaler">scaler</span></code></b>
              (<code><span title="sklearn.preprocessing.StandardScaler">StandardScaler</span></code>)
          –
          <div class="doc-md-description">
            <p>an instance of <code>sklearn.preprocessing.StandardScaler</code>
if 'cols_to_scale' is not None or 'scale' is 'True'</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor.column_idx">column_idx</span></code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span></code>)
          –
          <div class="doc-md-description">
            <p>Dictionary where keys are column names and values are column indexes.
This is neccesary to slice tensors</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.tab_preprocessor.ChunkTabPreprocessor.quantizer">quantizer</span></code></b>
              (<code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.tab_preprocessor.Quantizer" href="#pytorch_widedeep.preprocessing.tab_preprocessor.Quantizer">Quantizer</a></code>)
          –
          <div class="doc-md-description">
            <p>an instance of <code>Quantizer</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">ChunkTabPreprocessor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">chunk_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;cat_col&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span>
<span class="gp">... </span><span class="s1">&#39;cont_col&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">)})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;cat_col&#39;</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cont_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cont_col&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tab_preprocessor</span> <span class="o">=</span> <span class="n">ChunkTabPreprocessor</span><span class="p">(</span>
<span class="gp">... </span><span class="n">n_chunks</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cat_embed_cols</span><span class="o">=</span><span class="n">cat_embed_cols</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">cont_cols</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">chunk_df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">cat_embed_cols</span>
<span class="go">[(&#39;cat_col&#39;, 4)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">column_idx</span>
<span class="go">{&#39;cat_col&#39;: 0, &#39;cont_col&#39;: 1}</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/tab_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;with_attention&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;for_transformer&quot;</span><span class="p">])</span>
<span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;cat_embed_cols&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;embed_cols&quot;</span><span class="p">])</span>
<span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;scale_cont_cols&quot;</span><span class="p">])</span>
<span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;cols_and_bins&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;quantization_setup&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">n_chunks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">cat_embed_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cols_and_bins</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cols_to_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">with_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">with_cls_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">already_standard</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ChunkTabPreprocessor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">cat_embed_cols</span><span class="o">=</span><span class="n">cat_embed_cols</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">cols_to_scale</span><span class="o">=</span><span class="n">cols_to_scale</span><span class="p">,</span>
        <span class="n">auto_embed_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">embedding_rule</span><span class="o">=</span><span class="s2">&quot;google&quot;</span><span class="p">,</span>  <span class="c1"># does not matter, irrelevant</span>
        <span class="n">default_embed_dim</span><span class="o">=</span><span class="n">default_embed_dim</span><span class="p">,</span>
        <span class="n">with_attention</span><span class="o">=</span><span class="n">with_attention</span><span class="p">,</span>
        <span class="n">with_cls_token</span><span class="o">=</span><span class="n">with_cls_token</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">already_standard</span><span class="o">=</span><span class="n">already_standard</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_chunks</span> <span class="o">=</span> <span class="n">n_chunks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">chunk_counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">cols_and_bins</span> <span class="o">=</span> <span class="n">cols_and_bins</span>  <span class="c1"># type: ignore[assignment]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cols_and_bins</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cols_and_bins</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">quant_args</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed_prepared</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">continuous_prepared</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.text_preprocessor.ChunkTextPreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChunkTextPreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.text_preprocessor.ChunkTextPreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">ChunkTextPreprocessor</span><span class="p">(</span>
    <span class="n">text_col</span><span class="p">,</span>
    <span class="n">n_chunks</span><span class="p">,</span>
    <span class="n">root_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_vocab</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span>
    <span class="n">min_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">maxlen</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
    <span class="n">pad_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">pad_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">already_processed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">word_vectors_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_cpus</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor" href="#pytorch_widedeep.preprocessing.text_preprocessor.TextPreprocessor">TextPreprocessor</a></code></p>


        <p>Preprocessor to prepare the <code>deeptext</code> input dataset</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>text_col</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>column in the input dataframe containing either the texts or the
filenames where the text documents are stored</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_chunks</code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Number of chunks that the text dataset is divided by.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>root_dir</code></b>
              (<code><span title="typing.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If 'text_col' contains the filenames with the text documents, this is
the path to the directory where those documents are stored.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>max_vocab</code></b>
              (<code>int</code>, default:
                  <code>30000</code>
)
          –
          <div class="doc-md-description">
            <p>Maximum number of tokens in the vocabulary</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>min_freq</code></b>
              (<code>int</code>, default:
                  <code>5</code>
)
          –
          <div class="doc-md-description">
            <p>Minimum frequency for a token to be part of the vocabulary</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>maxlen</code></b>
              (<code>int</code>, default:
                  <code>80</code>
)
          –
          <div class="doc-md-description">
            <p>Maximum length of the tokenized sequences</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>pad_first</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Indicates whether the padding index will be added at the beginning or the
end of the sequences</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>pad_idx</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>padding index. Fastai's Tokenizer leaves 0 for the 'unknown' token.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>word_vectors_path</code></b>
              (<code><span title="typing.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Path to the pretrained word vectors</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_cpus</code></b>
              (<code><span title="typing.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>number of CPUs to used during the tokenization process</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>verbose</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>Enable verbose output.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.text_preprocessor.ChunkTextPreprocessor.vocab">vocab</span></code></b>
              (<code><a class="autorefs autorefs-internal" title="pytorch_widedeep.utils.fastai_transforms.Vocab" href="utils/fastai_transforms.html#pytorch_widedeep.utils.fastai_transforms.Vocab">Vocab</a></code>)
          –
          <div class="doc-md-description">
            <p>an instance of <code>pytorch_widedeep.utils.fastai_transforms.ChunkVocab</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.text_preprocessor.ChunkTextPreprocessor.embedding_matrix">embedding_matrix</span></code></b>
              (<code><span title="numpy.ndarray">ndarray</span></code>)
          –
          <div class="doc-md-description">
            <p>Array with the pretrained embeddings if <code>word_vectors_path</code> is not None</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">ChunkTextPreprocessor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">chunk_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text_column&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;life is like a box of chocolates&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="s2">&quot;You never know what you&#39;re gonna get&quot;</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">chunk_text_preprocessor</span> <span class="o">=</span> <span class="n">ChunkTextPreprocessor</span><span class="p">(</span><span class="n">text_col</span><span class="o">=</span><span class="s1">&#39;text_column&#39;</span><span class="p">,</span> <span class="n">n_chunks</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="n">max_vocab</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">processed_chunk</span> <span class="o">=</span> <span class="n">chunk_text_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">chunk_df</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/text_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">text_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">n_chunks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">root_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_vocab</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">30000</span><span class="p">,</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">maxlen</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span>
    <span class="n">pad_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">pad_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">already_processed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">word_vectors_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_cpus</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ChunkTextPreprocessor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">text_col</span><span class="o">=</span><span class="n">text_col</span><span class="p">,</span>
        <span class="n">max_vocab</span><span class="o">=</span><span class="n">max_vocab</span><span class="p">,</span>
        <span class="n">min_freq</span><span class="o">=</span><span class="n">min_freq</span><span class="p">,</span>
        <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">,</span>
        <span class="n">pad_first</span><span class="o">=</span><span class="n">pad_first</span><span class="p">,</span>
        <span class="n">pad_idx</span><span class="o">=</span><span class="n">pad_idx</span><span class="p">,</span>
        <span class="n">already_processed</span><span class="o">=</span><span class="n">already_processed</span><span class="p">,</span>
        <span class="n">word_vectors_path</span><span class="o">=</span><span class="n">word_vectors_path</span><span class="p">,</span>
        <span class="n">n_cpus</span><span class="o">=</span><span class="n">n_cpus</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_chunks</span> <span class="o">=</span> <span class="n">n_chunks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span> <span class="o">=</span> <span class="n">root_dir</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">chunk_counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.preprocessing.hf_preprocessor.ChunkHFPreprocessor" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChunkHFPreprocessor</span>


<a href="#pytorch_widedeep.preprocessing.hf_preprocessor.ChunkHFPreprocessor" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">ChunkHFPreprocessor</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">text_col</span><span class="p">,</span>
    <span class="n">root_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_fast_tokenizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">preprocessing_rules</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">encode_params</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor" href="#pytorch_widedeep.preprocessing.hf_preprocessor.HFPreprocessor">HFPreprocessor</a></code></p>


        <p>Text processor to prepare the <code>deeptext</code> input dataset that is a
wrapper around HuggingFace's tokenizers.</p>
<p>Hugginface Tokenizer's are already 'trained'. Therefore, unlike the
<code>ChunkTextPreprocessor</code> this is mostly identical to the <code>HFPreprocessor</code>
with the only difference that the class needs a 'text_col' parameter to
be passed. Also the parameter <code>encode_params</code> is not really optional when
using this class. It must be passed containing at least the
'max_length' encoding parameter. This is because we need to ensure that
 all sequences have the same length when encoding in chunks.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>model_name</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>The model name from the transformers library e.g. <em>'bert-base-uncased'</em>.
Currently supported models are those from the families: BERT, RoBERTa,
DistilBERT, ALBERT and ELECTRA.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>text_col</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>The column in the input dataframe containing the text data. When using
the <code>ChunkHFPreprocessor</code> the <code>text_col</code> parameter is mandatory.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>root_dir</code></b>
              (<code><span title="typing.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The root directory where the text files are located. This is only
needed if the text data is stored in text files. If the text data is
stored in a column in the input dataframe, this parameter is not
needed.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_fast_tokenizer</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Whether to use the fast tokenizer from HuggingFace or not</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>num_workers</code></b>
              (<code><span title="typing.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Number of workers to use when preprocessing the text data. If not
None, and <code>use_fast_tokenizer</code> is False, the text data will be
preprocessed in parallel using the number of workers specified. If
<code>use_fast_tokenizer</code> is True, this argument is ignored.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>preprocessing_rules</code></b>
              (<code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="typing.Callable">Callable</span>[[str], str]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>A list of functions to be applied to the text data before encoding.
This can be useful to clean the text data before encoding. For
example, removing html tags, special characters, etc.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>tokenizer_params</code></b>
              (<code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Additional parameters to be passed to the HuggingFace's
<code>PreTrainedTokenizer</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>encode_params</code></b>
              (<code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Additional parameters to be passed to the <code>batch_encode_plus</code> method
of the HuggingFace's <code>PreTrainedTokenizer</code>. In the case of the
<code>ChunkHFPreprocessor</code>, this parameter is not really <code>Optional</code>. It
must be passed containing at least the 'max_length' encoding
parameter</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.preprocessing.hf_preprocessor.ChunkHFPreprocessor.is_fitted">is_fitted</span></code></b>
              (<code>bool</code>)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the preprocessor has been fitted. This is a
HuggingFacea tokenizer, so it is always considered fitted and this
attribute is manually set to True internally. This parameter exists
for consistency with the rest of the library and because is needed
for some functionality in the library.</p>
          </div>
        </li>
    </ul>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/preprocessing/hf_preprocessor.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">text_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">root_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_fast_tokenizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">preprocessing_rules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encode_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">use_fast_tokenizer</span><span class="o">=</span><span class="n">use_fast_tokenizer</span><span class="p">,</span>
        <span class="n">text_col</span><span class="o">=</span><span class="n">text_col</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">preprocessing_rules</span><span class="o">=</span><span class="n">preprocessing_rules</span><span class="p">,</span>
        <span class="n">tokenizer_params</span><span class="o">=</span><span class="n">tokenizer_params</span><span class="p">,</span>
        <span class="n">encode_params</span><span class="o">=</span><span class="n">encode_params</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span> <span class="o">=</span> <span class="n">root_dir</span>

    <span class="c1"># when using in chunks encode_params is not really optional. I will</span>
    <span class="c1"># review types in due time</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The &#39;encode_params&#39; dict must be passed to the ChunkHFTokenizer &quot;</span>
            <span class="s2">&quot;containing at least the &#39;max_length&#39; encoding parameter&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;padding&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span><span class="p">[</span><span class="s2">&quot;padding&quot;</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span><span class="p">[</span><span class="s2">&quot;padding&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="s2">&quot;truncation&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span>
        <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span><span class="p">[</span><span class="s2">&quot;truncation&quot;</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encode_params</span><span class="p">[</span><span class="s2">&quot;truncation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>








  <aside class="md-source-file">
    
    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 5.5A3.5 3.5 0 0 1 15.5 9a3.5 3.5 0 0 1-3.5 3.5A3.5 3.5 0 0 1 8.5 9 3.5 3.5 0 0 1 12 5.5M5 8c.56 0 1.08.15 1.53.42-.15 1.43.27 2.85 1.13 3.96C7.16 13.34 6.16 14 5 14a3 3 0 0 1-3-3 3 3 0 0 1 3-3m14 0a3 3 0 0 1 3 3 3 3 0 0 1-3 3c-1.16 0-2.16-.66-2.66-1.62a5.54 5.54 0 0 0 1.13-3.96c.45-.27.97-.42 1.53-.42M5.5 18.25c0-2.07 2.91-3.75 6.5-3.75s6.5 1.68 6.5 3.75V20h-13zM0 20v-1.5c0-1.39 1.89-2.56 4.45-2.9-.59.68-.95 1.62-.95 2.65V20zm24 0h-3.5v-1.75c0-1.03-.36-1.97-.95-2.65 2.56.34 4.45 1.51 4.45 2.9z"/></svg>
      
    </span>
    <nav>
      
        <a href="mailto:jrzaurin@gmail.com">Javier</a>, 
        <a href="mailto:mulinka.pavol@gmail.com">Pavol Mulinka</a>
    </nav>
  </span>

    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Javier Zaurin and Pavol Mulinka
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://jrzaurin.medium.com/" target="_blank" rel="noopener" title="jrzaurin.medium.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262m288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.expand", "toc.integrate"], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="../stylesheets/extra.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>