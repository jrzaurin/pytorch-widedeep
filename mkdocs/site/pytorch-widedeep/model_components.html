
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch-widedeep.readthedocs.io/pytorch-widedeep/model_components.html">
      
      
        <link rel="prev" href="load_from_folder.html">
      
      
        <link rel="next" href="the_rec_module.html">
      
      
      <link rel="icon" href="../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.43">
    
    
      
        <title>Model Components - pytorch_widedeep</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-models-module" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="pytorch_widedeep" class="md-header__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            pytorch_widedeep
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Model Components
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../installation.html" class="md-tabs__link">
        
  
    
  
  Installation

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../quick_start.html" class="md-tabs__link">
        
  
    
  
  Quick Start

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="utils/index.html" class="md-tabs__link">
          
  
    
  
  Pytorch-widedeep

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../examples/01_preprocessors_and_utils.html" class="md-tabs__link">
          
  
    
  
  Examples

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../contributing.html" class="md-tabs__link">
        
  
    
  
  Contributing

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="pytorch_widedeep" class="md-nav__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    pytorch_widedeep
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quick_start.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Pytorch-widedeep
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Pytorch-widedeep
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="utils/index.html" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/deeptabular_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deeptabular utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/fastai_transforms.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fastai transforms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/image_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/text_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="preprocessing.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Preprocessing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="load_from_folder.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Load From Folder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Model Components
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="model_components.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Model Components
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.linear.wide.Wide" class="md-nav__link">
    <span class="md-ellipsis">
      Wide
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="md-nav__link">
    <span class="md-ellipsis">
      TabMlp
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      TabMlpDecoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="md-nav__link">
    <span class="md-ellipsis">
      TabResnet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      TabResnetDecoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="md-nav__link">
    <span class="md-ellipsis">
      TabNet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      TabNetDecoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="md-nav__link">
    <span class="md-ellipsis">
      ContextAttentionMLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="md-nav__link">
    <span class="md-ellipsis">
      SelfAttentionMLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      TabTransformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="md-nav__link">
    <span class="md-ellipsis">
      SAINT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      FTTransformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="md-nav__link">
    <span class="md-ellipsis">
      TabPerceiver
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="md-nav__link">
    <span class="md-ellipsis">
      TabFastFormer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" class="md-nav__link">
    <span class="md-ellipsis">
      BasicRNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN" class="md-nav__link">
    <span class="md-ellipsis">
      AttentiveRNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN" class="md-nav__link">
    <span class="md-ellipsis">
      StackedAttentiveRNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel" class="md-nav__link">
    <span class="md-ellipsis">
      HFModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.image.vision.Vision" class="md-nav__link">
    <span class="md-ellipsis">
      Vision
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.model_fusion.ModelFuser" class="md-nav__link">
    <span class="md-ellipsis">
      ModelFuser
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.wide_deep.WideDeep" class="md-nav__link">
    <span class="md-ellipsis">
      WideDeep
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="the_rec_module.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Rec Module
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bayesian models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="losses.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Losses
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Metrics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="dataloaders.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataloaders
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="callbacks.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="trainer.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_trainer.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bayesian Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="self_supervised_pretraining.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Self Supervised Pretraining
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="tab2vec.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tab2Vec
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/01_preprocessors_and_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01_preprocessors_and_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/02_model_components.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    02_model_components
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/03_binary_classification_with_defaults.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    03_binary_classification_with_defaults
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/04_regression_with_images_and_text.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    04_regression_with_images_and_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/05_save_and_load_model_and_artifacts.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    05_save_and_load_model_and_artifacts
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/06_finetune_and_warmup.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06_finetune_and_warmup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/07_custom_components.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    07_custom_components
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/08_custom_dataLoader_imbalanced_dataset.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    08_custom_dataLoader_imbalanced_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/09_extracting_embeddings.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    09_extracting_embeddings
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/10_3rd_party_integration-RayTune_WnB.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10_3rd_party_integration-RayTune_WnB
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/11_auc_multiclass.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11_auc_multiclass
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/12_ZILNLoss_origkeras_vs_pytorch_widedeep.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12_ZILNLoss_origkeras_vs_pytorch_widedeep
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/13_model_uncertainty_prediction.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13_model_uncertainty_prediction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/14_bayesian_models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14_bayesian_models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/15_Self_Supervised_Pretraning_pt1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15_Self-Supervised Pre-Training pt 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/15_Self_Supervised_Pretraning_pt2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15_Self-Supervised Pre-Training pt 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/16_Usign_a_custom_hugging_face_model.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16_Usign-a-custom-hugging-face-model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/17_feature_importance_via_attention_weights.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17_feature_importance_via_attention_weights
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/18_wide_and_deep_for_recsys_pt1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18_wide_and_deep_for_recsys_pt1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/18_wide_and_deep_for_recsys_pt2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18_wide_and_deep_for_recsys_pt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/19_load_from_folder_functionality.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    19_load_from_folder_functionality
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/20_Using_huggingface_within_widedeep.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    20-Using-huggingface-within-widedeep
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="the-models-module">The <code>models</code> module<a class="headerlink" href="#the-models-module" title="Permanent link">&para;</a></h1>
<p>This module contains the models that can be used as the four main components
that will comprise a Wide and Deep model (<code>wide</code>, <code>deeptabular</code>,
<code>deeptext</code>, <code>deepimage</code>), as well as the <code>WideDeep</code> "constructor"
class. Note that each of the four components can be used independently. It
also contains all the documentation for the models that can be used for
self-supervised pre-training with tabular data.</p>


<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.linear.wide.Wide" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Wide</span>


<a href="#pytorch_widedeep.models.tabular.linear.wide.Wide" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Defines a <code>Wide</code> (linear) model where the non-linearities are
captured via the so-called crossed-columns. This can be used as the
<code>wide</code> component of a Wide &amp; Deep model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>size of the Linear layer (implemented via an Embedding layer).
<code>input_dim</code> is the summation of all the individual values for all the
features that go through the wide model. For example, if the wide
model receives 2 features with 5 individual values each, <code>input_dim =
10</code></p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pred_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>size of the ouput tensor containing the predictions. Note that unlike
all the other models, the wide model is connected directly to the
output neuron(s) when used to build a Wide and Deep model. Therefore,
it requires the <code>pred_dim</code> parameter.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.linear.wide.Wide.wide_linear">wide_linear</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the linear layer that comprises the wide branch of the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Wide</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">wide</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/linear/wide.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Wide</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a `Wide` (linear) model where the non-linearities are</span>
<span class="sd">    captured via the so-called crossed-columns. This can be used as the</span>
<span class="sd">    `wide` component of a Wide &amp; Deep model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    -----------</span>
<span class="sd">    input_dim: int</span>
<span class="sd">        size of the Linear layer (implemented via an Embedding layer).</span>
<span class="sd">        `input_dim` is the summation of all the individual values for all the</span>
<span class="sd">        features that go through the wide model. For example, if the wide</span>
<span class="sd">        model receives 2 features with 5 individual values each, `input_dim =</span>
<span class="sd">        10`</span>
<span class="sd">    pred_dim: int, default = 1</span>
<span class="sd">        size of the ouput tensor containing the predictions. Note that unlike</span>
<span class="sd">        all the other models, the wide model is connected directly to the</span>
<span class="sd">        output neuron(s) when used to build a Wide and Deep model. Therefore,</span>
<span class="sd">        it requires the `pred_dim` parameter.</span>

<span class="sd">    Attributes</span>
<span class="sd">    -----------</span>
<span class="sd">    wide_linear: nn.Module</span>
<span class="sd">        the linear layer that comprises the wide branch of the model</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import Wide</span>
<span class="sd">    &gt;&gt;&gt; X = torch.empty(4, 4).random_(20)</span>
<span class="sd">    &gt;&gt;&gt; wide = Wide(input_dim=int(X.max().item()), pred_dim=1)</span>
<span class="sd">    &gt;&gt;&gt; out = wide(X)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;pred_dim&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;pred_size&quot;</span><span class="p">,</span> <span class="s2">&quot;num_class&quot;</span><span class="p">])</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">pred_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Wide</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span> <span class="o">=</span> <span class="n">pred_dim</span>

        <span class="c1"># Embeddings: val + 1 because 0 is reserved for padding/unseen cateogories.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pred_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># (Sum(Embedding) + bias) is equivalent to (OneHotVector + Linear)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">pred_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;initialize Embedding and bias like nn.Linear. See [original</span>
<span class="sd">        implementation](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass. Simply connecting the Embedding layer with the ouput</span>
<span class="sd">        neuron(s)&quot;&quot;&quot;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.models.tabular.linear.wide.Wide.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#pytorch_widedeep.models.tabular.linear.wide.Wide.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Forward pass. Simply connecting the Embedding layer with the ouput
neuron(s)</p>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/models/tabular/linear/wide.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass. Simply connecting the Embedding layer with the ouput</span>
<span class="sd">    neuron(s)&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabMlp</span>


<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>


        <p>Defines a <code>TabMlp</code> model that can be used as the <code>deeptabular</code>
component of a Wide &amp; Deep model or independently by itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features, embedded or not. These are then
passed through a series of dense layers (i.e. a MLP).</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the continuous columns will be embedded using
one of the available methods: <em>'standard'</em>, <em>'periodic'</em>
or <em>'piecewise'</em>. If <code>None</code>, it will default to 'False'.<br/>
<img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This parameter is deprecated and it
 will be removed in future releases. Please, use the
 <code>embed_continuous_method</code> parameter instead.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dim</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the continuous embeddings. If the continuous columns are
embedded, <code>cont_embed_dim</code> must be passed.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the mlp.</p>
              </div>
            </td>
            <td>
                  <code>[200, 100]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float or List of floats with the dropout between the dense layers.
e.g: <em>[0.5,0.5]</em></p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm_last</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_linear_first</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>mlp model that will receive the concatenation of the embeddings and
the continuous columns</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabMlp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabMlp</span><span class="p">(</span><span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/tab_mlp.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TabMlp</span><span class="p">(</span><span class="n">BaseTabularModelWithoutAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a `TabMlp` model that can be used as the `deeptabular`</span>
<span class="sd">    component of a Wide &amp; Deep model or independently by itself.</span>

<span class="sd">    This class combines embedding representations of the categorical features</span>
<span class="sd">    with numerical (aka continuous) features, embedded or not. These are then</span>
<span class="sd">    passed through a series of dense layers (i.e. a MLP).</span>

<span class="sd">    Most of the parameters for this class are `Optional` since the use of</span>
<span class="sd">    categorical or continuous is in fact optional (i.e. one can use</span>
<span class="sd">    categorical features only, continuous features only or both).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name, number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11, 32), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if the continuous columns will be embedded using</span>
<span class="sd">        one of the available methods: _&#39;standard&#39;_, _&#39;periodic&#39;_</span>
<span class="sd">        or _&#39;piecewise&#39;_. If `None`, it will default to &#39;False&#39;.&lt;br/&gt;</span>
<span class="sd">        :information_source: **NOTE**: This parameter is deprecated and it</span>
<span class="sd">         will be removed in future releases. Please, use the</span>
<span class="sd">         `embed_continuous_method` parameter instead.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dim: int, Optional, default = None,</span>
<span class="sd">        Size of the continuous embeddings. If the continuous columns are</span>
<span class="sd">        embedded, `cont_embed_dim` must be passed.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    mlp_hidden_dims: List, default = [200, 100]</span>
<span class="sd">        List with the number of neurons per dense layer in the mlp.</span>
<span class="sd">    mlp_activation: str, default = &quot;relu&quot;</span>
<span class="sd">        Activation function for the dense layers of the MLP. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    mlp_dropout: float or List, default = 0.1</span>
<span class="sd">        float or List of floats with the dropout between the dense layers.</span>
<span class="sd">        e.g: _[0.5,0.5]_</span>
<span class="sd">    mlp_batchnorm: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">    mlp_batchnorm_last: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">    mlp_linear_first: bool, default = False</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: nn.Module</span>
<span class="sd">        mlp model that will receive the concatenation of the embeddings and</span>
<span class="sd">        the continuous columns</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import TabMlp</span>
<span class="sd">    &gt;&gt;&gt; X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u, i, j) for u, i, j in zip(colnames[:4], [4] * 4, [8] * 4)]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k: v for v, k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = TabMlp(mlp_hidden_dims=[8, 4], column_idx=column_idx, cat_embed_input=cat_embed_input,</span>
<span class="sd">    ... continuous_cols=[&quot;e&quot;])</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_tab)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
        <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabMlp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
            <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>
        <span class="c1"># Mlp</span>
        <span class="n">mlp_input_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_out_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_out_dim</span>
        <span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">mlp_input_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">mlp_activation</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
            <span class="n">mlp_linear_first</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabMlpDecoder</span>


<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Companion decoder model for the <code>TabMlp</code> model (which can be considered
an encoder itself).</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). The <code>TabMlpDecoder</code> will receive the output from the MLP
and '<em>reconstruct</em>' the embeddings.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the embeddings tensor that needs to be reconstructed.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the mlp.</p>
              </div>
            </td>
            <td>
                  <code>[100, 200]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float or List of floats with the dropout between the dense layers.
e.g: <em>[0.5,0.5]</em></p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm_last</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_linear_first</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder.decoder">decoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>mlp model that will receive the output of the encoder</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabMlpDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabMlpDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/tab_mlp.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TabMlpDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Companion decoder model for the `TabMlp` model (which can be considered</span>
<span class="sd">    an encoder itself).</span>

<span class="sd">    This class is designed to be used with the `EncoderDecoderTrainer` when</span>
<span class="sd">    using self-supervised pre-training (see the corresponding section in the</span>
<span class="sd">    docs). The `TabMlpDecoder` will receive the output from the MLP</span>
<span class="sd">    and &#39;_reconstruct_&#39; the embeddings.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    embed_dim: int</span>
<span class="sd">        Size of the embeddings tensor that needs to be reconstructed.</span>
<span class="sd">    mlp_hidden_dims: List, default = [200, 100]</span>
<span class="sd">        List with the number of neurons per dense layer in the mlp.</span>
<span class="sd">    mlp_activation: str, default = &quot;relu&quot;</span>
<span class="sd">        Activation function for the dense layers of the MLP. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    mlp_dropout: float or List, default = 0.1</span>
<span class="sd">        float or List of floats with the dropout between the dense layers.</span>
<span class="sd">        e.g: _[0.5,0.5]_</span>
<span class="sd">    mlp_batchnorm: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">    mlp_batchnorm_last: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">    mlp_linear_first: bool, default = False</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    decoder: nn.Module</span>
<span class="sd">        mlp model that will receive the output of the encoder</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import TabMlpDecoder</span>
<span class="sd">    &gt;&gt;&gt; x_inp = torch.rand(3, 8)</span>
<span class="sd">    &gt;&gt;&gt; decoder = TabMlpDecoder(embed_dim=32, mlp_hidden_dims=[8,16])</span>
<span class="sd">    &gt;&gt;&gt; res = decoder(x_inp)</span>
<span class="sd">    &gt;&gt;&gt; res.shape</span>
<span class="sd">    torch.Size([3, 32])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
        <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabMlpDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">mlp_hidden_dims</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">],</span>
            <span class="n">mlp_activation</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
            <span class="n">mlp_linear_first</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabResnet</span>


<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>


        <p>Defines a <code>TabResnet</code> model that can be used as the <code>deeptabular</code>
component of a Wide &amp; Deep model or independently by itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features, embedded or not. These are then
passed through a series of Resnet blocks. See
<code>pytorch_widedeep.models.tab_resnet._layers</code> for details on the
structure of each block.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the continuous columns will be embedded using
one of the available methods: <em>'standard'</em>, <em>'periodic'</em>
or <em>'piecewise'</em>. If <code>None</code>, it will default to 'False'.<br/>
<img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This parameter is deprecated and it
 will be removed in future releases. Please, use the
 <code>embed_continuous_method</code> parameter instead.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dim</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the continuous embeddings. If the continuous columns are
embedded, <code>cont_embed_dim</code> must be passed.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>blocks_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of integers that define the input and output units of each block.
For example: <em>[200, 100, 100]</em> will generate 2 blocks. The first will
receive a tensor of size 200 and output a tensor of size 100, and the
second will receive a tensor of size 100 and output a tensor of size
100. See <code>pytorch_widedeep.models.tab_resnet._layers</code> for
details on the structure of each block.</p>
              </div>
            </td>
            <td>
                  <code>[200, 100, 100]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>blocks_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Block's internal dropout.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>simplify_blocks</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the simplest possible residual blocks (<code>X -&gt; [
[LIN, BN, ACT]  + X ]</code>) will be used instead of a standard one
(<code>X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]</code>).</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If <code>None</code> the  output of the Resnet Blocks will be
connected directly to the output neuron(s).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm_last</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_linear_first</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>deep dense Resnet model that will receive the concatenation of the
embeddings and the continuous columns</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.mlp">mlp</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>mlp_hidden_dims</code> is <code>True</code>, this attribute will be an mlp
model that will receive the results of the concatenation of the
embeddings and the continuous columns -- if present --.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_deep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabResnet</span><span class="p">(</span><span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_deep</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/resnet/tab_resnet.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TabResnet</span><span class="p">(</span><span class="n">BaseTabularModelWithoutAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a `TabResnet` model that can be used as the `deeptabular`</span>
<span class="sd">    component of a Wide &amp; Deep model or independently by itself.</span>

<span class="sd">    This class combines embedding representations of the categorical features</span>
<span class="sd">    with numerical (aka continuous) features, embedded or not. These are then</span>
<span class="sd">    passed through a series of Resnet blocks. See</span>
<span class="sd">    `pytorch_widedeep.models.tab_resnet._layers` for details on the</span>
<span class="sd">    structure of each block.</span>

<span class="sd">    Most of the parameters for this class are `Optional` since the use of</span>
<span class="sd">    categorical or continuous is in fact optional (i.e. one can use</span>
<span class="sd">    categorical features only, continuous features only or both).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name, number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11, 32), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if the continuous columns will be embedded using</span>
<span class="sd">        one of the available methods: _&#39;standard&#39;_, _&#39;periodic&#39;_</span>
<span class="sd">        or _&#39;piecewise&#39;_. If `None`, it will default to &#39;False&#39;.&lt;br/&gt;</span>
<span class="sd">        :information_source: **NOTE**: This parameter is deprecated and it</span>
<span class="sd">         will be removed in future releases. Please, use the</span>
<span class="sd">         `embed_continuous_method` parameter instead.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dim: int, Optional, default = None,</span>
<span class="sd">        Size of the continuous embeddings. If the continuous columns are</span>
<span class="sd">        embedded, `cont_embed_dim` must be passed.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    blocks_dims: List, default = [200, 100, 100]</span>
<span class="sd">        List of integers that define the input and output units of each block.</span>
<span class="sd">        For example: _[200, 100, 100]_ will generate 2 blocks. The first will</span>
<span class="sd">        receive a tensor of size 200 and output a tensor of size 100, and the</span>
<span class="sd">        second will receive a tensor of size 100 and output a tensor of size</span>
<span class="sd">        100. See `pytorch_widedeep.models.tab_resnet._layers` for</span>
<span class="sd">        details on the structure of each block.</span>
<span class="sd">    blocks_dropout: float, default =  0.1</span>
<span class="sd">        Block&#39;s internal dropout.</span>
<span class="sd">    simplify_blocks: bool, default = False,</span>
<span class="sd">        Boolean indicating if the simplest possible residual blocks (`X -&gt; [</span>
<span class="sd">        [LIN, BN, ACT]  + X ]`) will be used instead of a standard one</span>
<span class="sd">        (`X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]`).</span>
<span class="sd">    mlp_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the number of neurons per dense layer in the MLP. e.g:</span>
<span class="sd">        _[64, 32]_. If `None` the  output of the Resnet Blocks will be</span>
<span class="sd">        connected directly to the output neuron(s).</span>
<span class="sd">    mlp_activation: str, Optional, default = None</span>
<span class="sd">        Activation function for the dense layers of the MLP. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky&#39;_relu&#39; and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to _&#39;relu&#39;_.</span>
<span class="sd">    mlp_dropout: float, Optional, default = None</span>
<span class="sd">        float with the dropout between the dense layers of the MLP.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to 0.0.</span>
<span class="sd">    mlp_batchnorm: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_batchnorm_last: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_linear_first: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to `True`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: nn.Module</span>
<span class="sd">        deep dense Resnet model that will receive the concatenation of the</span>
<span class="sd">        embeddings and the continuous columns</span>
<span class="sd">    mlp: nn.Module</span>
<span class="sd">        if `mlp_hidden_dims` is `True`, this attribute will be an mlp</span>
<span class="sd">        model that will receive the results of the concatenation of the</span>
<span class="sd">        embeddings and the continuous columns -- if present --.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import TabResnet</span>
<span class="sd">    &gt;&gt;&gt; X_deep = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u,i,j) for u,i,j in zip(colnames[:4], [4]*4, [8]*4)]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k:v for v,k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = TabResnet(blocks_dims=[16,4], column_idx=column_idx, cat_embed_input=cat_embed_input,</span>
<span class="sd">    ... continuous_cols = [&#39;e&#39;])</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_deep)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">blocks_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
        <span class="n">blocks_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">simplify_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabResnet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
            <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;&#39;blocks&#39; must contain at least two elements, e.g. [256, 128]&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span> <span class="o">=</span> <span class="n">blocks_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dropout</span> <span class="o">=</span> <span class="n">blocks_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span> <span class="o">=</span> <span class="n">simplify_blocks</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>

        <span class="c1"># Resnet</span>
        <span class="n">dense_resnet_input_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_out_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
            <span class="n">dense_resnet_input_dim</span><span class="p">,</span> <span class="n">blocks_dims</span><span class="p">,</span> <span class="n">blocks_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span>
        <span class="p">)</span>

        <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
        <span class="c1"># therefore all related params are optional</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
                <span class="p">),</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
                <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
                <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
                <span class="p">),</span>
                <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabResnetDecoder</span>


<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Companion decoder model for the <code>TabResnet</code> model (which can be
considered an encoder itself)</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). This class will receive the output from the ResNet blocks or the
MLP(if present) and '<em>reconstruct</em>' the embeddings.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the embeddings tensor to be reconstructed.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>blocks_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of integers that define the input and output units of each block.
For example: <em>[200, 100, 100]</em> will generate 2 blocks. The first will
receive a tensor of size 200 and output a tensor of size 100, and the
second will receive a tensor of size 100 and output a tensor of size
100. See <code>pytorch_widedeep.models.tab_resnet._layers</code> for
details on the structure of each block.</p>
              </div>
            </td>
            <td>
                  <code>[100, 100, 200]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>blocks_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Block's internal dropout.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>simplify_blocks</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the simplest possible residual blocks (<code>X -&gt; [
[LIN, BN, ACT]  + X ]</code>) will be used instead of a standard one
(<code>X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]</code>).</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If <code>None</code> the  output of the Resnet Blocks will be
connected directly to the output neuron(s).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm_last</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_linear_first</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder.decoder">decoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>deep dense Resnet model that will receive the output of the encoder IF
<code>mlp_hidden_dims</code> is None</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder.mlp">mlp</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if <code>mlp_hidden_dims</code> is not None, the overall decoder will consist
in an MLP that will receive the output of the encoder followed by the
deep dense Resnet.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnetDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabResnetDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/resnet/tab_resnet.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TabResnetDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Companion decoder model for the `TabResnet` model (which can be</span>
<span class="sd">    considered an encoder itself)</span>

<span class="sd">    This class is designed to be used with the `EncoderDecoderTrainer` when</span>
<span class="sd">    using self-supervised pre-training (see the corresponding section in the</span>
<span class="sd">    docs). This class will receive the output from the ResNet blocks or the</span>
<span class="sd">    MLP(if present) and &#39;_reconstruct_&#39; the embeddings.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    embed_dim: int</span>
<span class="sd">        Size of the embeddings tensor to be reconstructed.</span>
<span class="sd">    blocks_dims: List, default = [200, 100, 100]</span>
<span class="sd">        List of integers that define the input and output units of each block.</span>
<span class="sd">        For example: _[200, 100, 100]_ will generate 2 blocks. The first will</span>
<span class="sd">        receive a tensor of size 200 and output a tensor of size 100, and the</span>
<span class="sd">        second will receive a tensor of size 100 and output a tensor of size</span>
<span class="sd">        100. See `pytorch_widedeep.models.tab_resnet._layers` for</span>
<span class="sd">        details on the structure of each block.</span>
<span class="sd">    blocks_dropout: float, default =  0.1</span>
<span class="sd">        Block&#39;s internal dropout.</span>
<span class="sd">    simplify_blocks: bool, default = False,</span>
<span class="sd">        Boolean indicating if the simplest possible residual blocks (`X -&gt; [</span>
<span class="sd">        [LIN, BN, ACT]  + X ]`) will be used instead of a standard one</span>
<span class="sd">        (`X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]`).</span>
<span class="sd">    mlp_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the number of neurons per dense layer in the MLP. e.g:</span>
<span class="sd">        _[64, 32]_. If `None` the  output of the Resnet Blocks will be</span>
<span class="sd">        connected directly to the output neuron(s).</span>
<span class="sd">    mlp_activation: str, Optional, default = None</span>
<span class="sd">        Activation function for the dense layers of the MLP. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky&#39;_relu&#39; and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to _&#39;relu&#39;_.</span>
<span class="sd">    mlp_dropout: float, Optional, default = None</span>
<span class="sd">        float with the dropout between the dense layers of the MLP.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to 0.0.</span>
<span class="sd">    mlp_batchnorm: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_batchnorm_last: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_linear_first: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to `True`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    decoder: nn.Module</span>
<span class="sd">        deep dense Resnet model that will receive the output of the encoder IF</span>
<span class="sd">        `mlp_hidden_dims` is None</span>
<span class="sd">    mlp: nn.Module</span>
<span class="sd">        if `mlp_hidden_dims` is not None, the overall decoder will consist</span>
<span class="sd">        in an MLP that will receive the output of the encoder followed by the</span>
<span class="sd">        deep dense Resnet.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import TabResnetDecoder</span>
<span class="sd">    &gt;&gt;&gt; x_inp = torch.rand(3, 8)</span>
<span class="sd">    &gt;&gt;&gt; decoder = TabResnetDecoder(embed_dim=32, blocks_dims=[8, 16, 16])</span>
<span class="sd">    &gt;&gt;&gt; res = decoder(x_inp)</span>
<span class="sd">    &gt;&gt;&gt; res.shape</span>
<span class="sd">    torch.Size([3, 32])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">blocks_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
        <span class="n">blocks_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">simplify_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabResnetDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;&#39;blocks&#39; must contain at least two elements, e.g. [256, 128]&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span> <span class="o">=</span> <span class="n">blocks_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dropout</span> <span class="o">=</span> <span class="n">blocks_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span> <span class="o">=</span> <span class="n">simplify_blocks</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
                <span class="p">),</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
                <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
                <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
                <span class="p">),</span>
                <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">blocks_dims</span><span class="p">,</span>
                <span class="n">blocks_dropout</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
                <span class="n">blocks_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">blocks_dims</span><span class="p">,</span> <span class="n">blocks_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">X</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabNet</span>


<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>


        <p>Defines a <a href="https://arxiv.org/abs/1908.07442">TabNet model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>The implementation in this library is fully based on that
<a href="https://github.com/dreamquark-ai/tabnet">here</a> by the dreamquark-ai team,
simply adapted so that it can work within the <code>WideDeep</code> frame.
Therefore, <strong>ALL CREDIT TO THE DREAMQUARK-AI TEAM</strong>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the continuous columns will be embedded using
one of the available methods: <em>'standard'</em>, <em>'periodic'</em>
or <em>'piecewise'</em>. If <code>None</code>, it will default to 'False'.<br/>
<img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This parameter is deprecated and it
 will be removed in future releases. Please, use the
 <code>embed_continuous_method</code> parameter instead.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dim</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the continuous embeddings. If the continuous columns are
embedded, <code>cont_embed_dim</code> must be passed.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_steps</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of decision steps. For a better understanding of the function
of <code>n_steps</code> and the upcoming parameters, please see the
<a href="https://arxiv.org/abs/1908.07442">paper</a>.</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>step_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Step's output dimension. This is the output dimension that
<code>WideDeep</code> will collect and connect to the output neuron(s).</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Attention dimension</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>GLU block's internal dropout</p>
              </div>
            </td>
            <td>
                  <code>0.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_glu_step_dependent</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that are step dependent</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_glu_shared</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that will be shared
across decision steps</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ghost_bn</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if <a href="https://arxiv.org/abs/1705.08741">Ghost Batch Normalization</a>
will be used.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>virtual_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Batch size when using Ghost Batch Normalization</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>momentum</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Ghost Batch Normalization's momentum. The dreamquark-ai advises for
very low values. However high values are used in the original
publication. During our tests higher values lead to better results</p>
              </div>
            </td>
            <td>
                  <code>0.02</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gamma</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Relaxation parameter in the paper. When gamma = 1, a feature is
enforced to be used only at one decision step. As gamma increases,
more flexibility is provided to use a feature at multiple decision
steps</p>
              </div>
            </td>
            <td>
                  <code>1.3</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>epsilon</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Float to avoid log(0). Always keep low</p>
              </div>
            </td>
            <td>
                  <code>1e-15</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_type</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Mask function to use. Either <em>'sparsemax'</em> or <em>'entmax'</em></p>
              </div>
            </td>
            <td>
                  <code>&#39;sparsemax&#39;</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the TabNet encoder. For details see the <a href="https://arxiv.org/abs/1908.07442">original publication</a>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabNet</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/tabnet/tab_net.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TabNet</span><span class="p">(</span><span class="n">BaseTabularModelWithoutAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a [TabNet model](https://arxiv.org/abs/1908.07442) that</span>
<span class="sd">    can be used as the `deeptabular` component of a Wide &amp; Deep model or</span>
<span class="sd">    independently by itself.</span>

<span class="sd">    The implementation in this library is fully based on that</span>
<span class="sd">    [here](https://github.com/dreamquark-ai/tabnet) by the dreamquark-ai team,</span>
<span class="sd">    simply adapted so that it can work within the `WideDeep` frame.</span>
<span class="sd">    Therefore, **ALL CREDIT TO THE DREAMQUARK-AI TEAM**.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name, number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11, 32), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if the continuous columns will be embedded using</span>
<span class="sd">        one of the available methods: _&#39;standard&#39;_, _&#39;periodic&#39;_</span>
<span class="sd">        or _&#39;piecewise&#39;_. If `None`, it will default to &#39;False&#39;.&lt;br/&gt;</span>
<span class="sd">        :information_source: **NOTE**: This parameter is deprecated and it</span>
<span class="sd">         will be removed in future releases. Please, use the</span>
<span class="sd">         `embed_continuous_method` parameter instead.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dim: int, Optional, default = None,</span>
<span class="sd">        Size of the continuous embeddings. If the continuous columns are</span>
<span class="sd">        embedded, `cont_embed_dim` must be passed.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    n_steps: int, default = 3</span>
<span class="sd">        number of decision steps. For a better understanding of the function</span>
<span class="sd">        of `n_steps` and the upcoming parameters, please see the</span>
<span class="sd">        [paper](https://arxiv.org/abs/1908.07442).</span>
<span class="sd">    step_dim: int, default = 8</span>
<span class="sd">        Step&#39;s output dimension. This is the output dimension that</span>
<span class="sd">        `WideDeep` will collect and connect to the output neuron(s).</span>
<span class="sd">    attn_dim: int, default = 8</span>
<span class="sd">        Attention dimension</span>
<span class="sd">    dropout: float, default = 0.0</span>
<span class="sd">        GLU block&#39;s internal dropout</span>
<span class="sd">    n_glu_step_dependent: int, default = 2</span>
<span class="sd">        number of GLU Blocks (`[FC -&gt; BN -&gt; GLU]`) that are step dependent</span>
<span class="sd">    n_glu_shared: int, default = 2</span>
<span class="sd">        number of GLU Blocks (`[FC -&gt; BN -&gt; GLU]`) that will be shared</span>
<span class="sd">        across decision steps</span>
<span class="sd">    ghost_bn: bool, default=True</span>
<span class="sd">        Boolean indicating if [Ghost Batch Normalization](https://arxiv.org/abs/1705.08741)</span>
<span class="sd">        will be used.</span>
<span class="sd">    virtual_batch_size: int, default = 128</span>
<span class="sd">        Batch size when using Ghost Batch Normalization</span>
<span class="sd">    momentum: float, default = 0.02</span>
<span class="sd">        Ghost Batch Normalization&#39;s momentum. The dreamquark-ai advises for</span>
<span class="sd">        very low values. However high values are used in the original</span>
<span class="sd">        publication. During our tests higher values lead to better results</span>
<span class="sd">    gamma: float, default = 1.3</span>
<span class="sd">        Relaxation parameter in the paper. When gamma = 1, a feature is</span>
<span class="sd">        enforced to be used only at one decision step. As gamma increases,</span>
<span class="sd">        more flexibility is provided to use a feature at multiple decision</span>
<span class="sd">        steps</span>
<span class="sd">    epsilon: float, default = 1e-15</span>
<span class="sd">        Float to avoid log(0). Always keep low</span>
<span class="sd">    mask_type: str, default = &quot;sparsemax&quot;</span>
<span class="sd">        Mask function to use. Either _&#39;sparsemax&#39;_ or _&#39;entmax&#39;_</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: nn.Module</span>
<span class="sd">        the TabNet encoder. For details see the [original publication](https://arxiv.org/abs/1908.07442).</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u, i, j) for u, i, j in zip(colnames[:4], [4] * 4, [8] * 4)]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k: v for v, k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = TabNet(column_idx=column_idx, cat_embed_input=cat_embed_input, continuous_cols=[&quot;e&quot;])</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_tab)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">step_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">attn_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">n_glu_step_dependent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">n_glu_shared</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">ghost_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">virtual_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.3</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-15</span><span class="p">,</span>
        <span class="n">mask_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sparsemax&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
            <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span> <span class="o">=</span> <span class="n">step_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dim</span> <span class="o">=</span> <span class="n">attn_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_step_dependent</span> <span class="o">=</span> <span class="n">n_glu_step_dependent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_shared</span> <span class="o">=</span> <span class="n">n_glu_shared</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ghost_bn</span> <span class="o">=</span> <span class="n">ghost_bn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">virtual_batch_size</span> <span class="o">=</span> <span class="n">virtual_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask_type</span> <span class="o">=</span> <span class="n">mask_type</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_out_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_out_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_out_dim</span>

        <span class="c1"># TabNet</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TabNetEncoder</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_out_dim</span><span class="p">,</span>
            <span class="n">n_steps</span><span class="p">,</span>
            <span class="n">step_dim</span><span class="p">,</span>
            <span class="n">attn_dim</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
            <span class="n">n_glu_step_dependent</span><span class="p">,</span>
            <span class="n">n_glu_shared</span><span class="p">,</span>
            <span class="n">ghost_bn</span><span class="p">,</span>
            <span class="n">virtual_batch_size</span><span class="p">,</span>
            <span class="n">momentum</span><span class="p">,</span>
            <span class="n">gamma</span><span class="p">,</span>
            <span class="n">epsilon</span><span class="p">,</span>
            <span class="n">mask_type</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">prior</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">steps_output</span><span class="p">,</span> <span class="n">M_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">steps_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">M_loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward_masks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward_masks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabNetDecoder</span>


<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Companion decoder model for the <code>TabNet</code> model (which can be
considered an encoder itself)</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). This class will receive the output from the <code>TabNet</code> encoder
(i.e. the output from the so called 'steps') and '<em>reconstruct</em>' the
embeddings.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the embeddings tensor to be reconstructed.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_steps</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of decision steps. For a better understanding of the function
of <code>n_steps</code> and the upcoming parameters, please see the
<a href="https://arxiv.org/abs/1908.07442">paper</a>.</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>step_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Step's output dimension. This is the output dimension that
<code>WideDeep</code> will collect and connect to the output neuron(s).</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>GLU block's internal dropout</p>
              </div>
            </td>
            <td>
                  <code>0.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_glu_step_dependent</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that are step dependent</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_glu_shared</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that will be shared
across decision steps</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ghost_bn</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if <a href="https://arxiv.org/abs/1705.08741">Ghost Batch Normalization</a>
will be used.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>virtual_batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Batch size when using Ghost Batch Normalization</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>momentum</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Ghost Batch Normalization's momentum. The dreamquark-ai advises for
very low values. However high values are used in the original
publication. During our tests higher values lead to better results</p>
              </div>
            </td>
            <td>
                  <code>0.02</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder.decoder">decoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>decoder that will receive the output from the encoder's steps and will
reconstruct the embeddings</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabNetDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabNetDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/tabnet/tab_net.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TabNetDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Companion decoder model for the `TabNet` model (which can be</span>
<span class="sd">    considered an encoder itself)</span>

<span class="sd">    This class is designed to be used with the `EncoderDecoderTrainer` when</span>
<span class="sd">    using self-supervised pre-training (see the corresponding section in the</span>
<span class="sd">    docs). This class will receive the output from the `TabNet` encoder</span>
<span class="sd">    (i.e. the output from the so called &#39;steps&#39;) and &#39;_reconstruct_&#39; the</span>
<span class="sd">    embeddings.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    embed_dim: int</span>
<span class="sd">        Size of the embeddings tensor to be reconstructed.</span>
<span class="sd">    n_steps: int, default = 3</span>
<span class="sd">        number of decision steps. For a better understanding of the function</span>
<span class="sd">        of `n_steps` and the upcoming parameters, please see the</span>
<span class="sd">        [paper](https://arxiv.org/abs/1908.07442).</span>
<span class="sd">    step_dim: int, default = 8</span>
<span class="sd">        Step&#39;s output dimension. This is the output dimension that</span>
<span class="sd">        `WideDeep` will collect and connect to the output neuron(s).</span>
<span class="sd">    dropout: float, default = 0.0</span>
<span class="sd">        GLU block&#39;s internal dropout</span>
<span class="sd">    n_glu_step_dependent: int, default = 2</span>
<span class="sd">        number of GLU Blocks (`[FC -&gt; BN -&gt; GLU]`) that are step dependent</span>
<span class="sd">    n_glu_shared: int, default = 2</span>
<span class="sd">        number of GLU Blocks (`[FC -&gt; BN -&gt; GLU]`) that will be shared</span>
<span class="sd">        across decision steps</span>
<span class="sd">    ghost_bn: bool, default=True</span>
<span class="sd">        Boolean indicating if [Ghost Batch Normalization](https://arxiv.org/abs/1705.08741)</span>
<span class="sd">        will be used.</span>
<span class="sd">    virtual_batch_size: int, default = 128</span>
<span class="sd">        Batch size when using Ghost Batch Normalization</span>
<span class="sd">    momentum: float, default = 0.02</span>
<span class="sd">        Ghost Batch Normalization&#39;s momentum. The dreamquark-ai advises for</span>
<span class="sd">        very low values. However high values are used in the original</span>
<span class="sd">        publication. During our tests higher values lead to better results</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    decoder: nn.Module</span>
<span class="sd">        decoder that will receive the output from the encoder&#39;s steps and will</span>
<span class="sd">        reconstruct the embeddings</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import TabNetDecoder</span>
<span class="sd">    &gt;&gt;&gt; x_inp = [torch.rand(3, 8), torch.rand(3, 8), torch.rand(3, 8)]</span>
<span class="sd">    &gt;&gt;&gt; decoder = TabNetDecoder(embed_dim=32, ghost_bn=False)</span>
<span class="sd">    &gt;&gt;&gt; res = decoder(x_inp)</span>
<span class="sd">    &gt;&gt;&gt; res.shape</span>
<span class="sd">    torch.Size([3, 32])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">step_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">n_glu_step_dependent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">n_glu_shared</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">ghost_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">virtual_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabNetDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span> <span class="o">=</span> <span class="n">step_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_step_dependent</span> <span class="o">=</span> <span class="n">n_glu_step_dependent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_shared</span> <span class="o">=</span> <span class="n">n_glu_shared</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ghost_bn</span> <span class="o">=</span> <span class="n">ghost_bn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">virtual_batch_size</span> <span class="o">=</span> <span class="n">virtual_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>

        <span class="n">shared_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_glu_shared</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
            <span class="n">transformer</span> <span class="o">=</span> <span class="n">FeatTransformer</span><span class="p">(</span>
                <span class="n">step_dim</span><span class="p">,</span>
                <span class="n">step_dim</span><span class="p">,</span>
                <span class="n">dropout</span><span class="p">,</span>
                <span class="n">shared_layers</span><span class="p">,</span>
                <span class="n">n_glu_step_dependent</span><span class="p">,</span>
                <span class="n">ghost_bn</span><span class="p">,</span>
                <span class="n">virtual_batch_size</span><span class="p">,</span>
                <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">initialize_non_glu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span><span class="p">,</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ContextAttentionMLP</span>


<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


        <p>Defines a <code>ContextAttentionMLP</code> model that can be used as the
<code>deeptabular</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features that are also embedded. These
are then passed through a series of attention blocks. Each attention
block is comprised by a <code>ContextAttentionEncoder</code>. Such encoder is in
part inspired by the attention mechanism described in
<a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf">Hierarchical Attention Networks for Document
Classification</a>.
See <code>pytorch_widedeep.models.tabular.mlp._attention_layers</code> for details.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>frac_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>&#39;standard&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The so-called <em>dimension of the model</em>. Is the number of embeddings
used to encode the categorical and/or continuous columns</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for each attention block</p>
              </div>
            </td>
            <td>
                  <code>0.2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>with_addnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if residual connections will be used in the
attention blocks</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>String indicating the activation function to be applied to the dense
layer in each attention encoder. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em>
and <em>'gelu'</em> are supported.</p>
              </div>
            </td>
            <td>
                  <code>&#39;leaky_relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_blocks</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention blocks</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Sequence of attention encoders.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">ContextAttentionMLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ContextAttentionMLP</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/context_attention_mlp.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ContextAttentionMLP</span><span class="p">(</span><span class="n">BaseTabularModelWithAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a `ContextAttentionMLP` model that can be used as the</span>
<span class="sd">    `deeptabular` component of a Wide &amp; Deep model or independently by</span>
<span class="sd">    itself.</span>

<span class="sd">    This class combines embedding representations of the categorical features</span>
<span class="sd">    with numerical (aka continuous) features that are also embedded. These</span>
<span class="sd">    are then passed through a series of attention blocks. Each attention</span>
<span class="sd">    block is comprised by a `ContextAttentionEncoder`. Such encoder is in</span>
<span class="sd">    part inspired by the attention mechanism described in</span>
<span class="sd">    [Hierarchical Attention Networks for Document</span>
<span class="sd">    Classification](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf).</span>
<span class="sd">    See `pytorch_widedeep.models.tabular.mlp._attention_layers` for details.</span>

<span class="sd">    Most of the parameters for this class are `Optional` since the use of</span>
<span class="sd">    categorical or continuous is in fact optional (i.e. one can use</span>
<span class="sd">    categorical features only, continuous features only or both).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name and number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    shared_embed: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating if the embeddings will be &quot;shared&quot;. The idea behind `shared_embed` is</span>
<span class="sd">        described in the Appendix A in the [TabTransformer paper](https://arxiv.org/abs/2012.06678):</span>
<span class="sd">        _&#39;The goal of having column embedding is to enable the model to</span>
<span class="sd">        distinguish the classes in one column from those in the other</span>
<span class="sd">        columns&#39;_. In other words, the idea is to let the model learn which</span>
<span class="sd">        column is embedded at the time. See: `pytorch_widedeep.models.transformers._layers.SharedEmbeddings`.</span>
<span class="sd">    add_shared_embed: bool, Optional, default = None</span>
<span class="sd">        The two embedding sharing strategies are: 1) add the shared embeddings</span>
<span class="sd">        to the column embeddings or 2) to replace the first</span>
<span class="sd">        `frac_shared_embed` with the shared embeddings.</span>
<span class="sd">        See `pytorch_widedeep.models.embeddings_layers.SharedEmbeddings`</span>
<span class="sd">        If &#39;None&#39; is passed, it will default to &#39;False&#39;.</span>
<span class="sd">    frac_shared_embed: float, Optional, default = None</span>
<span class="sd">        The fraction of embeddings that will be shared (if `add_shared_embed</span>
<span class="sd">        = False`) by all the different categories for one particular</span>
<span class="sd">        column. If &#39;None&#39; is passed, it will default to 0.0.</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    input_dim: int, default = 32</span>
<span class="sd">        The so-called *dimension of the model*. Is the number of embeddings</span>
<span class="sd">        used to encode the categorical and/or continuous columns</span>
<span class="sd">    attn_dropout: float, default = 0.2</span>
<span class="sd">        Dropout for each attention block</span>
<span class="sd">    with_addnorm: bool = False,</span>
<span class="sd">        Boolean indicating if residual connections will be used in the</span>
<span class="sd">        attention blocks</span>
<span class="sd">    attn_activation: str, default = &quot;leaky_relu&quot;</span>
<span class="sd">        String indicating the activation function to be applied to the dense</span>
<span class="sd">        layer in each attention encoder. _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_</span>
<span class="sd">        and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">    n_blocks: int, default = 3</span>
<span class="sd">        Number of attention blocks</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: nn.Module</span>
<span class="sd">        Sequence of attention encoders.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import ContextAttentionMLP</span>
<span class="sd">    &gt;&gt;&gt; X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u,i,j) for u,i,j in zip(colnames[:4], [4]*4, [8]*4)]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k:v for v,k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = ContextAttentionMLP(column_idx=column_idx, cat_embed_input=cat_embed_input, continuous_cols = [&#39;e&#39;])</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_tab)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">attn_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ContextAttentionMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
            <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
            <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_activation</span> <span class="o">=</span> <span class="n">attn_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>
        <span class="c1"># Attention Blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;attention_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">ContextAttentionEncoder</span><span class="p">(</span>
                    <span class="n">input_dim</span><span class="p">,</span>
                    <span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="n">with_addnorm</span><span class="p">,</span>
                    <span class="n">attn_activation</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span>
            <span class="k">else</span> <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">        The shape of the attention weights is $(N, F)$, where $N$ is the batch</span>
<span class="sd">        size and $F$ is the number of features/columns in the dataset</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(F\)</span> is the number of features/columns in the dataset</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">SelfAttentionMLP</span>


<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


        <p>Defines a <code>SelfAttentionMLP</code> model that can be used as the
deeptabular component of a Wide &amp; Deep model or independently by
itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features that are also embedded. These
are then passed through a series of attention blocks. Each attention
block is comprised by what we would refer as a simplified
<code>SelfAttentionEncoder</code>. See
<code>pytorch_widedeep.models.tabular.mlp._attention_layers</code> for details. The
reason to use a simplified version of self attention is because we
observed that the '<em>standard</em>' attention mechanism used in the
TabTransformer has a notable tendency to overfit.</p>
<p>In more detail, this model only uses Q and K (and not V). If we think
about it as in terms of text (and intuitively), the Softmax(QK^T) is the
attention mechanism that tells us how much, at each position in the input
sentence, each word is represented or 'expressed'. We refer to that
as 'attention weights'. These attention weighst are normally multiplied
by a Value matrix to further strength the focus on the words that each
word should be attending to (again, intuitively).</p>
<p>In this implementation we skip this last multiplication and instead we
multiply the attention weights directly by the input tensor. This is a
simplification that we expect is beneficial in terms of avoiding
overfitting for tabular data.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>frac_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>&#39;standard&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for each attention block</p>
              </div>
            </td>
            <td>
                  <code>0.2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_heads</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads per attention block.</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to use bias in the Q, K projection
layers.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>with_addnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if residual connections will be used in the attention blocks</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>String indicating the activation function to be applied to the dense
layer in each attention encoder. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em>
and <em>'gelu'</em> are supported.</p>
              </div>
            </td>
            <td>
                  <code>&#39;leaky_relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_blocks</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention blocks</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.cat_and_cont_embed">cat_and_cont_embed</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the module that processes the categorical and continuous columns</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Sequence of attention encoders.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">SelfAttentionMLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SelfAttentionMLP</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/self_attention_mlp.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SelfAttentionMLP</span><span class="p">(</span><span class="n">BaseTabularModelWithAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a `SelfAttentionMLP` model that can be used as the</span>
<span class="sd">    deeptabular component of a Wide &amp; Deep model or independently by</span>
<span class="sd">    itself.</span>

<span class="sd">    This class combines embedding representations of the categorical features</span>
<span class="sd">    with numerical (aka continuous) features that are also embedded. These</span>
<span class="sd">    are then passed through a series of attention blocks. Each attention</span>
<span class="sd">    block is comprised by what we would refer as a simplified</span>
<span class="sd">    `SelfAttentionEncoder`. See</span>
<span class="sd">    `pytorch_widedeep.models.tabular.mlp._attention_layers` for details. The</span>
<span class="sd">    reason to use a simplified version of self attention is because we</span>
<span class="sd">    observed that the &#39;_standard_&#39; attention mechanism used in the</span>
<span class="sd">    TabTransformer has a notable tendency to overfit.</span>

<span class="sd">    In more detail, this model only uses Q and K (and not V). If we think</span>
<span class="sd">    about it as in terms of text (and intuitively), the Softmax(QK^T) is the</span>
<span class="sd">    attention mechanism that tells us how much, at each position in the input</span>
<span class="sd">    sentence, each word is represented or &#39;expressed&#39;. We refer to that</span>
<span class="sd">    as &#39;attention weights&#39;. These attention weighst are normally multiplied</span>
<span class="sd">    by a Value matrix to further strength the focus on the words that each</span>
<span class="sd">    word should be attending to (again, intuitively).</span>

<span class="sd">    In this implementation we skip this last multiplication and instead we</span>
<span class="sd">    multiply the attention weights directly by the input tensor. This is a</span>
<span class="sd">    simplification that we expect is beneficial in terms of avoiding</span>
<span class="sd">    overfitting for tabular data.</span>

<span class="sd">    Most of the parameters for this class are `Optional` since the use of</span>
<span class="sd">    categorical or continuous is in fact optional (i.e. one can use</span>
<span class="sd">    categorical features only, continuous features only or both).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name and number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    shared_embed: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating if the embeddings will be &quot;shared&quot;. The idea behind `shared_embed` is</span>
<span class="sd">        described in the Appendix A in the [TabTransformer paper](https://arxiv.org/abs/2012.06678):</span>
<span class="sd">        _&#39;The goal of having column embedding is to enable the model to</span>
<span class="sd">        distinguish the classes in one column from those in the other</span>
<span class="sd">        columns&#39;_. In other words, the idea is to let the model learn which</span>
<span class="sd">        column is embedded at the time. See: `pytorch_widedeep.models.transformers._layers.SharedEmbeddings`.</span>
<span class="sd">    add_shared_embed: bool, Optional, default = None</span>
<span class="sd">        The two embedding sharing strategies are: 1) add the shared embeddings</span>
<span class="sd">        to the column embeddings or 2) to replace the first</span>
<span class="sd">        `frac_shared_embed` with the shared embeddings.</span>
<span class="sd">        See `pytorch_widedeep.models.embeddings_layers.SharedEmbeddings`</span>
<span class="sd">        If &#39;None&#39; is passed, it will default to &#39;False&#39;.</span>
<span class="sd">    frac_shared_embed: float, Optional, default = None</span>
<span class="sd">        The fraction of embeddings that will be shared (if `add_shared_embed</span>
<span class="sd">        = False`) by all the different categories for one particular</span>
<span class="sd">        column. If &#39;None&#39; is passed, it will default to 0.0.</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    input_dim: int, default = 32</span>
<span class="sd">        The so-called *dimension of the model*. Is the number of</span>
<span class="sd">        embeddings used to encode the categorical and/or continuous columns</span>
<span class="sd">    attn_dropout: float, default = 0.2</span>
<span class="sd">        Dropout for each attention block</span>
<span class="sd">    n_heads: int, default = 8</span>
<span class="sd">        Number of attention heads per attention block.</span>
<span class="sd">    use_bias: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to use bias in the Q, K projection</span>
<span class="sd">        layers.</span>
<span class="sd">    with_addnorm: bool = False,</span>
<span class="sd">        Boolean indicating if residual connections will be used in the attention blocks</span>
<span class="sd">    attn_activation: str, default = &quot;leaky_relu&quot;</span>
<span class="sd">        String indicating the activation function to be applied to the dense</span>
<span class="sd">        layer in each attention encoder. _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_</span>
<span class="sd">        and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">    n_blocks: int, default = 3</span>
<span class="sd">        Number of attention blocks</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    cat_and_cont_embed: nn.Module</span>
<span class="sd">        This is the module that processes the categorical and continuous columns</span>
<span class="sd">    encoder: nn.Module</span>
<span class="sd">        Sequence of attention encoders.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import SelfAttentionMLP</span>
<span class="sd">    &gt;&gt;&gt; X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u,i,j) for u,i,j in zip(colnames[:4], [4]*4, [8]*4)]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k:v for v,k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = SelfAttentionMLP(column_idx=column_idx, cat_embed_input=cat_embed_input, continuous_cols = [&#39;e&#39;])</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_tab)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">attn_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttentionMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
            <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
            <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_activation</span> <span class="o">=</span> <span class="n">attn_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>
        <span class="c1"># Attention Blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;attention_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">SelfAttentionEncoder</span><span class="p">(</span>
                    <span class="n">input_dim</span><span class="p">,</span>
                    <span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="n">use_bias</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">,</span>
                    <span class="n">with_addnorm</span><span class="p">,</span>
                    <span class="n">attn_activation</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the WideDeep class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span>
            <span class="k">else</span> <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">        The shape of the attention weights is $(N, H, F, F)$, where $N$ is the</span>
<span class="sd">        batch size, $H$ is the number of attention heads and $F$ is the</span>
<span class="sd">        number of features/columns in the dataset</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the WideDeep class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabTransformer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


        <p>Defines our adptation of the
<a href="https://arxiv.org/abs/2012.06678">TabTransformer model</a>
that can be used as the <code>deeptabular</code> component of a
Wide &amp; Deep model or independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>:
This is an enhanced adaptation of the model described in the paper. It can
be considered as the flagship of our transformer family of models for
tabular data and offers mutiple, additional features relative to the
original publication(and some other models in the library)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>frac_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_heads</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads per Transformer block</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_qkv_bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_blocks</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of Transformer blocks</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the Multi-Head Attention layers</p>
              </div>
            </td>
            <td>
                  <code>0.2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the FeedForward network</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_factor</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;gelu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_linear_attention</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if Linear Attention (from <a href="https://arxiv.org/abs/2006.16236">Transformers are RNNs:
Fast Autoregressive Transformers with Linear Attention</a>)
will be used. The inclusing of this mode of attention is inspired by
<a href="https://www.uber.com/en-GB/blog/deepeta-how-uber-predicts-arrival-times/">this post</a>,
where the Uber team finds that this attention mechanism leads to the
best results for their tabular data.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_flash_attention</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if
<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">Flash Attention</a>
will be used. <br/></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm_last</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_linear_first</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Sequence of Transformer blocks</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.mlp">mlp</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>MLP component in the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabTransformer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_transformer.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TabTransformer</span><span class="p">(</span><span class="n">BaseTabularModelWithAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines our adptation of the</span>
<span class="sd">    [TabTransformer model](https://arxiv.org/abs/2012.06678)</span>
<span class="sd">    that can be used as the `deeptabular` component of a</span>
<span class="sd">    Wide &amp; Deep model or independently by itself.</span>

<span class="sd">    Most of the parameters for this class are `Optional` since the use of</span>
<span class="sd">    categorical or continuous is in fact optional (i.e. one can use</span>
<span class="sd">    categorical features only, continuous features only or both).</span>

<span class="sd">    :information_source: **NOTE**:</span>
<span class="sd">    This is an enhanced adaptation of the model described in the paper. It can</span>
<span class="sd">    be considered as the flagship of our transformer family of models for</span>
<span class="sd">    tabular data and offers mutiple, additional features relative to the</span>
<span class="sd">    original publication(and some other models in the library)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name and number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    shared_embed: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating if the embeddings will be &quot;shared&quot;. The idea behind `shared_embed` is</span>
<span class="sd">        described in the Appendix A in the [TabTransformer paper](https://arxiv.org/abs/2012.06678):</span>
<span class="sd">        _&#39;The goal of having column embedding is to enable the model to</span>
<span class="sd">        distinguish the classes in one column from those in the other</span>
<span class="sd">        columns&#39;_. In other words, the idea is to let the model learn which</span>
<span class="sd">        column is embedded at the time. See: `pytorch_widedeep.models.transformers._layers.SharedEmbeddings`.</span>
<span class="sd">    add_shared_embed: bool, Optional, default = None</span>
<span class="sd">        The two embedding sharing strategies are: 1) add the shared embeddings</span>
<span class="sd">        to the column embeddings or 2) to replace the first</span>
<span class="sd">        `frac_shared_embed` with the shared embeddings.</span>
<span class="sd">        See `pytorch_widedeep.models.embeddings_layers.SharedEmbeddings`</span>
<span class="sd">        If &#39;None&#39; is passed, it will default to &#39;False&#39;.</span>
<span class="sd">    frac_shared_embed: float, Optional, default = None</span>
<span class="sd">        The fraction of embeddings that will be shared (if `add_shared_embed</span>
<span class="sd">        = False`) by all the different categories for one particular</span>
<span class="sd">        column. If &#39;None&#39; is passed, it will default to 0.0.</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    input_dim: int, default = 32</span>
<span class="sd">        The so-called *dimension of the model*. Is the number of</span>
<span class="sd">        embeddings used to encode the categorical and/or continuous columns</span>
<span class="sd">    n_heads: int, default = 8</span>
<span class="sd">        Number of attention heads per Transformer block</span>
<span class="sd">    use_qkv_bias: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to use bias in the Q, K, and V</span>
<span class="sd">        projection layers.</span>
<span class="sd">    n_blocks: int, default = 4</span>
<span class="sd">        Number of Transformer blocks</span>
<span class="sd">    attn_dropout: float, default = 0.2</span>
<span class="sd">        Dropout that will be applied to the Multi-Head Attention layers</span>
<span class="sd">    ff_dropout: float, default = 0.1</span>
<span class="sd">        Dropout that will be applied to the FeedForward network</span>
<span class="sd">    ff_factor: float, default = 4</span>
<span class="sd">        Multiplicative factor applied to the first layer of the FF network in</span>
<span class="sd">        each Transformer block, This is normally set to 4.</span>
<span class="sd">    transformer_activation: str, default = &quot;gelu&quot;</span>
<span class="sd">        Transformer Encoder activation function. _&#39;tanh&#39;_, _&#39;relu&#39;_,</span>
<span class="sd">        _&#39;leaky_relu&#39;_, _&#39;gelu&#39;_, _&#39;geglu&#39;_ and _&#39;reglu&#39;_ are supported</span>
<span class="sd">    use_linear_attention: Boolean, default = False,</span>
<span class="sd">        Boolean indicating if Linear Attention (from [Transformers are RNNs:</span>
<span class="sd">        Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236))</span>
<span class="sd">        will be used. The inclusing of this mode of attention is inspired by</span>
<span class="sd">        [this post](https://www.uber.com/en-GB/blog/deepeta-how-uber-predicts-arrival-times/),</span>
<span class="sd">        where the Uber team finds that this attention mechanism leads to the</span>
<span class="sd">        best results for their tabular data.</span>
<span class="sd">    use_flash_attention: Boolean, default = False,</span>
<span class="sd">        Boolean indicating if</span>
<span class="sd">        [Flash Attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)</span>
<span class="sd">        will be used. &lt;br/&gt;</span>
<span class="sd">    mlp_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the number of neurons per dense layer in the MLP. e.g:</span>
<span class="sd">        _[64, 32]_. If not provided no MLP on top of the final</span>
<span class="sd">        Transformer block will be used.</span>
<span class="sd">    mlp_activation: str, Optional, default = None</span>
<span class="sd">        Activation function for the dense layers of the MLP. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky&#39;_relu&#39; and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to _&#39;relu&#39;_.</span>
<span class="sd">    mlp_dropout: float, Optional, default = None</span>
<span class="sd">        float with the dropout between the dense layers of the MLP.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to 0.0.</span>
<span class="sd">    mlp_batchnorm: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_batchnorm_last: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_linear_first: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to `True`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: nn.Module</span>
<span class="sd">        Sequence of Transformer blocks</span>
<span class="sd">    mlp: nn.Module</span>
<span class="sd">        MLP component in the model</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import TabTransformer</span>
<span class="sd">    &gt;&gt;&gt; X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u,i) for u,i in zip(colnames[:4], [4]*4)]</span>
<span class="sd">    &gt;&gt;&gt; continuous_cols = [&#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k:v for v,k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = TabTransformer(column_idx=column_idx, cat_embed_input=cat_embed_input, continuous_cols=continuous_cols)</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_tab)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
        <span class="n">use_linear_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_flash_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
            <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
            <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="c1"># UGLY hack to be able to use the base class __init__ method</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;standard&quot;</span>
                <span class="k">if</span> <span class="n">embed_continuous_method</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">embed_continuous_method</span>
            <span class="p">),</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># we overwrite the embed_continuous_method parameter that was set</span>
        <span class="c1"># to &#39;standard&#39; if embed_continuous_method is None. In other words,</span>
        <span class="c1"># this will already have the right value if the user provided a value</span>
        <span class="c1"># for embed_continuous_method, otherwise will be &quot;standard&quot; when it</span>
        <span class="c1"># should be None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous_method</span> <span class="o">=</span> <span class="n">embed_continuous_method</span>

        <span class="k">if</span> <span class="n">embed_continuous</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span> <span class="o">=</span> <span class="n">embed_continuous</span>
            <span class="k">if</span> <span class="n">embed_continuous</span> <span class="ow">and</span> <span class="n">embed_continuous_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;If &#39;embed_continuous&#39; is True, &#39;embed_continuous_method&#39; must be &quot;</span>
                    <span class="s2">&quot;one of &#39;standard&#39;, &#39;piecewise&#39; or &#39;periodic&#39;.&quot;</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_linear_attention</span> <span class="o">=</span> <span class="n">use_linear_attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attention</span> <span class="o">=</span> <span class="n">use_flash_attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If only continuous features are used &#39;embed_continuous&#39; must be set to &#39;True&#39;&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>
        <span class="c1"># Transformer blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;transformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">TransformerEncoder</span><span class="p">(</span>
                    <span class="n">input_dim</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">,</span>
                    <span class="n">use_qkv_bias</span><span class="p">,</span>
                    <span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="n">ff_dropout</span><span class="p">,</span>
                    <span class="n">ff_factor</span><span class="p">,</span>
                    <span class="n">transformer_activation</span><span class="p">,</span>
                    <span class="n">use_linear_attention</span><span class="p">,</span>
                    <span class="n">use_flash_attention</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_first_hidden_dim</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
                <span class="p">),</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
                <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
                <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
                <span class="p">),</span>
                <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x_embed</span><span class="p">,</span> <span class="n">x_cont_not_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings_tt</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x_embed</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">x_cont_not_embed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x_cont_not_embed</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_mlp_first_hidden_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span><span class="p">:</span>
                <span class="n">attn_output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">attn_output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span><span class="p">:</span>
            <span class="n">attn_output_dim</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

        <span class="k">return</span> <span class="n">attn_output_dim</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">        The shape of the attention weights is $(N, H, F, F)$, where $N$ is the</span>
<span class="sd">        batch size, $H$ is the number of attention heads and $F$ is the</span>
<span class="sd">        number of features/columns in the dataset</span>

<span class="sd">        :information_source: **NOTE**:</span>
<span class="sd">        if flash attention or linear attention</span>
<span class="sd">        are used, no attention weights are saved during the training process</span>
<span class="sd">        and calling this property will throw a ValueError</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attention</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_linear_attention</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Extraction of the attention weights is not supported for &quot;</span>
                <span class="s2">&quot;linear or flash attention&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_embeddings_tt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]:</span>  <span class="c1"># type: ignore[override]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span><span class="p">:</span>
            <span class="c1"># there are continuous and categorical features and the continuous</span>
            <span class="c1"># features are embedded</span>
            <span class="n">x_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">x_cont_not_embed</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span><span class="p">:</span>
            <span class="c1"># there are only continuous features and they are embedded</span>
            <span class="n">x_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_norm</span><span class="p">((</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_idx</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()))</span>
            <span class="n">x_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_embed</span><span class="p">(</span><span class="n">x_embed</span><span class="p">)</span>
            <span class="n">x_cont_not_embed</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span><span class="p">:</span>
            <span class="c1"># there are continuous and categorical features but the continuous</span>
            <span class="c1"># features are not embedded</span>
            <span class="n">x_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">x_cont_not_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_norm</span><span class="p">((</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_idx</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span><span class="p">:</span>
            <span class="c1"># there are only categorical features</span>
            <span class="n">x_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_embed</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">x_cont_not_embed</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Using only continuous features that are not embedded is &quot;</span>
                <span class="s2">&quot;is not supported in this implementation. Please set &#39;embed_continuous_method&#39; &quot;</span>
                <span class="s2">&quot;to one of [&#39;standard&#39;, &#39;piecewise&#39;, &#39;periodic&#39;]&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">x_embed</span><span class="p">,</span> <span class="n">x_cont_not_embed</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
<p><img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>:
if flash attention or linear attention
are used, no attention weights are saved during the training process
and calling this property will throw a ValueError</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">SAINT</span>


<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


        <p>Defines a <a href="https://arxiv.org/abs/2106.01342">SAINT model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This is an slightly modified and enhanced
 version of the model described in the paper,</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>frac_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>&#39;standard&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_heads</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads per Transformer block</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_qkv_bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_blocks</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of SAINT-Transformer blocks.</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the Multi-Head Attention column and
row layers</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the FeedForward network</p>
              </div>
            </td>
            <td>
                  <code>0.2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_factor</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;gelu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm_last</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_linear_first</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.saint.SAINT.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Sequence of SAINT-Transformer blocks</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.saint.SAINT.mlp">mlp</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>MLP component in the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">SAINT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SAINT</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/saint.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SAINT</span><span class="p">(</span><span class="n">BaseTabularModelWithAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a [SAINT model](https://arxiv.org/abs/2106.01342) that</span>
<span class="sd">    can be used as the `deeptabular` component of a Wide &amp; Deep model or</span>
<span class="sd">    independently by itself.</span>

<span class="sd">    Most of the parameters for this class are `Optional` since the use of</span>
<span class="sd">    categorical or continuous is in fact optional (i.e. one can use</span>
<span class="sd">    categorical features only, continuous features only or both).</span>

<span class="sd">    :information_source: **NOTE**: This is an slightly modified and enhanced</span>
<span class="sd">     version of the model described in the paper,</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name and number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    shared_embed: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating if the embeddings will be &quot;shared&quot;. The idea behind `shared_embed` is</span>
<span class="sd">        described in the Appendix A in the [TabTransformer paper](https://arxiv.org/abs/2012.06678):</span>
<span class="sd">        _&#39;The goal of having column embedding is to enable the model to</span>
<span class="sd">        distinguish the classes in one column from those in the other</span>
<span class="sd">        columns&#39;_. In other words, the idea is to let the model learn which</span>
<span class="sd">        column is embedded at the time. See: `pytorch_widedeep.models.transformers._layers.SharedEmbeddings`.</span>
<span class="sd">    add_shared_embed: bool, Optional, default = None</span>
<span class="sd">        The two embedding sharing strategies are: 1) add the shared embeddings</span>
<span class="sd">        to the column embeddings or 2) to replace the first</span>
<span class="sd">        `frac_shared_embed` with the shared embeddings.</span>
<span class="sd">        See `pytorch_widedeep.models.embeddings_layers.SharedEmbeddings`</span>
<span class="sd">        If &#39;None&#39; is passed, it will default to &#39;False&#39;.</span>
<span class="sd">    frac_shared_embed: float, Optional, default = None</span>
<span class="sd">        The fraction of embeddings that will be shared (if `add_shared_embed</span>
<span class="sd">        = False`) by all the different categories for one particular</span>
<span class="sd">        column. If &#39;None&#39; is passed, it will default to 0.0.</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    input_dim: int, default = 32</span>
<span class="sd">        The so-called *dimension of the model*. Is the number of</span>
<span class="sd">        embeddings used to encode the categorical and/or continuous columns</span>
<span class="sd">    n_heads: int, default = 8</span>
<span class="sd">        Number of attention heads per Transformer block</span>
<span class="sd">    use_qkv_bias: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to use bias in the Q, K, and V</span>
<span class="sd">        projection layers</span>
<span class="sd">    n_blocks: int, default = 2</span>
<span class="sd">        Number of SAINT-Transformer blocks.</span>
<span class="sd">    attn_dropout: float, default = 0.2</span>
<span class="sd">        Dropout that will be applied to the Multi-Head Attention column and</span>
<span class="sd">        row layers</span>
<span class="sd">    ff_dropout: float, default = 0.1</span>
<span class="sd">        Dropout that will be applied to the FeedForward network</span>
<span class="sd">    ff_factor: float, default = 4</span>
<span class="sd">        Multiplicative factor applied to the first layer of the FF network in</span>
<span class="sd">        each Transformer block, This is normally set to 4.</span>
<span class="sd">    transformer_activation: str, default = &quot;gelu&quot;</span>
<span class="sd">        Transformer Encoder activation function. _&#39;tanh&#39;_, _&#39;relu&#39;_,</span>
<span class="sd">        _&#39;leaky_relu&#39;_, _&#39;gelu&#39;_, _&#39;geglu&#39;_ and _&#39;reglu&#39;_ are supported</span>
<span class="sd">    mlp_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the number of neurons per dense layer in the MLP. e.g:</span>
<span class="sd">        _[64, 32]_. If not provided no MLP on top of the final</span>
<span class="sd">        Transformer block will be used.</span>
<span class="sd">    mlp_activation: str, Optional, default = None</span>
<span class="sd">        Activation function for the dense layers of the MLP. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky&#39;_relu&#39; and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to _&#39;relu&#39;_.</span>
<span class="sd">    mlp_dropout: float, Optional, default = None</span>
<span class="sd">        float with the dropout between the dense layers of the MLP.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to 0.0.</span>
<span class="sd">    mlp_batchnorm: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_batchnorm_last: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_linear_first: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to `True`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: nn.Module</span>
<span class="sd">        Sequence of SAINT-Transformer blocks</span>
<span class="sd">    mlp: nn.Module</span>
<span class="sd">        MLP component in the model</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import SAINT</span>
<span class="sd">    &gt;&gt;&gt; X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u,i) for u,i in zip(colnames[:4], [4]*4)]</span>
<span class="sd">    &gt;&gt;&gt; continuous_cols = [&#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k:v for v,k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = SAINT(column_idx=column_idx, cat_embed_input=cat_embed_input, continuous_cols=continuous_cols)</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_tab)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
        <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SAINT</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
            <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
            <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>
        <span class="c1"># Transformer blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;saint_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">SaintEncoder</span><span class="p">(</span>
                    <span class="n">input_dim</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">,</span>
                    <span class="n">use_qkv_bias</span><span class="p">,</span>
                    <span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="n">ff_dropout</span><span class="p">,</span>
                    <span class="n">ff_factor</span><span class="p">,</span>
                    <span class="n">transformer_activation</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
        <span class="c1"># therefore all related params are optional</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
                <span class="p">),</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
                <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
                <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
                <span class="p">),</span>
                <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights. Each element of the list is a tuple</span>
<span class="sd">        where the first and the second elements are the column and row</span>
<span class="sd">        attention weights respectively</span>

<span class="sd">        The shape of the attention weights is:</span>

<span class="sd">        - column attention: $(N, H, F, F)$</span>

<span class="sd">        - row attention: $(1, H, N, N)$</span>

<span class="sd">        where $N$ is the batch size, $H$ is the number of heads and $F$ is the</span>
<span class="sd">        number of features/columns in the dataset</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
            <span class="n">attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">(</span><span class="n">blk</span><span class="o">.</span><span class="n">col_attn</span><span class="o">.</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">blk</span><span class="o">.</span><span class="n">row_attn</span><span class="o">.</span><span class="n">attn_weights</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_weights</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List with the attention weights. Each element of the list is a tuple
where the first and the second elements are the column and row
attention weights respectively</p>
<p>The shape of the attention weights is:</p>
<ul>
<li>
<p>column attention: <span class="arithmatex">\((N, H, F, F)\)</span></p>
</li>
<li>
<p>row attention: <span class="arithmatex">\((1, H, N, N)\)</span></p>
</li>
</ul>
<p>where <span class="arithmatex">\(N\)</span> is the batch size, <span class="arithmatex">\(H\)</span> is the number of heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">FTTransformer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


        <p>Defines a <a href="https://arxiv.org/abs/2106.11959">FTTransformer model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>frac_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>&#39;standard&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The so-called <em>dimension of the model</em>. Is the number of embeddings used to encode
the categorical and/or continuous columns.</p>
              </div>
            </td>
            <td>
                  <code>64</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kv_compression_factor</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>By default, the FTTransformer uses Linear Attention
(See <a href="https://arxiv.org/abs/2006.04768&gt;">Linformer: Self-Attention with Linear Complexity</a> ).
The compression factor that will be used to reduce the input sequence
length. If we denote the resulting sequence length as
<span class="arithmatex">\(k = int(kv_{compression \space factor} \times s)\)</span>
where <span class="arithmatex">\(s\)</span> is the input sequence length.</p>
              </div>
            </td>
            <td>
                  <code>0.5</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kv_sharing</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the <span class="arithmatex">\(E\)</span> and <span class="arithmatex">\(F\)</span> projection matrices
will share weights.  See <a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear
Complexity</a> for details</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_heads</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads per FTTransformer block</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_qkv_bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_blocks</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of FTTransformer blocks</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the Linear-Attention layers</p>
              </div>
            </td>
            <td>
                  <code>0.2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the FeedForward network</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_factor</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4, but they use 4/3
in the paper.</p>
              </div>
            </td>
            <td>
                  <code>1.33</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;reglu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
FTTransformer block will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm_last</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_linear_first</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Sequence of FTTransformer blocks</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.mlp">mlp</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>MLP component in the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">FTTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FTTransformer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/ft_transformer.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FTTransformer</span><span class="p">(</span><span class="n">BaseTabularModelWithAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a [FTTransformer model](https://arxiv.org/abs/2106.11959) that</span>
<span class="sd">    can be used as the `deeptabular` component of a Wide &amp; Deep model or</span>
<span class="sd">    independently by itself.</span>

<span class="sd">    Most of the parameters for this class are `Optional` since the use of</span>
<span class="sd">    categorical or continuous is in fact optional (i.e. one can use</span>
<span class="sd">    categorical features only, continuous features only or both).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name and number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    shared_embed: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating if the embeddings will be &quot;shared&quot;. The idea behind `shared_embed` is</span>
<span class="sd">        described in the Appendix A in the [TabTransformer paper](https://arxiv.org/abs/2012.06678):</span>
<span class="sd">        _&#39;The goal of having column embedding is to enable the model to</span>
<span class="sd">        distinguish the classes in one column from those in the other</span>
<span class="sd">        columns&#39;_. In other words, the idea is to let the model learn which</span>
<span class="sd">        column is embedded at the time. See: `pytorch_widedeep.models.transformers._layers.SharedEmbeddings`.</span>
<span class="sd">    add_shared_embed: bool, Optional, default = None</span>
<span class="sd">        The two embedding sharing strategies are: 1) add the shared embeddings</span>
<span class="sd">        to the column embeddings or 2) to replace the first</span>
<span class="sd">        `frac_shared_embed` with the shared embeddings.</span>
<span class="sd">        See `pytorch_widedeep.models.embeddings_layers.SharedEmbeddings`</span>
<span class="sd">        If &#39;None&#39; is passed, it will default to &#39;False&#39;.</span>
<span class="sd">    frac_shared_embed: float, Optional, default = None</span>
<span class="sd">        The fraction of embeddings that will be shared (if `add_shared_embed</span>
<span class="sd">        = False`) by all the different categories for one particular</span>
<span class="sd">        column. If &#39;None&#39; is passed, it will default to 0.0.</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    input_dim: int, default = 64</span>
<span class="sd">        The so-called *dimension of the model*. Is the number of embeddings used to encode</span>
<span class="sd">        the categorical and/or continuous columns.</span>
<span class="sd">    kv_compression_factor: int, default = 0.5</span>
<span class="sd">        By default, the FTTransformer uses Linear Attention</span>
<span class="sd">        (See [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768&gt;) ).</span>
<span class="sd">        The compression factor that will be used to reduce the input sequence</span>
<span class="sd">        length. If we denote the resulting sequence length as</span>
<span class="sd">        $k = int(kv_{compression \space factor} \times s)$</span>
<span class="sd">        where $s$ is the input sequence length.</span>
<span class="sd">    kv_sharing: bool, default = False</span>
<span class="sd">        Boolean indicating if the $E$ and $F$ projection matrices</span>
<span class="sd">        will share weights.  See [Linformer: Self-Attention with Linear</span>
<span class="sd">        Complexity](https://arxiv.org/abs/2006.04768) for details</span>
<span class="sd">    n_heads: int, default = 8</span>
<span class="sd">        Number of attention heads per FTTransformer block</span>
<span class="sd">    use_qkv_bias: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to use bias in the Q, K, and V</span>
<span class="sd">        projection layers</span>
<span class="sd">    n_blocks: int, default = 4</span>
<span class="sd">        Number of FTTransformer blocks</span>
<span class="sd">    attn_dropout: float, default = 0.2</span>
<span class="sd">        Dropout that will be applied to the Linear-Attention layers</span>
<span class="sd">    ff_dropout: float, default = 0.1</span>
<span class="sd">        Dropout that will be applied to the FeedForward network</span>
<span class="sd">    ff_factor: float, default = 4 / 3</span>
<span class="sd">        Multiplicative factor applied to the first layer of the FF network in</span>
<span class="sd">        each Transformer block, This is normally set to 4, but they use 4/3</span>
<span class="sd">        in the paper.</span>
<span class="sd">    transformer_activation: str, default = &quot;gelu&quot;</span>
<span class="sd">        Transformer Encoder activation function. _&#39;tanh&#39;_, _&#39;relu&#39;_,</span>
<span class="sd">        _&#39;leaky_relu&#39;_, _&#39;gelu&#39;_, _&#39;geglu&#39;_ and _&#39;reglu&#39;_ are supported</span>
<span class="sd">    mlp_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the number of neurons per dense layer in the MLP. e.g:</span>
<span class="sd">        _[64, 32]_. If not provided no MLP on top of the final</span>
<span class="sd">        FTTransformer block will be used.</span>
<span class="sd">    mlp_activation: str, Optional, default = None</span>
<span class="sd">        Activation function for the dense layers of the MLP. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky&#39;_relu&#39; and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to _&#39;relu&#39;_.</span>
<span class="sd">    mlp_dropout: float, Optional, default = None</span>
<span class="sd">        float with the dropout between the dense layers of the MLP.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to 0.0.</span>
<span class="sd">    mlp_batchnorm: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_batchnorm_last: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_linear_first: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to `True`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: nn.Module</span>
<span class="sd">        Sequence of FTTransformer blocks</span>
<span class="sd">    mlp: nn.Module</span>
<span class="sd">        MLP component in the model</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import FTTransformer</span>
<span class="sd">    &gt;&gt;&gt; X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u,i) for u,i in zip(colnames[:4], [4]*4)]</span>
<span class="sd">    &gt;&gt;&gt; continuous_cols = [&#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k:v for v,k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = FTTransformer(column_idx=column_idx, cat_embed_input=cat_embed_input, continuous_cols=continuous_cols)</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_tab)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">kv_compression_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">kv_sharing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.33</span><span class="p">,</span>
        <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reglu&quot;</span><span class="p">,</span>
        <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FTTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
            <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
            <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kv_compression_factor</span> <span class="o">=</span> <span class="n">kv_compression_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kv_sharing</span> <span class="o">=</span> <span class="n">kv_sharing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>
        <span class="c1"># Transformer blocks</span>
        <span class="n">is_first</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;fttransformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">FTTransformerEncoder</span><span class="p">(</span>
                    <span class="n">input_dim</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">,</span>
                    <span class="n">use_qkv_bias</span><span class="p">,</span>
                    <span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="n">ff_dropout</span><span class="p">,</span>
                    <span class="n">ff_factor</span><span class="p">,</span>
                    <span class="n">kv_compression_factor</span><span class="p">,</span>
                    <span class="n">kv_sharing</span><span class="p">,</span>
                    <span class="n">transformer_activation</span><span class="p">,</span>
                    <span class="n">is_first</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">is_first</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
        <span class="c1"># therefore all related params are optional</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
                <span class="p">),</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
                <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
                <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
                <span class="p">),</span>
                <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">        The shape of the attention weights is: $(N, H, F, k)$, where $N$ is</span>
<span class="sd">        the batch size, $H$ is the number of attention heads, $F$ is the</span>
<span class="sd">        number of features/columns and $k$ is the reduced sequence length or</span>
<span class="sd">        dimension, i.e. $k = int(kv_{compression \space factor} \times s)$</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List with the attention weights per block</p>
<p>The shape of the attention weights is: <span class="arithmatex">\((N, H, F, k)\)</span>, where <span class="arithmatex">\(N\)</span> is
the batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads, <span class="arithmatex">\(F\)</span> is the
number of features/columns and <span class="arithmatex">\(k\)</span> is the reduced sequence length or
dimension, i.e. <span class="arithmatex">\(k = int(kv_{compression \space factor} \times s)\)</span></p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabPerceiver</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


        <p>Defines an adaptation of a <a href="https://arxiv.org/abs/2103.03206">Perceiver</a>
 that can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model
 or independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: while there are scientific publications for
 the <code>TabTransformer</code>, <code>SAINT</code> and <code>FTTransformer</code>, the <code>TabPerceiver</code>
 and the <code>TabFastFormer</code> are our own adaptations of the
 <a href="https://arxiv.org/abs/2103.03206">Perceiver</a> and the
 <a href="https://arxiv.org/abs/2108.09084">FastFormer</a> for tabular data.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>frac_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>&#39;standard&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The so-called <em>dimension of the model</em>. Is the number of embeddings
used to encode the categorical and/or continuous columns.</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_cross_attns</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of times each perceiver block will cross attend to the input
data (i.e. number of cross attention components per perceiver block).
This should normally be 1. However, in the paper they describe some
architectures (normally computer vision-related problems) where the
Perceiver attends multiple times to the input array. Therefore, maybe
multiple cross attention to the input array is also useful in some
cases for tabular data <img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f937.png" title=":shrug:" /> .</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_cross_attn_heads</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads for the cross attention component</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_latents</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of latents. This is the <span class="arithmatex">\(N\)</span> parameter in the paper. As
indicated in the paper, this number should be significantly lower
than <span class="arithmatex">\(M\)</span> (the number of columns in the dataset). Setting <span class="arithmatex">\(N\)</span> closer
to <span class="arithmatex">\(M\)</span> defies the main purpose of the Perceiver, which is to overcome
the transformer quadratic bottleneck</p>
              </div>
            </td>
            <td>
                  <code>16</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>latent_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Latent dimension.</p>
              </div>
            </td>
            <td>
                  <code>128</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_latent_heads</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads per Latent Transformer</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_latent_blocks</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of transformer encoder blocks (normalised MHA + normalised FF)
per Latent Transformer</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_perceiver_blocks</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of Perceiver blocks defined as [Cross Attention + Latent
Transformer]</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_weights</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the weights will be shared between Perceiver
blocks</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the Multi-Head Attention layers</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the FeedForward network</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_factor</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;geglu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm_last</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_linear_first</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.ModuleDict">ModuleDict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>ModuleDict with the Perceiver blocks</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.latents">latents</span></code></td>
            <td>
                  <code><span title="torch.nn.Parameter">Parameter</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Latents that will be used for prediction</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.mlp">mlp</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>MLP component in the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabPerceiver</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabPerceiver</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span> <span class="n">n_latents</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span><span class="n">n_perceiver_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_perceiver.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TabPerceiver</span><span class="p">(</span><span class="n">BaseTabularModelWithAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines an adaptation of a [Perceiver](https://arxiv.org/abs/2103.03206)</span>
<span class="sd">     that can be used as the `deeptabular` component of a Wide &amp; Deep model</span>
<span class="sd">     or independently by itself.</span>

<span class="sd">    Most of the parameters for this class are `Optional` since the use of</span>
<span class="sd">    categorical or continuous is in fact optional (i.e. one can use</span>
<span class="sd">    categorical features only, continuous features only or both).</span>

<span class="sd">    :information_source: **NOTE**: while there are scientific publications for</span>
<span class="sd">     the `TabTransformer`, `SAINT` and `FTTransformer`, the `TabPerceiver`</span>
<span class="sd">     and the `TabFastFormer` are our own adaptations of the</span>
<span class="sd">     [Perceiver](https://arxiv.org/abs/2103.03206) and the</span>
<span class="sd">     [FastFormer](https://arxiv.org/abs/2108.09084) for tabular data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name and number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    shared_embed: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating if the embeddings will be &quot;shared&quot;. The idea behind `shared_embed` is</span>
<span class="sd">        described in the Appendix A in the [TabTransformer paper](https://arxiv.org/abs/2012.06678):</span>
<span class="sd">        _&#39;The goal of having column embedding is to enable the model to</span>
<span class="sd">        distinguish the classes in one column from those in the other</span>
<span class="sd">        columns&#39;_. In other words, the idea is to let the model learn which</span>
<span class="sd">        column is embedded at the time. See: `pytorch_widedeep.models.transformers._layers.SharedEmbeddings`.</span>
<span class="sd">    add_shared_embed: bool, Optional, default = None</span>
<span class="sd">        The two embedding sharing strategies are: 1) add the shared embeddings</span>
<span class="sd">        to the column embeddings or 2) to replace the first</span>
<span class="sd">        `frac_shared_embed` with the shared embeddings.</span>
<span class="sd">        See `pytorch_widedeep.models.embeddings_layers.SharedEmbeddings`</span>
<span class="sd">        If &#39;None&#39; is passed, it will default to &#39;False&#39;.</span>
<span class="sd">    frac_shared_embed: float, Optional, default = None</span>
<span class="sd">        The fraction of embeddings that will be shared (if `add_shared_embed</span>
<span class="sd">        = False`) by all the different categories for one particular</span>
<span class="sd">        column. If &#39;None&#39; is passed, it will default to 0.0.</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    input_dim: int, default = 32</span>
<span class="sd">        The so-called *dimension of the model*. Is the number of embeddings</span>
<span class="sd">        used to encode the categorical and/or continuous columns.</span>
<span class="sd">    n_cross_attns: int, default = 1</span>
<span class="sd">        Number of times each perceiver block will cross attend to the input</span>
<span class="sd">        data (i.e. number of cross attention components per perceiver block).</span>
<span class="sd">        This should normally be 1. However, in the paper they describe some</span>
<span class="sd">        architectures (normally computer vision-related problems) where the</span>
<span class="sd">        Perceiver attends multiple times to the input array. Therefore, maybe</span>
<span class="sd">        multiple cross attention to the input array is also useful in some</span>
<span class="sd">        cases for tabular data :shrug: .</span>
<span class="sd">    n_cross_attn_heads: int, default = 4</span>
<span class="sd">        Number of attention heads for the cross attention component</span>
<span class="sd">    n_latents: int, default = 16</span>
<span class="sd">        Number of latents. This is the $N$ parameter in the paper. As</span>
<span class="sd">        indicated in the paper, this number should be significantly lower</span>
<span class="sd">        than $M$ (the number of columns in the dataset). Setting $N$ closer</span>
<span class="sd">        to $M$ defies the main purpose of the Perceiver, which is to overcome</span>
<span class="sd">        the transformer quadratic bottleneck</span>
<span class="sd">    latent_dim: int, default = 128</span>
<span class="sd">        Latent dimension.</span>
<span class="sd">    n_latent_heads: int, default = 4</span>
<span class="sd">        Number of attention heads per Latent Transformer</span>
<span class="sd">    n_latent_blocks: int, default = 4</span>
<span class="sd">        Number of transformer encoder blocks (normalised MHA + normalised FF)</span>
<span class="sd">        per Latent Transformer</span>
<span class="sd">    n_perceiver_blocks: int, default = 4</span>
<span class="sd">        Number of Perceiver blocks defined as [Cross Attention + Latent</span>
<span class="sd">        Transformer]</span>
<span class="sd">    share_weights: Boolean, default = False</span>
<span class="sd">        Boolean indicating if the weights will be shared between Perceiver</span>
<span class="sd">        blocks</span>
<span class="sd">    attn_dropout: float, default = 0.2</span>
<span class="sd">        Dropout that will be applied to the Multi-Head Attention layers</span>
<span class="sd">    ff_dropout: float, default = 0.1</span>
<span class="sd">        Dropout that will be applied to the FeedForward network</span>
<span class="sd">    ff_factor: float, default = 4</span>
<span class="sd">        Multiplicative factor applied to the first layer of the FF network in</span>
<span class="sd">        each Transformer block, This is normally set to 4.</span>
<span class="sd">    transformer_activation: str, default = &quot;gelu&quot;</span>
<span class="sd">        Transformer Encoder activation function. _&#39;tanh&#39;_, _&#39;relu&#39;_,</span>
<span class="sd">        _&#39;leaky_relu&#39;_, _&#39;gelu&#39;_, _&#39;geglu&#39;_ and _&#39;reglu&#39;_ are supported</span>
<span class="sd">    mlp_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the number of neurons per dense layer in the MLP. e.g:</span>
<span class="sd">        _[64, 32]_. If not provided no MLP on top of the final</span>
<span class="sd">        Transformer block will be used.</span>
<span class="sd">    mlp_activation: str, Optional, default = None</span>
<span class="sd">        Activation function for the dense layers of the MLP. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky&#39;_relu&#39; and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to _&#39;relu&#39;_.</span>
<span class="sd">    mlp_dropout: float, Optional, default = None</span>
<span class="sd">        float with the dropout between the dense layers of the MLP.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to 0.0.</span>
<span class="sd">    mlp_batchnorm: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_batchnorm_last: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_linear_first: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to `True`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: nn.ModuleDict</span>
<span class="sd">        ModuleDict with the Perceiver blocks</span>
<span class="sd">    latents: nn.Parameter</span>
<span class="sd">        Latents that will be used for prediction</span>
<span class="sd">    mlp: nn.Module</span>
<span class="sd">        MLP component in the model</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import TabPerceiver</span>
<span class="sd">    &gt;&gt;&gt; X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u,i) for u,i in zip(colnames[:4], [4]*4)]</span>
<span class="sd">    &gt;&gt;&gt; continuous_cols = [&#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k:v for v,k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = TabPerceiver(column_idx=column_idx, cat_embed_input=cat_embed_input,</span>
<span class="sd">    ... continuous_cols=continuous_cols, n_latents=2, latent_dim=16,</span>
<span class="sd">    ... n_perceiver_blocks=2)</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_tab)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">n_cross_attns</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">n_cross_attn_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">n_latents</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">n_latent_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">n_latent_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">n_perceiver_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">share_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;geglu&quot;</span><span class="p">,</span>
        <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabPerceiver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
            <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
            <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attns</span> <span class="o">=</span> <span class="n">n_cross_attns</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attn_heads</span> <span class="o">=</span> <span class="n">n_cross_attn_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_latents</span> <span class="o">=</span> <span class="n">n_latents</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="n">latent_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_latent_heads</span> <span class="o">=</span> <span class="n">n_latent_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_latent_blocks</span> <span class="o">=</span> <span class="n">n_latent_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_perceiver_blocks</span> <span class="o">=</span> <span class="n">n_perceiver_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span> <span class="o">=</span> <span class="n">share_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>
        <span class="c1"># Transformer blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latents</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">trunc_normal_</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_latents</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
        <span class="n">first_perceiver_block</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_perceiver_block</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block0&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">first_perceiver_block</span>

        <span class="k">if</span> <span class="n">share_weights</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_perceiver_blocks</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="n">first_perceiver_block</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_perceiver_blocks</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_perceiver_block</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span>

        <span class="c1"># Mlp</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
                <span class="p">),</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
                <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
                <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
                <span class="p">),</span>
                <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latents</span><span class="p">,</span> <span class="s2">&quot;n d -&gt; b n d&quot;</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_perceiver_blocks</span><span class="p">):</span>
            <span class="n">cross_attns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)][</span><span class="s2">&quot;cross_attns&quot;</span><span class="p">]</span>
            <span class="n">latent_transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)][</span>
                <span class="s2">&quot;latent_transformer&quot;</span>
            <span class="p">]</span>
            <span class="k">for</span> <span class="n">cross_attn</span> <span class="ow">in</span> <span class="n">cross_attns</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_emb</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">latent_transformer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># average along the latent index axis</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights. If the weights are not shared</span>
<span class="sd">        between perceiver blocks each element of the list will be a list</span>
<span class="sd">        itself containing the Cross Attention and Latent Transformer</span>
<span class="sd">        attention weights respectively</span>

<span class="sd">        The shape of the attention weights is:</span>

<span class="sd">        - Cross Attention: $(N, C, L, F)$</span>

<span class="sd">        - Latent Attention: $(N, T, L, L)$</span>

<span class="sd">        WHere $N$ is the batch size, $C$ is the number of Cross Attention</span>
<span class="sd">        heads, $L$ is the number of Latents, $F$ is the number of</span>
<span class="sd">        features/columns in the dataset and $T$ is the number of Latent</span>
<span class="sd">        Attention heads</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span><span class="p">:</span>
            <span class="n">cross_attns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block0&quot;</span><span class="p">][</span><span class="s2">&quot;cross_attns&quot;</span><span class="p">]</span>
            <span class="n">latent_transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block0&quot;</span><span class="p">][</span><span class="s2">&quot;latent_transformer&quot;</span><span class="p">]</span>
            <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_attn_weights</span><span class="p">(</span>
                <span class="n">cross_attns</span><span class="p">,</span> <span class="n">latent_transformer</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_perceiver_blocks</span><span class="p">):</span>
                <span class="n">cross_attns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)][</span><span class="s2">&quot;cross_attns&quot;</span><span class="p">]</span>
                <span class="n">latent_transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)][</span>
                    <span class="s2">&quot;latent_transformer&quot;</span>
                <span class="p">]</span>
                <span class="n">attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_attn_weights</span><span class="p">(</span><span class="n">cross_attns</span><span class="p">,</span> <span class="n">latent_transformer</span><span class="p">)</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_weights</span>

    <span class="k">def</span> <span class="nf">_build_perceiver_block</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">:</span>
        <span class="n">perceiver_block</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>

        <span class="c1"># Cross Attention</span>
        <span class="n">cross_attns</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attns</span><span class="p">):</span>
            <span class="n">cross_attns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">PerceiverEncoder</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attn_heads</span><span class="p">,</span>
                    <span class="kc">False</span><span class="p">,</span>  <span class="c1"># use_bias</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span><span class="p">,</span>  <span class="c1"># q_dim,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="n">perceiver_block</span><span class="p">[</span><span class="s2">&quot;cross_attns&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cross_attns</span>

        <span class="c1"># Latent Transformer</span>
        <span class="n">latent_transformer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_latent_blocks</span><span class="p">):</span>
            <span class="n">latent_transformer</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;latent_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">PerceiverEncoder</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span><span class="p">,</span>  <span class="c1"># input_dim</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_latent_heads</span><span class="p">,</span>
                    <span class="kc">False</span><span class="p">,</span>  <span class="c1"># use_bias</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="n">perceiver_block</span><span class="p">[</span><span class="s2">&quot;latent_transformer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">latent_transformer</span>

        <span class="k">return</span> <span class="n">perceiver_block</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_extract_attn_weights</span><span class="p">(</span><span class="n">cross_attns</span><span class="p">,</span> <span class="n">latent_transformer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">cross_attn</span> <span class="ow">in</span> <span class="n">cross_attns</span><span class="p">:</span>
            <span class="n">attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cross_attn</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">latent_block</span> <span class="ow">in</span> <span class="n">latent_transformer</span><span class="p">:</span>
            <span class="n">attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">latent_block</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_weights</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List with the attention weights. If the weights are not shared
between perceiver blocks each element of the list will be a list
itself containing the Cross Attention and Latent Transformer
attention weights respectively</p>
<p>The shape of the attention weights is:</p>
<ul>
<li>
<p>Cross Attention: <span class="arithmatex">\((N, C, L, F)\)</span></p>
</li>
<li>
<p>Latent Attention: <span class="arithmatex">\((N, T, L, L)\)</span></p>
</li>
</ul>
<p>WHere <span class="arithmatex">\(N\)</span> is the batch size, <span class="arithmatex">\(C\)</span> is the number of Cross Attention
heads, <span class="arithmatex">\(L\)</span> is the number of Latents, <span class="arithmatex">\(F\)</span> is the number of
features/columns in the dataset and <span class="arithmatex">\(T\)</span> is the number of Latent
Attention heads</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabFastFormer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


        <p>Defines an adaptation of a <a href="https://arxiv.org/abs/2108.09084">FastFormer</a>
that can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model
or independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: while there are scientific publications for
 the <code>TabTransformer</code>, <code>SAINT</code> and <code>FTTransformer</code>, the <code>TabPerceiver</code>
 and the <code>TabFastFormer</code> are our own adaptations of the
 <a href="https://arxiv.org/abs/2103.03206">Perceiver</a> and the
 <a href="https://arxiv.org/abs/2108.09084">FastFormer</a> for tabular data.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>column_idx</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_input</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cat_bias</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cat_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>add_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>frac_shared_embed</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>continuous_cols</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the name of the numeric (aka continuous) columns</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_norm_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_continuous_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
              </div>
            </td>
            <td>
                  <code>&#39;standard&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cont_embed_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_frequencies</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sigma</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_last_layer</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>full_embed_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
              </div>
            </td>
            <td>
                  <code>32</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_heads</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention heads per FastFormer block</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_blocks</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of FastFormer blocks</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the Additive Attention layers</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout that will be applied to the FeedForward network</p>
              </div>
            </td>
            <td>
                  <code>0.2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ff_factor</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_qv_weights</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Following the paper, this is a boolean indicating if the Value (<span class="arithmatex">\(V\)</span>) and
the Query (<span class="arithmatex">\(Q\)</span>) transformation parameters will be shared.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>share_weights</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>In addition to sharing the <span class="arithmatex">\(V\)</span> and <span class="arithmatex">\(Q\)</span> transformation parameters, the
parameters across different Fastformer layers can also be shared.
Please, see
<code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code> for
details</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>MLP hidden dimensions. If not provided no MLP on top of the final
FTTransformer block will be used</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_batchnorm_last</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mlp_linear_first</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.encoder">encoder</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Sequence of FasFormer blocks.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.mlp">mlp</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>MLP component in the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabFastFormer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabFastFormer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TabFastFormer</span><span class="p">(</span><span class="n">BaseTabularModelWithAttention</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines an adaptation of a [FastFormer](https://arxiv.org/abs/2108.09084)</span>
<span class="sd">    that can be used as the `deeptabular` component of a Wide &amp; Deep model</span>
<span class="sd">    or independently by itself.</span>

<span class="sd">    Most of the parameters for this class are `Optional` since the use of</span>
<span class="sd">    categorical or continuous is in fact optional (i.e. one can use</span>
<span class="sd">    categorical features only, continuous features only or both).</span>

<span class="sd">    :information_source: **NOTE**: while there are scientific publications for</span>
<span class="sd">     the `TabTransformer`, `SAINT` and `FTTransformer`, the `TabPerceiver`</span>
<span class="sd">     and the `TabFastFormer` are our own adaptations of the</span>
<span class="sd">     [Perceiver](https://arxiv.org/abs/2103.03206) and the</span>
<span class="sd">     [FastFormer](https://arxiv.org/abs/2108.09084) for tabular data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    column_idx: Dict</span>
<span class="sd">        Dict containing the index of the columns that will be passed through</span>
<span class="sd">        the `TabMlp` model. Required to slice the tensors. e.g. _{&#39;education&#39;:</span>
<span class="sd">        0, &#39;relationship&#39;: 1, &#39;workclass&#39;: 2, ...}_.</span>
<span class="sd">    cat_embed_input: List, Optional, default = None</span>
<span class="sd">        List of Tuples with the column name and number of unique values and</span>
<span class="sd">        embedding dimension. e.g. _[(education, 11), ...]_</span>
<span class="sd">    cat_embed_dropout: float, Optional, default = None</span>
<span class="sd">        Categorical embeddings dropout. If `None`, it will default</span>
<span class="sd">        to 0.</span>
<span class="sd">    use_cat_bias: bool, Optional, default = None,</span>
<span class="sd">        Boolean indicating if bias will be used for the categorical embeddings.</span>
<span class="sd">        If `None`, it will default to &#39;False&#39;.</span>
<span class="sd">    cat_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the categorical embeddings, if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    shared_embed: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating if the embeddings will be &quot;shared&quot;. The idea behind `shared_embed` is</span>
<span class="sd">        described in the Appendix A in the [TabTransformer paper](https://arxiv.org/abs/2012.06678):</span>
<span class="sd">        _&#39;The goal of having column embedding is to enable the model to</span>
<span class="sd">        distinguish the classes in one column from those in the other</span>
<span class="sd">        columns&#39;_. In other words, the idea is to let the model learn which</span>
<span class="sd">        column is embedded at the time. See: `pytorch_widedeep.models.transformers._layers.SharedEmbeddings`.</span>
<span class="sd">    add_shared_embed: bool, Optional, default = None</span>
<span class="sd">        The two embedding sharing strategies are: 1) add the shared embeddings</span>
<span class="sd">        to the column embeddings or 2) to replace the first</span>
<span class="sd">        `frac_shared_embed` with the shared embeddings.</span>
<span class="sd">        See `pytorch_widedeep.models.embeddings_layers.SharedEmbeddings`</span>
<span class="sd">        If &#39;None&#39; is passed, it will default to &#39;False&#39;.</span>
<span class="sd">    frac_shared_embed: float, Optional, default = None</span>
<span class="sd">        The fraction of embeddings that will be shared (if `add_shared_embed</span>
<span class="sd">        = False`) by all the different categories for one particular</span>
<span class="sd">        column. If &#39;None&#39; is passed, it will default to 0.0.</span>
<span class="sd">    continuous_cols: List, Optional, default = None</span>
<span class="sd">        List with the name of the numeric (aka continuous) columns</span>
<span class="sd">    cont_norm_layer: str, Optional, default =  None</span>
<span class="sd">        Type of normalization layer applied to the continuous features.</span>
<span class="sd">        Options are: _&#39;layernorm&#39;_ and _&#39;batchnorm&#39;_. if `None`, no</span>
<span class="sd">        normalization layer will be used.</span>
<span class="sd">    embed_continuous_method: Optional, str, default = None,</span>
<span class="sd">        Method to use to embed the continuous features. Options are:</span>
<span class="sd">        _&#39;standard&#39;_, _&#39;periodic&#39;_ or _&#39;piecewise&#39;_. The _&#39;standard&#39;_</span>
<span class="sd">        embedding method is based on the FT-Transformer implementation</span>
<span class="sd">        presented in the paper: [Revisiting Deep Learning Models for</span>
<span class="sd">        Tabular Data](https://arxiv.org/abs/2106.11959v5). The _&#39;periodic&#39;_</span>
<span class="sd">        and_&#39;piecewise&#39;_ methods were presented in the paper: [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556).</span>
<span class="sd">        Please, read the papers for details.</span>
<span class="sd">    cont_embed_dropout: float, Optional, default = None,</span>
<span class="sd">        Dropout for the continuous embeddings. If `None`, it will default to 0.0</span>
<span class="sd">    cont_embed_activation: Optional, str, default = None,</span>
<span class="sd">        Activation function for the continuous embeddings if any. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If `None`, no activation function will be applied.</span>
<span class="sd">    quantization_setup: Dict[str, List[float]], Optional, default = None,</span>
<span class="sd">        This parameter is used when the _&#39;piecewise&#39;_ method is used to embed</span>
<span class="sd">        the continuous cols. It is a dict where keys are the name of the continuous</span>
<span class="sd">        columns and values are lists with the boundaries for the quantization</span>
<span class="sd">        of the continuous_cols. See the examples for details. If</span>
<span class="sd">        If the _&#39;piecewise&#39;_ method is used, this parameter is required.</span>
<span class="sd">    n_frequencies: int, Optional, default = None,</span>
<span class="sd">        This is the so called _&#39;k&#39;_ in their paper [On Embeddings for</span>
<span class="sd">        Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556),</span>
<span class="sd">        and is the number of &#39;frequencies&#39; that will be used to represent each</span>
<span class="sd">        continuous column. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    sigma: float, Optional, default = None,</span>
<span class="sd">        This is the sigma parameter in the paper mentioned when describing the</span>
<span class="sd">        previous parameters and it is used to initialise the &#39;frequency</span>
<span class="sd">        weights&#39;. See their Eq 2 in the paper for details. If</span>
<span class="sd">        the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    share_last_layer: bool, Optional, default = None,</span>
<span class="sd">        This parameter is not present in the before mentioned paper but it is implemented in</span>
<span class="sd">        the [official repo](https://github.com/yandex-research/rtdl-num-embeddings/tree/main).</span>
<span class="sd">        If `True` the linear layer that turns the frequencies into embeddings</span>
<span class="sd">        will be shared across the continuous columns. If `False` a different</span>
<span class="sd">        linear layer will be used for each continuous column.</span>
<span class="sd">        If the _&#39;periodic&#39;_ method is used, this parameter is required.</span>
<span class="sd">    full_embed_dropout: bool, Optional, default = None,</span>
<span class="sd">        If `True`, the full embedding corresponding to a column will be masked</span>
<span class="sd">        out/dropout. If `None`, it will default to `False`.</span>
<span class="sd">    input_dim: int, default = 32</span>
<span class="sd">        The so-called *dimension of the model*. Is the number of</span>
<span class="sd">        embeddings used to encode the categorical and/or continuous columns</span>
<span class="sd">    n_heads: int, default = 8</span>
<span class="sd">        Number of attention heads per FastFormer block</span>
<span class="sd">    use_bias: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to use bias in the Q, K, and V</span>
<span class="sd">        projection layers</span>
<span class="sd">    n_blocks: int, default = 4</span>
<span class="sd">        Number of FastFormer blocks</span>
<span class="sd">    attn_dropout: float, default = 0.2</span>
<span class="sd">        Dropout that will be applied to the Additive Attention layers</span>
<span class="sd">    ff_dropout: float, default = 0.1</span>
<span class="sd">        Dropout that will be applied to the FeedForward network</span>
<span class="sd">    ff_factor: float, default = 4</span>
<span class="sd">        Multiplicative factor applied to the first layer of the FF network in</span>
<span class="sd">        each Transformer block, This is normally set to 4.</span>
<span class="sd">    share_qv_weights: bool, default = False</span>
<span class="sd">        Following the paper, this is a boolean indicating if the Value ($V$) and</span>
<span class="sd">        the Query ($Q$) transformation parameters will be shared.</span>
<span class="sd">    share_weights: bool, default = False</span>
<span class="sd">        In addition to sharing the $V$ and $Q$ transformation parameters, the</span>
<span class="sd">        parameters across different Fastformer layers can also be shared.</span>
<span class="sd">        Please, see</span>
<span class="sd">        `pytorch_widedeep/models/tabular/transformers/tab_fastformer.py` for</span>
<span class="sd">        details</span>
<span class="sd">    transformer_activation: str, default = &quot;gelu&quot;</span>
<span class="sd">        Transformer Encoder activation function. _&#39;tanh&#39;_, _&#39;relu&#39;_,</span>
<span class="sd">        _&#39;leaky_relu&#39;_, _&#39;gelu&#39;_, _&#39;geglu&#39;_ and _&#39;reglu&#39;_ are supported</span>
<span class="sd">    mlp_hidden_dims: List, Optional, default = None</span>
<span class="sd">        MLP hidden dimensions. If not provided no MLP on top of the final</span>
<span class="sd">        FTTransformer block will be used</span>
<span class="sd">    mlp_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the number of neurons per dense layer in the MLP. e.g:</span>
<span class="sd">        _[64, 32]_. If not provided no MLP on top of the final</span>
<span class="sd">        Transformer block will be used.</span>
<span class="sd">    mlp_activation: str, Optional, default = None</span>
<span class="sd">        Activation function for the dense layers of the MLP. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky&#39;_relu&#39; and _&#39;gelu&#39;_ are supported.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to _&#39;relu&#39;_.</span>
<span class="sd">    mlp_dropout: float, Optional, default = None</span>
<span class="sd">        float with the dropout between the dense layers of the MLP.</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to 0.0.</span>
<span class="sd">    mlp_batchnorm: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_batchnorm_last: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to False.</span>
<span class="sd">    mlp_linear_first: bool, Optional, default = None</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>
<span class="sd">        If &#39;mlp_hidden_dims&#39; is not `None` and this parameter is `None`, it</span>
<span class="sd">        will default to `True`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: nn.Module</span>
<span class="sd">        Sequence of FasFormer blocks.</span>
<span class="sd">    mlp: nn.Module</span>
<span class="sd">        MLP component in the model</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import TabFastFormer</span>
<span class="sd">    &gt;&gt;&gt; X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; colnames = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; cat_embed_input = [(u,i) for u,i in zip(colnames[:4], [4]*4)]</span>
<span class="sd">    &gt;&gt;&gt; continuous_cols = [&#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k:v for v,k in enumerate(colnames)}</span>
<span class="sd">    &gt;&gt;&gt; model = TabFastFormer(column_idx=column_idx, cat_embed_input=cat_embed_input, continuous_cols=continuous_cols)</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_tab)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">share_qv_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">share_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TabFastFormer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
            <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
            <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
            <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
            <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
            <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
            <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
            <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
            <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
            <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
            <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
            <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
            <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
            <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
            <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">share_qv_weights</span> <span class="o">=</span> <span class="n">share_qv_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span> <span class="o">=</span> <span class="n">share_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

        <span class="c1"># Embeddings are instantiated at the base model</span>
        <span class="c1"># Transformer blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="n">first_fastformer_block</span> <span class="o">=</span> <span class="n">FastFormerEncoder</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="p">,</span>
            <span class="n">n_heads</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="p">,</span>
            <span class="n">attn_dropout</span><span class="p">,</span>
            <span class="n">ff_dropout</span><span class="p">,</span>
            <span class="n">ff_factor</span><span class="p">,</span>
            <span class="n">share_qv_weights</span><span class="p">,</span>
            <span class="n">transformer_activation</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;fastformer_block0&quot;</span><span class="p">,</span> <span class="n">first_fastformer_block</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">share_weights</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                    <span class="s2">&quot;fastformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">first_fastformer_block</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                    <span class="s2">&quot;fastformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="n">FastFormerEncoder</span><span class="p">(</span>
                        <span class="n">input_dim</span><span class="p">,</span>
                        <span class="n">n_heads</span><span class="p">,</span>
                        <span class="n">use_bias</span><span class="p">,</span>
                        <span class="n">attn_dropout</span><span class="p">,</span>
                        <span class="n">ff_dropout</span><span class="p">,</span>
                        <span class="n">ff_factor</span><span class="p">,</span>
                        <span class="n">share_qv_weights</span><span class="p">,</span>
                        <span class="n">transformer_activation</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
        <span class="c1"># therefore all related params are optional</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
                <span class="p">),</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
                <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
                <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
                <span class="p">),</span>
                <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights. Each element of the list is a</span>
<span class="sd">        tuple where the first and second elements are the $\alpha$</span>
<span class="sd">        and $\beta$ attention weights in the paper.</span>

<span class="sd">        The shape of the attention weights is $(N, H, F)$ where $N$ is the</span>
<span class="sd">        batch size, $H$ is the number of attention heads and $F$ is the</span>
<span class="sd">        number of features/columns in the dataset</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span><span class="p">:</span>
            <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weight</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">attention_weights</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List with the attention weights. Each element of the list is a
tuple where the first and second elements are the <span class="arithmatex">\(\alpha\)</span>
and <span class="arithmatex">\(\beta\)</span> attention weights in the paper.</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F)\)</span> where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">BasicRNN</span>


<a href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


        <p>Standard text classifier/regressor comprised by a stack of RNNs
(LSTMs or GRUs) that can be used as the <code>deeptext</code> component of a Wide &amp;
Deep model or independently by itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of
dense layers on top of the stack of RNNs</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of words in the vocabulary</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_matrix</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pretrained word embeddings</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_trainable</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the pretrained embeddings are trainable</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rnn_type</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[lstm, gru]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>String indicating the type of RNN to use. One of <em>'lstm'</em> or <em>'gru'</em></p>
              </div>
            </td>
            <td>
                  <code>&#39;lstm&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Hidden dim of the RNN</p>
              </div>
            </td>
            <td>
                  <code>64</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_layers</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of recurrent layers</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rnn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for each RNN layer except the last layer</p>
              </div>
            </td>
            <td>
                  <code>0.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bidirectional</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether the staked RNNs are bidirectional</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_hidden_state</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether to use the final hidden state or the RNN's
output as predicting features. Typically the former is used.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_idx</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's tokenizer
where the token index 0 is reserved for the <em>'unknown'</em> word token.
Therefore, the default value is set to 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm_last</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_linear_first</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.word_embed">word_embed</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>word embedding matrix</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.rnn">rnn</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stack of RNNs</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.rnn_mlp">rnn_mlp</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">BasicRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/text/rnns/basic_rnn.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BasicRNN</span><span class="p">(</span><span class="n">BaseWDModelComponent</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Standard text classifier/regressor comprised by a stack of RNNs</span>
<span class="sd">    (LSTMs or GRUs) that can be used as the `deeptext` component of a Wide &amp;</span>
<span class="sd">    Deep model or independently by itself.</span>

<span class="sd">    In addition, there is the option to add a Fully Connected (FC) set of</span>
<span class="sd">    dense layers on top of the stack of RNNs</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vocab_size: int</span>
<span class="sd">        Number of words in the vocabulary</span>
<span class="sd">    embed_dim: int, Optional, default = None</span>
<span class="sd">        Dimension of the word embeddings if non-pretained word vectors are</span>
<span class="sd">        used</span>
<span class="sd">    embed_matrix: np.ndarray, Optional, default = None</span>
<span class="sd">        Pretrained word embeddings</span>
<span class="sd">    embed_trainable: bool, default = True</span>
<span class="sd">        Boolean indicating if the pretrained embeddings are trainable</span>
<span class="sd">    rnn_type: str, default = &#39;lstm&#39;</span>
<span class="sd">        String indicating the type of RNN to use. One of _&#39;lstm&#39;_ or _&#39;gru&#39;_</span>
<span class="sd">    hidden_dim: int, default = 64</span>
<span class="sd">        Hidden dim of the RNN</span>
<span class="sd">    n_layers: int, default = 3</span>
<span class="sd">        Number of recurrent layers</span>
<span class="sd">    rnn_dropout: float, default = 0.1</span>
<span class="sd">        Dropout for each RNN layer except the last layer</span>
<span class="sd">    bidirectional: bool, default = True</span>
<span class="sd">        Boolean indicating whether the staked RNNs are bidirectional</span>
<span class="sd">    use_hidden_state: str, default = True</span>
<span class="sd">        Boolean indicating whether to use the final hidden state or the RNN&#39;s</span>
<span class="sd">        output as predicting features. Typically the former is used.</span>
<span class="sd">    padding_idx: int, default = 1</span>
<span class="sd">        index of the padding token in the padded-tokenised sequences. The</span>
<span class="sd">        `TextPreprocessor` class within this library uses fastai&#39;s tokenizer</span>
<span class="sd">        where the token index 0 is reserved for the _&#39;unknown&#39;_ word token.</span>
<span class="sd">        Therefore, the default value is set to 1.</span>
<span class="sd">    head_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the sizes of the dense layers in the head e.g: _[128, 64]_</span>
<span class="sd">    head_activation: str, default = &quot;relu&quot;</span>
<span class="sd">        Activation function for the dense layers in the head. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    head_dropout: float, Optional, default = None</span>
<span class="sd">        Dropout of the dense layers in the head</span>
<span class="sd">    head_batchnorm: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to include batch normalization in</span>
<span class="sd">        the dense layers that form the _&#39;rnn_mlp&#39;_</span>
<span class="sd">    head_batchnorm_last: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to apply batch normalization to the</span>
<span class="sd">        last of the dense layers in the head</span>
<span class="sd">    head_linear_first: bool, default = False</span>
<span class="sd">        Boolean indicating whether the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    word_embed: nn.Module</span>
<span class="sd">        word embedding matrix</span>
<span class="sd">    rnn: nn.Module</span>
<span class="sd">        Stack of RNNs</span>
<span class="sd">    rnn_mlp: nn.Module</span>
<span class="sd">        Stack of dense layers on top of the RNN. This will only exists if</span>
<span class="sd">        `head_layers_dim` is not None</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import BasicRNN</span>
<span class="sd">    &gt;&gt;&gt; X_text = torch.cat((torch.zeros([5,1]), torch.empty(5, 4).random_(1,4)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; model = BasicRNN(vocab_size=4, hidden_dim=4, n_layers=2, padding_idx=0, embed_dim=4)</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_text)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">rnn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_hidden_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BasicRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">embed_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If no &#39;embed_matrix&#39; is passed, the embedding dimension must&quot;</span>
                <span class="s2">&quot;be specified with &#39;embed_dim&#39;&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;the input embedding dimension </span><span class="si">{}</span><span class="s2"> and the dimension of the &quot;</span>
                <span class="s2">&quot;pretrained embeddings </span><span class="si">{}</span><span class="s2"> do not match. The pretrained embeddings &quot;</span>
                <span class="s2">&quot;dimension (</span><span class="si">{}</span><span class="s2">) will be used&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">),</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span> <span class="o">=</span> <span class="n">embed_trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_dropout</span> <span class="o">=</span> <span class="n">rnn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_hidden_state</span> <span class="o">=</span> <span class="n">use_hidden_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

        <span class="c1"># Embeddings</span>
        <span class="k">if</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_embeddings</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
            <span class="p">)</span>

        <span class="c1"># RNN</span>
        <span class="n">rnn_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">hidden_dim</span><span class="p">,</span>
            <span class="s2">&quot;num_layers&quot;</span><span class="p">:</span> <span class="n">n_layers</span><span class="p">,</span>
            <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">,</span>
            <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">rnn_dropout</span><span class="p">,</span>
            <span class="s2">&quot;batch_first&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bidirectional</span> <span class="k">else</span> <span class="n">hidden_dim</span>

        <span class="c1"># FC-Head (Mlp)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MLP</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">head_hidden_dims</span><span class="p">,</span>
                <span class="n">head_activation</span><span class="p">,</span>
                <span class="n">head_dropout</span><span class="p">,</span>
                <span class="n">head_batchnorm</span><span class="p">,</span>
                <span class="n">head_batchnorm_last</span><span class="p">,</span>
                <span class="n">head_linear_first</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># simple hack to add readability in the forward pass</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
            <span class="n">o</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
            <span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
            <span class="p">)</span>

        <span class="n">processed_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_rnn_outputs</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">(</span><span class="n">processed_outputs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_embeddings</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">embed_matrix</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;float32&quot;</span>
            <span class="p">),</span> <span class="s2">&quot;&#39;embed_matrix&#39; must be of dtype &#39;float32&#39;, got dtype &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="nb">str</span><span class="p">(</span><span class="n">embed_matrix</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">word_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span><span class="p">:</span>
                <span class="n">word_embed</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">word_embed</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>
            <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">word_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
            <span class="p">)</span>
            <span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span>

        <span class="k">return</span> <span class="n">word_embed</span><span class="p">,</span> <span class="n">embed_dim</span>

    <span class="k">def</span> <span class="nf">_process_rnn_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">hidden</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span><span class="p">:</span>
            <span class="n">processed_outputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_hidden_state</span>
                <span class="k">else</span> <span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">processed_outputs</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_hidden_state</span> <span class="k">else</span> <span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">processed_outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">AttentiveRNN</span>


<a href="#pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN">BasicRNN</a></code></p>


        <p>Text classifier/regressor comprised by a stack of RNNs
(LSTMs or GRUs) plus an attention layer. This model can be used as the
<code>deeptext</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of dense
layers on top of attention layer</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of words in the vocabulary</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_matrix</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pretrained word embeddings</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_trainable</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the pretrained embeddings are trainable</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rnn_type</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[lstm, gru]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>String indicating the type of RNN to use. One of <em>'lstm'</em> or <em>'gru'</em></p>
              </div>
            </td>
            <td>
                  <code>&#39;lstm&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Hidden dim of the RNN</p>
              </div>
            </td>
            <td>
                  <code>64</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_layers</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of recurrent layers</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rnn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout for each RNN layer except the last layer</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bidirectional</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether the staked RNNs are bidirectional</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_hidden_state</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether to use the final hidden state or the RNN's
output as predicting features. Typically the former is used.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_idx</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's
tokenizer where the token index 0 is reserved for the <em>'unknown'</em>
word token. Therefore, the default value is set to 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_concatenate</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the input to the attention mechanism will be the
output of the RNN or the output of the RNN concatenated with the last
hidden state.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Internal dropout for the attention mechanism</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm_last</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_linear_first</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.word_embed">word_embed</span></code></td>
            <td>
                  <code><span title="nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>word embedding matrix</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.rnn">rnn</span></code></td>
            <td>
                  <code><span title="nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stack of RNNs</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.rnn_mlp">rnn_mlp</span></code></td>
            <td>
                  <code><span title="nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not <code>None</code></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">AttentiveRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AttentiveRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/text/rnns/attentive_rnn.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AttentiveRNN</span><span class="p">(</span><span class="n">BasicRNN</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Text classifier/regressor comprised by a stack of RNNs</span>
<span class="sd">    (LSTMs or GRUs) plus an attention layer. This model can be used as the</span>
<span class="sd">    `deeptext` component of a Wide &amp; Deep model or independently by</span>
<span class="sd">    itself.</span>

<span class="sd">    In addition, there is the option to add a Fully Connected (FC) set of dense</span>
<span class="sd">    layers on top of attention layer</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vocab_size: int</span>
<span class="sd">        Number of words in the vocabulary</span>
<span class="sd">    embed_dim: int, Optional, default = None</span>
<span class="sd">        Dimension of the word embeddings if non-pretained word vectors are</span>
<span class="sd">        used</span>
<span class="sd">    embed_matrix: np.ndarray, Optional, default = None</span>
<span class="sd">        Pretrained word embeddings</span>
<span class="sd">    embed_trainable: bool, default = True</span>
<span class="sd">        Boolean indicating if the pretrained embeddings are trainable</span>
<span class="sd">    rnn_type: str, default = &#39;lstm&#39;</span>
<span class="sd">        String indicating the type of RNN to use. One of _&#39;lstm&#39;_ or _&#39;gru&#39;_</span>
<span class="sd">    hidden_dim: int, default = 64</span>
<span class="sd">        Hidden dim of the RNN</span>
<span class="sd">    n_layers: int, default = 3</span>
<span class="sd">        Number of recurrent layers</span>
<span class="sd">    rnn_dropout: float, default = 0.1</span>
<span class="sd">        Dropout for each RNN layer except the last layer</span>
<span class="sd">    bidirectional: bool, default = True</span>
<span class="sd">        Boolean indicating whether the staked RNNs are bidirectional</span>
<span class="sd">    use_hidden_state: str, default = True</span>
<span class="sd">        Boolean indicating whether to use the final hidden state or the RNN&#39;s</span>
<span class="sd">        output as predicting features. Typically the former is used.</span>
<span class="sd">    padding_idx: int, default = 1</span>
<span class="sd">        index of the padding token in the padded-tokenised sequences. The</span>
<span class="sd">        `TextPreprocessor` class within this library uses fastai&#39;s</span>
<span class="sd">        tokenizer where the token index 0 is reserved for the _&#39;unknown&#39;_</span>
<span class="sd">        word token. Therefore, the default value is set to 1.</span>
<span class="sd">    attn_concatenate: bool, default = True</span>
<span class="sd">        Boolean indicating if the input to the attention mechanism will be the</span>
<span class="sd">        output of the RNN or the output of the RNN concatenated with the last</span>
<span class="sd">        hidden state.</span>
<span class="sd">    attn_dropout: float, default = 0.1</span>
<span class="sd">        Internal dropout for the attention mechanism</span>
<span class="sd">    head_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the sizes of the dense layers in the head e.g: _[128, 64]_</span>
<span class="sd">    head_activation: str, default = &quot;relu&quot;</span>
<span class="sd">        Activation function for the dense layers in the head. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    head_dropout: float, Optional, default = None</span>
<span class="sd">        Dropout of the dense layers in the head</span>
<span class="sd">    head_batchnorm: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to include batch normalization in</span>
<span class="sd">        the dense layers that form the _&#39;rnn_mlp&#39;_</span>
<span class="sd">    head_batchnorm_last: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to apply batch normalization to the</span>
<span class="sd">        last of the dense layers in the head</span>
<span class="sd">    head_linear_first: bool, default = False</span>
<span class="sd">        Boolean indicating whether the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    word_embed: nn.Module</span>
<span class="sd">        word embedding matrix</span>
<span class="sd">    rnn: nn.Module</span>
<span class="sd">        Stack of RNNs</span>
<span class="sd">    rnn_mlp: nn.Module</span>
<span class="sd">        Stack of dense layers on top of the RNN. This will only exists if</span>
<span class="sd">        `head_layers_dim` is not `None`</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import AttentiveRNN</span>
<span class="sd">    &gt;&gt;&gt; X_text = torch.cat((torch.zeros([5,1]), torch.empty(5, 4).random_(1,4)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; model = AttentiveRNN(vocab_size=4, hidden_dim=4, n_layers=2, padding_idx=0, embed_dim=4)</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_text)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">rnn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_hidden_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">attn_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">embed_matrix</span><span class="o">=</span><span class="n">embed_matrix</span><span class="p">,</span>
            <span class="n">embed_trainable</span><span class="o">=</span><span class="n">embed_trainable</span><span class="p">,</span>
            <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
            <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
            <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
            <span class="n">rnn_dropout</span><span class="o">=</span><span class="n">rnn_dropout</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
            <span class="n">use_hidden_state</span><span class="o">=</span><span class="n">use_hidden_state</span><span class="p">,</span>
            <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span>
            <span class="n">head_hidden_dims</span><span class="o">=</span><span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="o">=</span><span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="o">=</span><span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="o">=</span><span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="o">=</span><span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Embeddings and RNN defined in the BasicRNN inherited class</span>

        <span class="c1"># Attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_concatenate</span> <span class="o">=</span> <span class="n">attn_concatenate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>

        <span class="k">if</span> <span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">attn_concatenate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">elif</span> <span class="n">bidirectional</span> <span class="ow">or</span> <span class="n">attn_concatenate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">ContextAttention</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="p">,</span> <span class="n">sum_along_seq</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># FC-Head (Mlp)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">head_hidden_dims</span><span class="p">,</span>
                <span class="n">head_activation</span><span class="p">,</span>
                <span class="n">head_dropout</span><span class="p">,</span>
                <span class="n">head_batchnorm</span><span class="p">,</span>
                <span class="n">head_batchnorm_last</span><span class="p">,</span>
                <span class="n">head_linear_first</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_rnn_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">hidden</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_concatenate</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span><span class="p">:</span>
                <span class="n">bi_hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">attn_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">output</span><span class="p">,</span> <span class="n">bi_hidden</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">output</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">attn_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">output</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_inp</span> <span class="o">=</span> <span class="n">output</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">attn_inp</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights</span>

<span class="sd">        The shape of the attention weights is $(N, S)$, where $N$ is the batch</span>
<span class="sd">        size and $S$ is the length of the sequence</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List with the attention weights</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, S)\)</span>, where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(S\)</span> is the length of the sequence</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">StackedAttentiveRNN</span>


<a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


        <p>Text classifier/regressor comprised by a stack of blocks:
<code>[RNN + Attention]</code>. This can be used as the <code>deeptext</code> component of a
Wide &amp; Deep model or independently by itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of
dense layers on top of the attentiob blocks</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of words in the vocabulary</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_dim</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_matrix</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pretrained word embeddings</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embed_trainable</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the pretrained embeddings are trainable</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rnn_type</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[lstm, gru]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>String indicating the type of RNN to use. One of 'lstm' or 'gru'</p>
              </div>
            </td>
            <td>
                  <code>&#39;lstm&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Hidden dim of the RNN</p>
              </div>
            </td>
            <td>
                  <code>64</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bidirectional</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether the staked RNNs are bidirectional</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_idx</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's
tokenizer where the token index 0 is reserved for the <em>'unknown'</em>
word token. Therefore, the default value is set to 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_blocks</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of attention blocks. Each block is comprised by an RNN and a
Context Attention Encoder</p>
              </div>
            </td>
            <td>
                  <code>3</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_concatenate</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the input to the attention mechanism will be the
output of the RNN or the output of the RNN concatenated with the last
hidden state or simply</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Internal dropout for the attention mechanism</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>with_addnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the output of each block will be added to the
input and normalised</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the sizes of the dense layers in the head e.g: [128, 64]</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm_last</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_linear_first</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.word_embed">word_embed</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>word embedding matrix</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.rnn">rnn</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stack of RNNs</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.rnn_mlp">rnn_mlp</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not <code>None</code></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">StackedAttentiveRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">StackedAttentiveRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/text/rnns/stacked_attentive_rnn.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">StackedAttentiveRNN</span><span class="p">(</span><span class="n">BaseWDModelComponent</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Text classifier/regressor comprised by a stack of blocks:</span>
<span class="sd">    `[RNN + Attention]`. This can be used as the `deeptext` component of a</span>
<span class="sd">    Wide &amp; Deep model or independently by itself.</span>

<span class="sd">    In addition, there is the option to add a Fully Connected (FC) set of</span>
<span class="sd">    dense layers on top of the attentiob blocks</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vocab_size: int</span>
<span class="sd">        Number of words in the vocabulary</span>
<span class="sd">    embed_dim: int, Optional, default = None</span>
<span class="sd">        Dimension of the word embeddings if non-pretained word vectors are</span>
<span class="sd">        used</span>
<span class="sd">    embed_matrix: np.ndarray, Optional, default = None</span>
<span class="sd">        Pretrained word embeddings</span>
<span class="sd">    embed_trainable: bool, default = True</span>
<span class="sd">        Boolean indicating if the pretrained embeddings are trainable</span>
<span class="sd">    rnn_type: str, default = &#39;lstm&#39;</span>
<span class="sd">        String indicating the type of RNN to use. One of &#39;lstm&#39; or &#39;gru&#39;</span>
<span class="sd">    hidden_dim: int, default = 64</span>
<span class="sd">        Hidden dim of the RNN</span>
<span class="sd">    bidirectional: bool, default = True</span>
<span class="sd">        Boolean indicating whether the staked RNNs are bidirectional</span>
<span class="sd">    padding_idx: int, default = 1</span>
<span class="sd">        index of the padding token in the padded-tokenised sequences. The</span>
<span class="sd">        `TextPreprocessor` class within this library uses fastai&#39;s</span>
<span class="sd">        tokenizer where the token index 0 is reserved for the _&#39;unknown&#39;_</span>
<span class="sd">        word token. Therefore, the default value is set to 1.</span>
<span class="sd">    n_blocks: int, default = 3</span>
<span class="sd">        Number of attention blocks. Each block is comprised by an RNN and a</span>
<span class="sd">        Context Attention Encoder</span>
<span class="sd">    attn_concatenate: bool, default = True</span>
<span class="sd">        Boolean indicating if the input to the attention mechanism will be the</span>
<span class="sd">        output of the RNN or the output of the RNN concatenated with the last</span>
<span class="sd">        hidden state or simply</span>
<span class="sd">    attn_dropout: float, default = 0.1</span>
<span class="sd">        Internal dropout for the attention mechanism</span>
<span class="sd">    with_addnorm: bool, default = False</span>
<span class="sd">        Boolean indicating if the output of each block will be added to the</span>
<span class="sd">        input and normalised</span>
<span class="sd">    head_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the sizes of the dense layers in the head e.g: [128, 64]</span>
<span class="sd">    head_activation: str, default = &quot;relu&quot;</span>
<span class="sd">        Activation function for the dense layers in the head. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    head_dropout: float, Optional, default = None</span>
<span class="sd">        Dropout of the dense layers in the head</span>
<span class="sd">    head_batchnorm: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to include batch normalization in</span>
<span class="sd">        the dense layers that form the _&#39;rnn_mlp&#39;_</span>
<span class="sd">    head_batchnorm_last: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to apply batch normalization to the</span>
<span class="sd">        last of the dense layers in the head</span>
<span class="sd">    head_linear_first: bool, default = False</span>
<span class="sd">        Boolean indicating whether the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    word_embed: nn.Module</span>
<span class="sd">        word embedding matrix</span>
<span class="sd">    rnn: nn.Module</span>
<span class="sd">        Stack of RNNs</span>
<span class="sd">    rnn_mlp: nn.Module</span>
<span class="sd">        Stack of dense layers on top of the RNN. This will only exists if</span>
<span class="sd">        `head_layers_dim` is not `None`</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import StackedAttentiveRNN</span>
<span class="sd">    &gt;&gt;&gt; X_text = torch.cat((torch.zeros([5,1]), torch.empty(5, 4).random_(1,4)), axis=1)</span>
<span class="sd">    &gt;&gt;&gt; model = StackedAttentiveRNN(vocab_size=4, hidden_dim=4, padding_idx=0, embed_dim=4)</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_text)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">attn_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StackedAttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;the input embedding dimension </span><span class="si">{}</span><span class="s2"> and the dimension of the &quot;</span>
                <span class="s2">&quot;pretrained embeddings </span><span class="si">{}</span><span class="s2"> do not match. The pretrained embeddings &quot;</span>
                <span class="s2">&quot;dimension (</span><span class="si">{}</span><span class="s2">) will be used&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">),</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span> <span class="o">=</span> <span class="n">embed_trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_concatenate</span> <span class="o">=</span> <span class="n">attn_concatenate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

        <span class="c1"># Embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_embeddings</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">)</span>

        <span class="c1"># Linear Projection: if embed_dim is different that the input of the</span>
        <span class="c1"># attention blocks we add a linear projection</span>
        <span class="k">if</span> <span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">attn_concatenate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">elif</span> <span class="n">bidirectional</span> <span class="ow">or</span> <span class="n">attn_concatenate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_proj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

        <span class="c1"># RNN</span>
        <span class="n">rnn_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span>
            <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">hidden_dim</span><span class="p">,</span>
            <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">,</span>
            <span class="s2">&quot;batch_first&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>

        <span class="c1"># FC-Head (Mlp)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">ContextAttentionEncoder</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span>
                    <span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="n">attn_concatenate</span><span class="p">,</span>
                    <span class="n">with_addnorm</span><span class="o">=</span><span class="n">with_addnorm</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">sum_along_seq</span><span class="o">=</span><span class="n">i</span> <span class="o">==</span> <span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Mlp</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MLP</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">head_hidden_dims</span><span class="p">,</span>
                <span class="n">head_activation</span><span class="p">,</span>
                <span class="n">head_dropout</span><span class="p">,</span>
                <span class="n">head_batchnorm</span><span class="p">,</span>
                <span class="n">head_batchnorm_last</span><span class="p">,</span>
                <span class="n">head_linear_first</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># simple hack to add readability in the forward pass</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">long</span><span class="p">()))</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
                    <span class="mi">2</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span>
                <span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">c</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">blk</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">        The shape of the attention weights is $(N, S)$ Where $N$ is the batch</span>
<span class="sd">        size and $S$ is the length of the sequence</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_set_embeddings</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">embed_matrix</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;float32&quot;</span>
            <span class="p">),</span> <span class="s2">&quot;&#39;embed_matrix&#39; must be of dtype &#39;float32&#39;, got dtype &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="nb">str</span><span class="p">(</span><span class="n">embed_matrix</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">word_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span><span class="p">:</span>
                <span class="n">word_embed</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">word_embed</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>
            <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">word_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
            <span class="p">)</span>
            <span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span>

        <span class="k">return</span> <span class="n">word_embed</span><span class="p">,</span> <span class="n">embed_dim</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, S)\)</span> Where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(S\)</span> is the length of the sequence</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">HFModel</span>


<a href="#pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


        <p>This class is a wrapper around the Hugging Face transformers library. It
can be used as the text component of a Wide &amp; Deep model or independently
by itself.</p>
<p>At the moment only models from the families BERT, RoBERTa, DistilBERT,
ALBERT and ELECTRA are supported. This is because this library is
designed to address classification and regression tasks and these are the
most 'popular' encoder-only models, which have proved to be those that
work best for these tasks.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model_name</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The model name from the transformers library e.g. 'bert-base-uncased'.
Currently supported models are those from the families: BERT, RoBERTa,
DistilBERT, ALBERT and ELECTRA.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cls_token</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether to use the [CLS] token or the mean of the
sequence of hidden states as the sentence embedding</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>trainable_parameters</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the names of the model parameters that will be trained. If
None, none of the parameters will be trainable</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to include batch normalization in the
dense layers that form the head</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm_last</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_linear_first</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>verbose</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If True, it will print information about the model</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional kwargs to be passed to the model</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel.head">head</span></code></td>
            <td>
                  <code><span title="nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stack of dense layers on top of the transformer. This will only exists
if <code>head_layers_dim</code> is not None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">HFModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">HFModel</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/text/huggingface_transformers/hf_model.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">HFModel</span><span class="p">(</span><span class="n">BaseWDModelComponent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This class is a wrapper around the Hugging Face transformers library. It</span>
<span class="sd">    can be used as the text component of a Wide &amp; Deep model or independently</span>
<span class="sd">    by itself.</span>

<span class="sd">    At the moment only models from the families BERT, RoBERTa, DistilBERT,</span>
<span class="sd">    ALBERT and ELECTRA are supported. This is because this library is</span>
<span class="sd">    designed to address classification and regression tasks and these are the</span>
<span class="sd">    most &#39;popular&#39; encoder-only models, which have proved to be those that</span>
<span class="sd">    work best for these tasks.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model_name: str</span>
<span class="sd">        The model name from the transformers library e.g. &#39;bert-base-uncased&#39;.</span>
<span class="sd">        Currently supported models are those from the families: BERT, RoBERTa,</span>
<span class="sd">        DistilBERT, ALBERT and ELECTRA.</span>
<span class="sd">    use_cls_token: bool, default = True</span>
<span class="sd">        Boolean indicating whether to use the [CLS] token or the mean of the</span>
<span class="sd">        sequence of hidden states as the sentence embedding</span>
<span class="sd">    trainable_parameters: List, Optional, default = None</span>
<span class="sd">        List with the names of the model parameters that will be trained. If</span>
<span class="sd">        None, none of the parameters will be trainable</span>
<span class="sd">    head_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the sizes of the dense layers in the head e.g: _[128, 64]_</span>
<span class="sd">    head_activation: str, default = &quot;relu&quot;</span>
<span class="sd">        Activation function for the dense layers in the head. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    head_dropout: float, Optional, default = None</span>
<span class="sd">        Dropout of the dense layers in the head</span>
<span class="sd">    head_batchnorm: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to include batch normalization in the</span>
<span class="sd">        dense layers that form the head</span>
<span class="sd">    head_batchnorm_last: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to apply batch normalization to the</span>
<span class="sd">        last of the dense layers in the head</span>
<span class="sd">    head_linear_first: bool, default = False</span>
<span class="sd">        Boolean indicating whether the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>
<span class="sd">    verbose: bool, default = False</span>
<span class="sd">        If True, it will print information about the model</span>
<span class="sd">    **kwargs</span>
<span class="sd">        Additional kwargs to be passed to the model</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    head: nn.Module</span>
<span class="sd">        Stack of dense layers on top of the transformer. This will only exists</span>
<span class="sd">        if `head_layers_dim` is not None</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import HFModel</span>
<span class="sd">    &gt;&gt;&gt; X_text = torch.cat((torch.zeros([5,1]), torch.empty(5, 4).random_(1,4)), axis=1).long()</span>
<span class="sd">    &gt;&gt;&gt; model = HFModel(model_name=&#39;bert-base-uncased&#39;)</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_text)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;use_cls_token&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;use_special_token&quot;</span><span class="p">])</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">use_cls_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">trainable_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># TO DO: add warning regarging ELECTRA as ELECTRA does not have a cls</span>
        <span class="c1"># token.  Research what happens with ELECTRA</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cls_token</span> <span class="o">=</span> <span class="n">use_cls_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span> <span class="o">=</span> <span class="n">trainable_parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cls_token</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The model will use the [CLS] token. Make sure the tokenizer &quot;</span>
                <span class="s2">&quot;was run with add_special_tokens=True&quot;</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model_class</span> <span class="o">=</span> <span class="n">get_model_class</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">get_config_and_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_attention_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="nb">any</span><span class="p">([</span><span class="n">tl</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">tl</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span><span class="p">])</span>

        <span class="c1"># FC-Head (Mlp). Note that the FC head will always be trainable</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">head_hidden_dims</span><span class="p">,</span>
                <span class="n">head_activation</span><span class="p">,</span>
                <span class="n">head_dropout</span><span class="p">,</span>
                <span class="n">head_batchnorm</span><span class="p">,</span>
                <span class="n">head_batchnorm_last</span><span class="p">,</span>
                <span class="n">head_linear_first</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>

        <span class="c1"># this is inefficient since the attention mask is returned by the</span>
        <span class="c1"># tokenizer, but all models in this library use a forward pass that</span>
        <span class="c1"># takes ONLY an input tensor. A fix will be addressed in a future</span>
        <span class="n">attn_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_attention_weights</span><span class="p">:</span>
            <span class="c1"># TO CONSIDER: attention weights as a returned object and not an</span>
            <span class="c1"># attribute</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn_weights</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">&quot;attentions&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cls_token</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Here one can choose to flatten, but unless the sequence length</span>
            <span class="c1"># is very small, flatten will result in a very large output</span>
            <span class="c1"># tensor.</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">attention_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the attention weights if the model was created with the</span>
<span class="sd">        output_attention_weights=True argument. If not, it will raise an</span>
<span class="sd">        AttributeError.</span>

<span class="sd">        The shape of the attention weights is $(N, H, F, F)$, where $N$ is the</span>
<span class="sd">        batch size, $H$ is the number of attention heads and $F$ is the</span>
<span class="sd">        sequence length.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_attention_weights</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;The output_attention_weights attribute was not set to True when creating the model object &quot;</span>
                <span class="s2">&quot;Please pass an output_attention_weights=True argument when creating the HFModel object&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_weights</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel.attention_weight" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weight</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel.attention_weight" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weight</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Returns the attention weights if the model was created with the
output_attention_weights=True argument. If not, it will raise an
AttributeError.</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
sequence length.</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.image.vision.Vision" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Vision</span>


<a href="#pytorch_widedeep.models.image.vision.Vision" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


        <p>Defines a standard image classifier/regressor using a pretrained
network or a sequence of convolution layers that can be used as the
<code>deepimage</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p><img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: this class represents the integration
 between <code>pytorch-widedeep</code> and <code>torchvision</code>. New architectures will be
 available as they are added to <code>torchvision</code>. In a distant future we aim
 to bring transformer-based architectures as well. However, simple
 CNN-based architectures (and even MLP-based) seem to produce SoTA
 results. For the time being, we describe below the options available
 through this class</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_setup</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[str, <span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.Union">Union</span>[str, <span title="pytorch_widedeep.wdtypes.WeightsEnum">WeightsEnum</span>]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Name of the pretrained model. Should be a variant of the following
architectures: <em>'resnet'</em>, <em>'shufflenet'</em>, <em>'resnext'</em>,
<em>'wide_resnet'</em>, <em>'regnet'</em>, <em>'densenet'</em>, <em>'mobilenetv3'</em>,
<em>'mobilenetv2'</em>, <em>'mnasnet'</em>, <em>'efficientnet'</em> and <em>'squeezenet'</em>. if
<code>pretrained_model_setup = None</code> a basic, fully trainable CNN will be
used. Alternatively, since Torchvision 0.13 one can use pretrained
models with different weigths. Therefore, <code>pretrained_model_setup</code> can
also be dictionary with the name of the model and the weights (e.g.
<code>{'resnet50': ResNet50_Weights.DEFAULT}</code> or
<code>{'resnet50': "IMAGENET1K_V2"}</code>). <br/> Aliased as <code>pretrained_model_name</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_trainable</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of trainable layers starting from the layer closer to the
output neuron(s). Note that this number DOES NOT take into account
the so-called <em>'head'</em> which is ALWAYS trainable. If
<code>trainable_params</code> is not None this parameter will be ignored</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>trainable_params</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of strings containing the names (or substring within the name) of
the parameters that will be trained. For example, if we use a
<em>'resnet18'</em> pretrained model and we set <code>trainable_params =
['layer4']</code> only the parameters of <em>'layer4'</em> of the network
(and the head, as mentioned before) will be trained. Note that
setting this or the previous parameter involves some knowledge of
the architecture used.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>channel_sizes</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of integers with the channel sizes of a CNN in case we choose not
to use a pretrained model</p>
              </div>
            </td>
            <td>
                  <code>[64, 128, 256, 512]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_sizes</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of integers with the kernel sizes of a CNN in case we choose not
to use a pretrained model. Must be of length equal to <code>len(channel_sizes) - 1</code>.</p>
              </div>
            </td>
            <td>
                  <code>[7, 3, 3, 3]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>strides</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of integers with the stride sizes of a CNN in case we choose not
to use a pretrained model. Must be of length equal to <code>len(channel_sizes) - 1</code>.</p>
              </div>
            </td>
            <td>
                  <code>[2, 1, 1, 1]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per dense layer in the head. e.g: <em>[64,32]</em></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>float indicating the dropout between the dense layers.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm_last</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_linear_first</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.image.vision.Vision.features">features</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The pretrained model or Standard CNN plus the optional head</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Vision</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Vision</span><span class="p">(</span><span class="n">channel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">kernel_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">head_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_img</span><span class="p">)</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/image/vision.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Vision</span><span class="p">(</span><span class="n">BaseWDModelComponent</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines a standard image classifier/regressor using a pretrained</span>
<span class="sd">    network or a sequence of convolution layers that can be used as the</span>
<span class="sd">    `deepimage` component of a Wide &amp; Deep model or independently by</span>
<span class="sd">    itself.</span>

<span class="sd">    :information_source: **NOTE**: this class represents the integration</span>
<span class="sd">     between `pytorch-widedeep` and `torchvision`. New architectures will be</span>
<span class="sd">     available as they are added to `torchvision`. In a distant future we aim</span>
<span class="sd">     to bring transformer-based architectures as well. However, simple</span>
<span class="sd">     CNN-based architectures (and even MLP-based) seem to produce SoTA</span>
<span class="sd">     results. For the time being, we describe below the options available</span>
<span class="sd">     through this class</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    pretrained_model_setup: Optional, str or dict, default = None</span>
<span class="sd">        Name of the pretrained model. Should be a variant of the following</span>
<span class="sd">        architectures: _&#39;resnet&#39;_, _&#39;shufflenet&#39;_, _&#39;resnext&#39;_,</span>
<span class="sd">        _&#39;wide_resnet&#39;_, _&#39;regnet&#39;_, _&#39;densenet&#39;_, _&#39;mobilenetv3&#39;_,</span>
<span class="sd">        _&#39;mobilenetv2&#39;_, _&#39;mnasnet&#39;_, _&#39;efficientnet&#39;_ and _&#39;squeezenet&#39;_. if</span>
<span class="sd">        `pretrained_model_setup = None` a basic, fully trainable CNN will be</span>
<span class="sd">        used. Alternatively, since Torchvision 0.13 one can use pretrained</span>
<span class="sd">        models with different weigths. Therefore, `pretrained_model_setup` can</span>
<span class="sd">        also be dictionary with the name of the model and the weights (e.g.</span>
<span class="sd">        `{&#39;resnet50&#39;: ResNet50_Weights.DEFAULT}` or</span>
<span class="sd">        `{&#39;resnet50&#39;: &quot;IMAGENET1K_V2&quot;}`). &lt;br/&gt; Aliased as `pretrained_model_name`.</span>
<span class="sd">    n_trainable: Optional, int, default = None</span>
<span class="sd">        Number of trainable layers starting from the layer closer to the</span>
<span class="sd">        output neuron(s). Note that this number DOES NOT take into account</span>
<span class="sd">        the so-called _&#39;head&#39;_ which is ALWAYS trainable. If</span>
<span class="sd">        `trainable_params` is not None this parameter will be ignored</span>
<span class="sd">    trainable_params: Optional, list, default = None</span>
<span class="sd">        List of strings containing the names (or substring within the name) of</span>
<span class="sd">        the parameters that will be trained. For example, if we use a</span>
<span class="sd">        _&#39;resnet18&#39;_ pretrained model and we set `trainable_params =</span>
<span class="sd">        [&#39;layer4&#39;]` only the parameters of _&#39;layer4&#39;_ of the network</span>
<span class="sd">        (and the head, as mentioned before) will be trained. Note that</span>
<span class="sd">        setting this or the previous parameter involves some knowledge of</span>
<span class="sd">        the architecture used.</span>
<span class="sd">    channel_sizes: list, default = [64, 128, 256, 512]</span>
<span class="sd">        List of integers with the channel sizes of a CNN in case we choose not</span>
<span class="sd">        to use a pretrained model</span>
<span class="sd">    kernel_sizes: list or int, default = 3</span>
<span class="sd">        List of integers with the kernel sizes of a CNN in case we choose not</span>
<span class="sd">        to use a pretrained model. Must be of length equal to `len(channel_sizes) - 1`.</span>
<span class="sd">    strides: list or int, default = 1</span>
<span class="sd">        List of integers with the stride sizes of a CNN in case we choose not</span>
<span class="sd">        to use a pretrained model. Must be of length equal to `len(channel_sizes) - 1`.</span>
<span class="sd">    head_hidden_dims: Optional, list, default = None</span>
<span class="sd">        List with the number of neurons per dense layer in the head. e.g: _[64,32]_</span>
<span class="sd">    head_activation: str, default = &quot;relu&quot;</span>
<span class="sd">        Activation function for the dense layers in the head. Currently</span>
<span class="sd">        _&#39;tanh&#39;_, _&#39;relu&#39;_, _&#39;leaky_relu&#39;_ and _&#39;gelu&#39;_ are supported</span>
<span class="sd">    head_dropout: float, default = 0.1</span>
<span class="sd">        float indicating the dropout between the dense layers.</span>
<span class="sd">    head_batchnorm: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the dense layers</span>
<span class="sd">    head_batchnorm_last: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not batch normalization will be applied</span>
<span class="sd">        to the last of the dense layers</span>
<span class="sd">    head_linear_first: bool, default = False</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    features: nn.Module</span>
<span class="sd">        The pretrained model or Standard CNN plus the optional head</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import Vision</span>
<span class="sd">    &gt;&gt;&gt; X_img = torch.rand((2,3,224,224))</span>
<span class="sd">    &gt;&gt;&gt; model = Vision(channel_sizes=[64, 128], kernel_sizes = [3, 3], strides=[1, 1], head_hidden_dims=[32, 8])</span>
<span class="sd">    &gt;&gt;&gt; out = model(X_img)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;pretrained_model_setup&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;pretrained_model_name&quot;</span><span class="p">])</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_setup</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">WeightsEnum</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_trainable</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trainable_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">channel_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
        <span class="n">kernel_sizes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="n">strides</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Vision</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_pretrained_model_setup</span><span class="p">(</span>
            <span class="n">pretrained_model_setup</span><span class="p">,</span> <span class="n">n_trainable</span><span class="p">,</span> <span class="n">trainable_params</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span> <span class="o">=</span> <span class="n">pretrained_model_setup</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trainable</span> <span class="o">=</span> <span class="n">n_trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable_params</span> <span class="o">=</span> <span class="n">trainable_params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">channel_sizes</span> <span class="o">=</span> <span class="n">channel_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_sizes</span> <span class="o">=</span> <span class="n">kernel_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">strides</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_features</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">pretrained_model_setup</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vision_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">head_hidden_dims</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">        neccesary to build the `WideDeep` class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_features</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span> <span class="ow">in</span> <span class="n">allowed_pretrained_models</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">model</span> <span class="o">=</span> <span class="n">allowed_pretrained_models</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span><span class="p">]</span>
                    <span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="n">model</span><span class="p">](</span>
                        <span class="n">weights</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">get_model_weights</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">DEFAULT</span>
                    <span class="p">)</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span><span class="si">}</span><span class="s2"> defaulting to </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="ne">UserWarning</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span>
                    <span class="p">](</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V1&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                <span class="n">model_name</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span><span class="p">))</span>
                <span class="n">model_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">allowed_pretrained_models</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">model_name</span> <span class="o">=</span> <span class="n">allowed_pretrained_models</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span>

                <span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="n">model_name</span><span class="p">](</span>
                    <span class="n">weights</span><span class="o">=</span><span class="n">model_weights</span>
                <span class="p">)</span>
            <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_backbone_output_dim</span><span class="p">(</span><span class="n">pretrained_model</span><span class="p">)</span>
            <span class="n">features</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">pretrained_model</span><span class="o">.</span><span class="n">children</span><span class="p">())[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_basic_cnn</span><span class="p">()</span>
            <span class="n">output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">channel_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">features</span><span class="p">,</span> <span class="n">output_dim</span>

    <span class="k">def</span> <span class="nf">_basic_cnn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">channel_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">channel_sizes</span>
        <span class="n">kernel_sizes</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_sizes</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">channel_sizes</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_sizes</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_sizes</span>
        <span class="p">)</span>
        <span class="n">strides</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">channel_sizes</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>
        <span class="p">)</span>

        <span class="n">BasicCNN</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">channel_sizes</span><span class="p">)):</span>
            <span class="n">BasicCNN</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;conv_layer_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">conv_layer</span><span class="p">(</span>
                    <span class="n">channel_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">channel_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">kernel_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">strides</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">maxpool</span><span class="o">=</span><span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="n">adaptiveavgpool</span><span class="o">=</span><span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">channel_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">BasicCNN</span>

    <span class="k">def</span> <span class="nf">_freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">features</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">tl</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">tl</span> <span class="ow">in</span> <span class="n">name</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_trainable</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>
            <span class="p">):</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_trainable</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Both &#39;trainable_params&#39; and &#39;n_trainable&#39; are &#39;None&#39; and the entire network will be trained&quot;</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_backbone_output_dim</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">features</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">features</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_modules&quot;</span><span class="p">][</span><span class="s2">&quot;0&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">in_features</span>
            <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">features</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_modules&quot;</span><span class="p">][</span><span class="s2">&quot;1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">in_features</span>
                <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">features</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_modules&quot;</span><span class="p">][</span><span class="s2">&quot;1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">in_channels</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_check_pretrained_model_setup</span><span class="p">(</span>
        <span class="n">pretrained_model_setup</span><span class="p">,</span> <span class="n">n_trainable</span><span class="p">,</span> <span class="n">trainable_params</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">pretrained_model_setup</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_setup</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="n">pretrained_model_setup</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_setup</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                <span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pretrained_model_setup</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">pretrained_model_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">valid_pretrained_model_name</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
                <span class="p">[</span><span class="n">name</span> <span class="ow">in</span> <span class="n">pretrained_model_name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">allowed_pretrained_models</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">valid_pretrained_model_name</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pretrained_model_setup</span><span class="si">}</span><span class="s2"> is not among the allowed pretrained models.&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; These are </span><span class="si">{</span><span class="n">allowed_pretrained_models</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">. Please choose a variant of these architectures&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">n_trainable</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">trainable_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">UserWarning</span><span class="p">(</span>
                    <span class="s2">&quot;Both &#39;n_trainable&#39; and &#39;trainable_params&#39; are not None. &#39;trainable_params&#39; will be used&quot;</span>
                <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.image.vision.Vision.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.image.vision.Vision.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.model_fusion.ModelFuser" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ModelFuser</span>


<a href="#pytorch_widedeep.models.model_fusion.ModelFuser" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


        <p>This class is a wrapper around a list of models that are associated to the
different text and/or image columns (and datasets) The class is designed
to 'fuse' the models using a variety of methods.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>models</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of models whose outputs will be fused</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>fusion_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[concatenate, mean, max, sum, mult, dot, <span title="pytorch_widedeep.models.model_fusion.ModelFuser.head">head</span>], <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[concatenate, mean, max, sum, mult, <span title="pytorch_widedeep.models.model_fusion.ModelFuser.head">head</span>]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method to fuse the output of the models. It can be one of
['concatenate', 'mean', 'max', 'sum', 'mult', 'dot', 'head'] or a
list of those, but 'dot'. If a list is provided the output of the
models will be fused using all the methods in the list and the final
output will be the concatenation of the outputs of each method</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>projection_method</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[min, max, mean]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If the fusion_method is not 'concatenate', this parameter will
determine how to project the output of the models to a common
dimension. It can be one of ['min', 'max', 'mean']. Default is None</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>custom_head</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>, <span title="torch.nn.Module">Module</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Custom head to be used to fuse the output of the models. If provided,
this will take precedence over head_hidden_dims. Also, if
provided, 'projection_method' will be ignored.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the number of neurons per layer in the custom head. If
custom_head is provided, this parameter will be ignored</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_activation</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function to be used in the custom head. Default is None</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_dropout</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout to be used in the custom head. Default is None</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to use batchnorm in the custom head. Default is None</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm_last</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether or not batch normalization will be applied to the last of the
dense layers</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_linear_first</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="pytorch_widedeep.models.model_fusion.ModelFuser.head">head</span></code></td>
            <td>
                  <code><span title="torch.nn.Module">Module</span> or <span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Custom head to be used to fuse the output of the models. If
custom_head is provided, this will take precedence over
head_hidden_dims</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">TextPreprocessor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">BasicRNN</span><span class="p">,</span> <span class="n">ModelFuser</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text_col1&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;hello world&#39;</span><span class="p">,</span> <span class="s1">&#39;this is a test&#39;</span><span class="p">],</span>
<span class="gp">... </span><span class="s1">&#39;text_col2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;goodbye world&#39;</span><span class="p">,</span> <span class="s1">&#39;this is another test&#39;</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_preprocessor_1</span> <span class="o">=</span> <span class="n">TextPreprocessor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">text_col</span><span class="o">=</span><span class="s2">&quot;text_col1&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_vocab</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">maxlen</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_preprocessor_2</span> <span class="o">=</span> <span class="n">TextPreprocessor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">text_col</span><span class="o">=</span><span class="s2">&quot;text_col2&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_vocab</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">maxlen</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text1</span> <span class="o">=</span> <span class="n">text_preprocessor_1</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text2</span> <span class="o">=</span> <span class="n">text_preprocessor_2</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text1_tnsr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_text1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text2_tnsr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_text2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rnn1</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">text_preprocessor_1</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rnn2</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">text_preprocessor_2</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fused_model</span> <span class="o">=</span> <span class="n">ModelFuser</span><span class="p">(</span><span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="n">rnn1</span><span class="p">,</span> <span class="n">rnn2</span><span class="p">],</span> <span class="n">fusion_method</span><span class="o">=</span><span class="s1">&#39;concatenate&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">fused_model</span><span class="p">([</span><span class="n">X_text1_tnsr</span><span class="p">,</span> <span class="n">X_text2_tnsr</span><span class="p">])</span>
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/model_fusion.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ModelFuser</span><span class="p">(</span><span class="n">BaseWDModelComponent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class is a wrapper around a list of models that are associated to the</span>
<span class="sd">    different text and/or image columns (and datasets) The class is designed</span>
<span class="sd">    to &#39;fuse&#39; the models using a variety of methods.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    models: List[BaseWDModelComponent]</span>
<span class="sd">        List of models whose outputs will be fused</span>
<span class="sd">    fusion_method: Union[str, List[str]]</span>
<span class="sd">        Method to fuse the output of the models. It can be one of</span>
<span class="sd">        [&#39;concatenate&#39;, &#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;, &#39;mult&#39;, &#39;dot&#39;, &#39;head&#39;] or a</span>
<span class="sd">        list of those, but &#39;dot&#39;. If a list is provided the output of the</span>
<span class="sd">        models will be fused using all the methods in the list and the final</span>
<span class="sd">        output will be the concatenation of the outputs of each method</span>
<span class="sd">    projection_method: Optional[str]</span>
<span class="sd">        If the fusion_method is not &#39;concatenate&#39;, this parameter will</span>
<span class="sd">        determine how to project the output of the models to a common</span>
<span class="sd">        dimension. It can be one of [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;]. Default is None</span>
<span class="sd">    custom_head: Optional[BaseWDModelComponent | nn.Module]</span>
<span class="sd">        Custom head to be used to fuse the output of the models. If provided,</span>
<span class="sd">        this will take precedence over head_hidden_dims. Also, if</span>
<span class="sd">        provided, &#39;projection_method&#39; will be ignored.</span>
<span class="sd">    head_hidden_dims: Optional[List[int]]</span>
<span class="sd">        List with the number of neurons per layer in the custom head. If</span>
<span class="sd">        custom_head is provided, this parameter will be ignored</span>
<span class="sd">    head_activation: Optional[str]</span>
<span class="sd">        Activation function to be used in the custom head. Default is None</span>
<span class="sd">    head_dropout: Optional[float]</span>
<span class="sd">        Dropout to be used in the custom head. Default is None</span>
<span class="sd">    head_batchnorm: Optional[bool]</span>
<span class="sd">        Whether to use batchnorm in the custom head. Default is None</span>
<span class="sd">    head_batchnorm_last: Optional[bool]</span>
<span class="sd">        Whether or not batch normalization will be applied to the last of the</span>
<span class="sd">        dense layers</span>
<span class="sd">    head_linear_first: Optional[bool]</span>
<span class="sd">        Boolean indicating the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    head: nn.Module or BaseWDModelComponent</span>
<span class="sd">        Custom head to be used to fuse the output of the models. If</span>
<span class="sd">        custom_head is provided, this will take precedence over</span>
<span class="sd">        head_hidden_dims</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.preprocessing import TextPreprocessor</span>
<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import BasicRNN, ModelFuser</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; import pandas as pd</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; df = pd.DataFrame({&#39;text_col1&#39;: [&#39;hello world&#39;, &#39;this is a test&#39;],</span>
<span class="sd">    ... &#39;text_col2&#39;: [&#39;goodbye world&#39;, &#39;this is another test&#39;]})</span>
<span class="sd">    &gt;&gt;&gt; text_preprocessor_1 = TextPreprocessor(</span>
<span class="sd">    ...     text_col=&quot;text_col1&quot;,</span>
<span class="sd">    ...     max_vocab=10,</span>
<span class="sd">    ...     min_freq=1,</span>
<span class="sd">    ...     maxlen=5,</span>
<span class="sd">    ...     n_cpus=1,</span>
<span class="sd">    ...     verbose=0)</span>
<span class="sd">    &gt;&gt;&gt; text_preprocessor_2 = TextPreprocessor(</span>
<span class="sd">    ...     text_col=&quot;text_col2&quot;,</span>
<span class="sd">    ...     max_vocab=10,</span>
<span class="sd">    ...     min_freq=1,</span>
<span class="sd">    ...     maxlen=5,</span>
<span class="sd">    ...     n_cpus=1,</span>
<span class="sd">    ...     verbose=0)</span>
<span class="sd">    &gt;&gt;&gt; X_text1 = text_preprocessor_1.fit_transform(df)</span>
<span class="sd">    &gt;&gt;&gt; X_text2 = text_preprocessor_2.fit_transform(df)</span>
<span class="sd">    &gt;&gt;&gt; X_text1_tnsr = torch.from_numpy(X_text1)</span>
<span class="sd">    &gt;&gt;&gt; X_text2_tnsr = torch.from_numpy(X_text2)</span>
<span class="sd">    &gt;&gt;&gt; rnn1 = BasicRNN(</span>
<span class="sd">    ...     vocab_size=len(text_preprocessor_1.vocab.itos),</span>
<span class="sd">    ...     embed_dim=4,</span>
<span class="sd">    ...     hidden_dim=4,</span>
<span class="sd">    ...     n_layers=1,</span>
<span class="sd">    ...     bidirectional=False)</span>
<span class="sd">    &gt;&gt;&gt; rnn2 = BasicRNN(</span>
<span class="sd">    ...     vocab_size=len(text_preprocessor_2.vocab.itos),</span>
<span class="sd">    ...     embed_dim=4,</span>
<span class="sd">    ...     hidden_dim=4,</span>
<span class="sd">    ...     n_layers=1,</span>
<span class="sd">    ...     bidirectional=False)</span>
<span class="sd">    &gt;&gt;&gt; fused_model = ModelFuser(models=[rnn1, rnn2], fusion_method=&#39;concatenate&#39;)</span>
<span class="sd">    &gt;&gt;&gt; out = fused_model([X_text1_tnsr, X_text2_tnsr])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">fusion_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">Literal</span><span class="p">[</span>
                <span class="s2">&quot;concatenate&quot;</span><span class="p">,</span>
                <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
                <span class="s2">&quot;max&quot;</span><span class="p">,</span>
                <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
                <span class="s2">&quot;mult&quot;</span><span class="p">,</span>
                <span class="s2">&quot;dot&quot;</span><span class="p">,</span>
                <span class="s2">&quot;head&quot;</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;concatenate&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="s2">&quot;mult&quot;</span><span class="p">,</span> <span class="s2">&quot;head&quot;</span><span class="p">]],</span>
        <span class="p">],</span>
        <span class="n">projection_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_head</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ModelFuser</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="o">=</span> <span class="n">fusion_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">=</span> <span class="n">projection_method</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">all_output_dim_equal</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">check_input_parameters</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;head&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">custom_head</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;When using &#39;head&#39; as fusion_method, either head_hidden_dims or custom_head must be provided&quot;</span>
            <span class="k">if</span> <span class="n">custom_head</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># custom_head takes precedence over head_hidden_dims (in case</span>
                <span class="c1"># both are provided)</span>
                <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span>
                    <span class="n">custom_head</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span>
                <span class="p">),</span> <span class="s2">&quot;custom_head must have an &#39;output_dim&#39; property&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">custom_head</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                    <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="nb">sum</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">])]</span>
                    <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">,</span>
                    <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                        <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span>
                    <span class="p">),</span>
                    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span><span class="p">,</span>
                    <span class="n">batchnorm</span><span class="o">=</span><span class="p">(</span>
                        <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span>
                    <span class="p">),</span>
                    <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                        <span class="kc">False</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span>
                    <span class="p">),</span>
                    <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                        <span class="kc">True</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="ow">is</span> <span class="kc">None</span>
                        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span>
                    <span class="p">),</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>  <span class="c1"># noqa: C901</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;head&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_head_fusion</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;dot&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dot_fusion</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_other_fusions</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_head_fusion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">,</span> <span class="n">X</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_dot_fusion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="p">),</span> <span class="s2">&quot;When using &#39;dot&#39; as fusion_method, only two models can be fused&quot;</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">,</span> <span class="n">X</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_other_fusions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">fusion_methods</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span>
        <span class="p">)</span>
        <span class="n">fused_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_apply_fusion_method</span><span class="p">(</span><span class="n">fm</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">fm</span> <span class="ow">in</span> <span class="n">fusion_methods</span><span class="p">]</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">fused_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">fused_outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">fused_outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_apply_fusion_method</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fusion_method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;concatenate&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">,</span> <span class="n">X</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">model_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">,</span> <span class="n">X</span><span class="p">)]</span>
        <span class="n">projections</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_project</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">)</span>
        <span class="n">stacked</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">projections</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">stacked</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">stacked</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">stacked</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stacked</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;mult&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">stacked</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported fusion method: </span><span class="si">{</span><span class="n">fusion_method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_project</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Projects the output of the models to a common dimension.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_output_dim_equal</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">X</span>

        <span class="n">output_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span><span class="p">:</span>
            <span class="n">proj_dim</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">output_dims</span><span class="p">)</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">output_dims</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">proj_dim</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
            <span class="n">proj_dim</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">output_dims</span><span class="p">)</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">output_dims</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">proj_dim</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="n">proj_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">output_dims</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_dims</span><span class="p">))</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;projection_method must be one of [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;]&quot;</span><span class="p">)</span>

        <span class="n">x_proj</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">idx</span><span class="p">:</span>
                <span class="n">x_proj</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x_proj</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">output_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">proj_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">x_proj</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the output dimension of the model.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;head&quot;</span><span class="p">:</span>
            <span class="n">output_dim</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;head_hidden_dims&quot;</span><span class="p">)</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">output_dim</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;dot&quot;</span><span class="p">:</span>
            <span class="n">output_dim</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_dim</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">fusion_methods</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fusion_methods</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span>  <span class="c1"># type: ignore</span>
            <span class="k">for</span> <span class="n">fm</span> <span class="ow">in</span> <span class="n">fusion_methods</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fm</span> <span class="o">==</span> <span class="s2">&quot;concatenate&quot;</span><span class="p">:</span>
                    <span class="n">output_dim</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">])</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
                    <span class="n">output_dim</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span>
                        <span class="nb">sum</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">])</span>
                        <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span><span class="p">:</span>
                    <span class="n">output_dim</span> <span class="o">+=</span> <span class="nb">min</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">])</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
                    <span class="n">output_dim</span> <span class="o">+=</span> <span class="nb">max</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">])</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_output_dim_equal</span><span class="p">:</span>
                    <span class="n">output_dim</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">output_dim</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;projection_method must be one of [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;]&quot;</span>
                    <span class="p">)</span>

        <span class="k">return</span> <span class="n">output_dim</span>

    <span class="k">def</span> <span class="nf">check_input_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_tabnet</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_fusion_methods</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_projection_requirements</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_head_dot_exclusivity</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_validate_tabnet</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">TabNet</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;TabNet is not supported in ModelFuser. &quot;</span>
                <span class="s2">&quot;Please, use another model for tabular data&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_fusion_methods</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">valid_methods</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;concatenate&quot;</span><span class="p">,</span>
            <span class="s2">&quot;min&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mult&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dot&quot;</span><span class="p">,</span>
            <span class="s2">&quot;head&quot;</span><span class="p">,</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_methods</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;fusion_method must be one of [&#39;concatenate&#39;, &#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;, &#39;mult&#39;, &#39;dot&#39;, &#39;head&#39;] &quot;</span>
                    <span class="s2">&quot;or a list of any those but &#39;dot&#39;&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">fm</span> <span class="ow">in</span> <span class="n">valid_methods</span> <span class="k">for</span> <span class="n">fm</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;fusion_method must be one of [&#39;concatenate&#39;, &#39;mean&#39;, &#39;max&#39;, &#39;sum&#39;, &#39;mult&#39;, &#39;dot&#39;, &#39;head&#39;] &quot;</span>
                    <span class="s2">&quot;or a list of those but &#39;dot&#39;&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_projection_requirements</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">needs_projection</span> <span class="o">=</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_output_dim_equal</span>
        <span class="n">has_size_dependent_method</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">has_size_dependent_method</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">has_size_dependent_method</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
                <span class="n">fm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">fm</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">has_size_dependent_method</span> <span class="ow">and</span> <span class="n">needs_projection</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;If &#39;fusion_method&#39; is not &#39;concatenate&#39; or &#39;head&#39;, &quot;</span>
                    <span class="s2">&quot;and the output dimensions of the models are not equal, &quot;</span>
                    <span class="s2">&quot;&#39;projection_method&#39; must be provided&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;projection_method must be one of [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;]&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_head_dot_exclusivity</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;head&quot;</span><span class="p">,</span> <span class="s2">&quot;dot&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When using &#39;head&#39; or &#39;dot&#39; as fusion_method, no other method should be provided&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">proj</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">proj</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Fusion method: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span><span class="si">}</span><span class="s2">. Projection method: </span><span class="si">{</span><span class="n">proj</span><span class="si">}</span><span class="se">\n</span><span class="s2">Fused Models:</span><span class="se">\n</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.model_fusion.ModelFuser.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.model_fusion.ModelFuser.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Returns the output dimension of the model.</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.wide_deep.WideDeep" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">WideDeep</span>


<a href="#pytorch_widedeep.models.wide_deep.WideDeep" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Main collector class that combines all <code>wide</code>, <code>deeptabular</code>
<code>deeptext</code> and <code>deepimage</code> models.</p>
<p>Note that all models described so far in this library must be passed to
the <code>WideDeep</code> class once constructed. This is because the models output
the last layer before the prediction layer. Such prediction layer is
added by the <code>WideDeep</code> class as it collects the components for every
data mode.</p>
<p>There are two options to combine these models that correspond to the
two main architectures that <code>pytorch-widedeep</code> can build.</p>
<ul>
<li>
<p>Directly connecting the output of the model components to an ouput neuron(s).</p>
</li>
<li>
<p>Adding a <code>Fully-Connected Head</code> (FC-Head) on top of the deep models.
  This FC-Head will combine the output form the <code>deeptabular</code>, <code>deeptext</code> and
  <code>deepimage</code> and will be then connected to the output neuron(s).</p>
</li>
</ul>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>wide</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="torch.nn.Module">Module</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p><code>Wide</code> model. This is a linear model where the non-linearities are
captured via crossed-columns.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>deeptabular</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>, <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Currently this library implements a number of possible architectures
for the <code>deeptabular</code> component. See the documenation of the
package. Note that <code>deeptabular</code> can be a list of models. This is
useful when using multiple tabular inputs (e.g. for example in the
context of a two-tower model for recommendation systems)</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>deeptext</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>, <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Currently this library implements a number of possible architectures
for the <code>deeptext</code> component. See the documenation of the
package. Note that <code>deeptext</code> can be a list of models. This is useful
when using multiple text inputs.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>deepimage</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>, <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Currently this library uses <code>torchvision</code> and implements a number of
possible architectures for the <code>deepimage</code> component. See the
documenation of the package. Note that <code>deepimage</code> can be a list of
models. This is useful when using multiple image inputs.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>deephead</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Alternatively, the user can pass a custom model that will receive the
output of the deep component. If <code>deephead</code> is not None all the
previous fc-head parameters will be ignored</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_hidden_dims</code>
            </td>
            <td>
                  <code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List with the sizes of the dense layers in the head e.g: [128, 64]</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function for the dense layers in the head. Currently
<code>'tanh'</code>, <code>'relu'</code>, <code>'leaky_relu'</code> and <code>'gelu'</code> are supported</p>
              </div>
            </td>
            <td>
                  <code>&#39;relu&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_dropout</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dropout of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <code>'rnn_mlp'</code></p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_batchnorm_last</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_linear_first</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>enforce_positive</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean indicating if the output from the final layer must be
positive. This is important if you are using loss functions with
non-negative input restrictions, e.g. RMSLE, or if you know your
predictions are bounded in between 0 and inf</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>enforce_positive_activation</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Activation function to enforce that the final layer has a positive
output. <code>'softplus'</code> or <code>'relu'</code> are supported.</p>
              </div>
            </td>
            <td>
                  <code>&#39;softplus&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pred_dim</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the final wide and deep output layer containing the
predictions. <code>1</code> for regression and binary classification or number
of classes for multiclass classification.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnet</span><span class="p">,</span> <span class="n">Vision</span><span class="p">,</span> <span class="n">BasicRNN</span><span class="p">,</span> <span class="n">Wide</span><span class="p">,</span> <span class="n">WideDeep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">][:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deeptabular</span> <span class="o">=</span> <span class="n">TabResnet</span><span class="p">(</span><span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">embed_input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deeptext</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deepimage</span> <span class="o">=</span> <span class="n">Vision</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">WideDeep</span><span class="p">(</span><span class="n">wide</span><span class="o">=</span><span class="n">wide</span><span class="p">,</span> <span class="n">deeptabular</span><span class="o">=</span><span class="n">deeptabular</span><span class="p">,</span> <span class="n">deeptext</span><span class="o">=</span><span class="n">deeptext</span><span class="p">,</span> <span class="n">deepimage</span><span class="o">=</span><span class="n">deepimage</span><span class="p">)</span>
</code></pre></div>
    <p><img alt="" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: It is possible to use custom components to
 build Wide &amp; Deep models. Simply, build them and pass them as the
 corresponding parameters. Note that the custom models MUST return a last
 layer of activations(i.e. not the final prediction) so that  these
 activations are collected by <code>WideDeep</code> and combined accordingly. In
 addition, the models MUST also contain an attribute <code>output_dim</code> with
 the size of these last layers of activations. See for example
 <code>pytorch_widedeep.models.tab_mlp.TabMlp</code></p>






              <details class="quote">
                <summary>Source code in <code>pytorch_widedeep/models/wide_deep.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">WideDeep</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Main collector class that combines all `wide`, `deeptabular`</span>
<span class="sd">    `deeptext` and `deepimage` models.</span>

<span class="sd">    Note that all models described so far in this library must be passed to</span>
<span class="sd">    the `WideDeep` class once constructed. This is because the models output</span>
<span class="sd">    the last layer before the prediction layer. Such prediction layer is</span>
<span class="sd">    added by the `WideDeep` class as it collects the components for every</span>
<span class="sd">    data mode.</span>

<span class="sd">    There are two options to combine these models that correspond to the</span>
<span class="sd">    two main architectures that `pytorch-widedeep` can build.</span>

<span class="sd">    - Directly connecting the output of the model components to an ouput neuron(s).</span>

<span class="sd">    - Adding a `Fully-Connected Head` (FC-Head) on top of the deep models.</span>
<span class="sd">      This FC-Head will combine the output form the `deeptabular`, `deeptext` and</span>
<span class="sd">      `deepimage` and will be then connected to the output neuron(s).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    wide: nn.Module, Optional, default = None</span>
<span class="sd">        `Wide` model. This is a linear model where the non-linearities are</span>
<span class="sd">        captured via crossed-columns.</span>
<span class="sd">    deeptabular: BaseWDModelComponent, Optional, default = None</span>
<span class="sd">        Currently this library implements a number of possible architectures</span>
<span class="sd">        for the `deeptabular` component. See the documenation of the</span>
<span class="sd">        package. Note that `deeptabular` can be a list of models. This is</span>
<span class="sd">        useful when using multiple tabular inputs (e.g. for example in the</span>
<span class="sd">        context of a two-tower model for recommendation systems)</span>
<span class="sd">    deeptext: BaseWDModelComponent | List[BaseWDModelComponent], Optional, default = None</span>
<span class="sd">        Currently this library implements a number of possible architectures</span>
<span class="sd">        for the `deeptext` component. See the documenation of the</span>
<span class="sd">        package. Note that `deeptext` can be a list of models. This is useful</span>
<span class="sd">        when using multiple text inputs.</span>
<span class="sd">    deepimage: BaseWDModelComponent | List[BaseWDModelComponent], Optional, default = None</span>
<span class="sd">        Currently this library uses `torchvision` and implements a number of</span>
<span class="sd">        possible architectures for the `deepimage` component. See the</span>
<span class="sd">        documenation of the package. Note that `deepimage` can be a list of</span>
<span class="sd">        models. This is useful when using multiple image inputs.</span>
<span class="sd">    deephead: BaseWDModelComponent, Optional, default = None</span>
<span class="sd">        Alternatively, the user can pass a custom model that will receive the</span>
<span class="sd">        output of the deep component. If `deephead` is not None all the</span>
<span class="sd">        previous fc-head parameters will be ignored</span>
<span class="sd">    head_hidden_dims: List, Optional, default = None</span>
<span class="sd">        List with the sizes of the dense layers in the head e.g: [128, 64]</span>
<span class="sd">    head_activation: str, default = &quot;relu&quot;</span>
<span class="sd">        Activation function for the dense layers in the head. Currently</span>
<span class="sd">        `&#39;tanh&#39;`, `&#39;relu&#39;`, `&#39;leaky_relu&#39;` and `&#39;gelu&#39;` are supported</span>
<span class="sd">    head_dropout: float, Optional, default = None</span>
<span class="sd">        Dropout of the dense layers in the head</span>
<span class="sd">    head_batchnorm: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to include batch normalization in</span>
<span class="sd">        the dense layers that form the `&#39;rnn_mlp&#39;`</span>
<span class="sd">    head_batchnorm_last: bool, default = False</span>
<span class="sd">        Boolean indicating whether or not to apply batch normalization to the</span>
<span class="sd">        last of the dense layers in the head</span>
<span class="sd">    head_linear_first: bool, default = False</span>
<span class="sd">        Boolean indicating whether the order of the operations in the dense</span>
<span class="sd">        layer. If `True: [LIN -&gt; ACT -&gt; BN -&gt; DP]`. If `False: [BN -&gt; DP -&gt;</span>
<span class="sd">        LIN -&gt; ACT]`</span>
<span class="sd">    enforce_positive: bool, default = False</span>
<span class="sd">        Boolean indicating if the output from the final layer must be</span>
<span class="sd">        positive. This is important if you are using loss functions with</span>
<span class="sd">        non-negative input restrictions, e.g. RMSLE, or if you know your</span>
<span class="sd">        predictions are bounded in between 0 and inf</span>
<span class="sd">    enforce_positive_activation: str, default = &quot;softplus&quot;</span>
<span class="sd">        Activation function to enforce that the final layer has a positive</span>
<span class="sd">        output. `&#39;softplus&#39;` or `&#39;relu&#39;` are supported.</span>
<span class="sd">    pred_dim: int, default = 1</span>
<span class="sd">        Size of the final wide and deep output layer containing the</span>
<span class="sd">        predictions. `1` for regression and binary classification or number</span>
<span class="sd">        of classes for multiclass classification.</span>


<span class="sd">    Examples</span>
<span class="sd">    --------</span>

<span class="sd">    &gt;&gt;&gt; from pytorch_widedeep.models import TabResnet, Vision, BasicRNN, Wide, WideDeep</span>
<span class="sd">    &gt;&gt;&gt; embed_input = [(u, i, j) for u, i, j in zip([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;][:4], [4] * 3, [8] * 3)]</span>
<span class="sd">    &gt;&gt;&gt; column_idx = {k: v for v, k in enumerate([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])}</span>
<span class="sd">    &gt;&gt;&gt; wide = Wide(10, 1)</span>
<span class="sd">    &gt;&gt;&gt; deeptabular = TabResnet(blocks_dims=[8, 4], column_idx=column_idx, cat_embed_input=embed_input)</span>
<span class="sd">    &gt;&gt;&gt; deeptext = BasicRNN(vocab_size=10, embed_dim=4, padding_idx=0)</span>
<span class="sd">    &gt;&gt;&gt; deepimage = Vision()</span>
<span class="sd">    &gt;&gt;&gt; model = WideDeep(wide=wide, deeptabular=deeptabular, deeptext=deeptext, deepimage=deepimage)</span>


<span class="sd">    :information_source: **NOTE**: It is possible to use custom components to</span>
<span class="sd">     build Wide &amp; Deep models. Simply, build them and pass them as the</span>
<span class="sd">     corresponding parameters. Note that the custom models MUST return a last</span>
<span class="sd">     layer of activations(i.e. not the final prediction) so that  these</span>
<span class="sd">     activations are collected by `WideDeep` and combined accordingly. In</span>
<span class="sd">     addition, the models MUST also contain an attribute `output_dim` with</span>
<span class="sd">     the size of these last layers of activations. See for example</span>
<span class="sd">     `pytorch_widedeep.models.tab_mlp.TabMlp`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@alias</span><span class="p">(</span>  <span class="c1"># noqa: C901</span>
        <span class="s2">&quot;pred_dim&quot;</span><span class="p">,</span>
        <span class="p">[</span><span class="s2">&quot;num_class&quot;</span><span class="p">,</span> <span class="s2">&quot;pred_size&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">wide</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">deeptabular</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">deeptext</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">deepimage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">deephead</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">enforce_positive</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">enforce_positive_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;softplus&quot;</span><span class="p">,</span>
        <span class="n">pred_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WideDeep</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span>
            <span class="n">wide</span><span class="p">,</span>
            <span class="n">deeptabular</span><span class="p">,</span>
            <span class="n">deeptext</span><span class="p">,</span>
            <span class="n">deepimage</span><span class="p">,</span>
            <span class="n">deephead</span><span class="p">,</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">pred_dim</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># this attribute will be eventually over-written by the Trainer&#39;s</span>
        <span class="c1"># device. Acts here as a &#39;placeholder&#39;.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wd_device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># required as attribute just in case we pass a deephead</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span> <span class="o">=</span> <span class="n">pred_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">enforce_positive</span> <span class="o">=</span> <span class="n">enforce_positive</span>

        <span class="c1"># better to set this attribute already here</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deeptabular</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span> <span class="o">=</span> <span class="n">deeptabular</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;TabNet&quot;</span>

        <span class="c1"># The main 5 components of the wide and deep assemble: wide,</span>
        <span class="c1"># deeptabular, deeptext, deepimage and deephead</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_deephead</span> <span class="o">=</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_deephead</span><span class="p">(</span>
                <span class="n">deeptabular</span><span class="p">,</span>
                <span class="n">deeptext</span><span class="p">,</span>
                <span class="n">deepimage</span><span class="p">,</span>
                <span class="n">head_hidden_dims</span><span class="p">,</span>
                <span class="n">head_activation</span><span class="p">,</span>
                <span class="n">head_dropout</span><span class="p">,</span>
                <span class="n">head_batchnorm</span><span class="p">,</span>
                <span class="n">head_batchnorm_last</span><span class="p">,</span>
                <span class="n">head_linear_first</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">deephead</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">deephead</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># for consistency with other components we default to None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">wide</span> <span class="o">=</span> <span class="n">wide</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptext</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepimage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_model_components</span><span class="p">(</span>
            <span class="n">deeptabular</span><span class="p">,</span> <span class="n">deeptext</span><span class="p">,</span> <span class="n">deepimage</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enforce_positive</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enf_pos</span> <span class="o">=</span> <span class="n">get_activation_fn</span><span class="p">(</span><span class="n">enforce_positive_activation</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">X</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]],</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>

        <span class="n">wide_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_wide</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_deephead</span><span class="p">:</span>
            <span class="n">deep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_deephead</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">wide_out</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">deep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_deep</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">wide_out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enforce_positive</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">enf_pos</span><span class="p">(</span><span class="n">deep</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">deep</span>

    <span class="k">def</span> <span class="nf">_build_deephead</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">deeptabular</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]],</span>
        <span class="n">deeptext</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]],</span>
        <span class="n">deepimage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]],</span>
        <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="n">deep_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">deeptabular</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deeptabular</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">deeptabular</span><span class="p">:</span>
                    <span class="n">deep_dim</span> <span class="o">+=</span> <span class="n">dt</span><span class="o">.</span><span class="n">output_dim</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">deep_dim</span> <span class="o">+=</span> <span class="n">deeptabular</span><span class="o">.</span><span class="n">output_dim</span>
        <span class="k">if</span> <span class="n">deeptext</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deeptext</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">deeptext</span><span class="p">:</span>
                    <span class="n">deep_dim</span> <span class="o">+=</span> <span class="n">dt</span><span class="o">.</span><span class="n">output_dim</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">deep_dim</span> <span class="o">+=</span> <span class="n">deeptext</span><span class="o">.</span><span class="n">output_dim</span>
        <span class="k">if</span> <span class="n">deepimage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deepimage</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">deepimage</span><span class="p">:</span>
                    <span class="n">deep_dim</span> <span class="o">+=</span> <span class="n">di</span><span class="o">.</span><span class="n">output_dim</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">deep_dim</span> <span class="o">+=</span> <span class="n">deepimage</span><span class="o">.</span><span class="n">output_dim</span>

        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">deep_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">head_hidden_dims</span>
        <span class="n">deephead</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">MLP</span><span class="p">(</span>
                <span class="n">head_hidden_dims</span><span class="p">,</span>
                <span class="n">head_activation</span><span class="p">,</span>
                <span class="n">head_dropout</span><span class="p">,</span>
                <span class="n">head_batchnorm</span><span class="p">,</span>
                <span class="n">head_batchnorm_last</span><span class="p">,</span>
                <span class="n">head_linear_first</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">head_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">deephead</span>

    <span class="k">def</span> <span class="nf">_set_model_components</span><span class="p">(</span>  <span class="c1"># noqa: C901</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">deeptabular</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]],</span>
        <span class="n">deeptext</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]],</span>
        <span class="n">deepimage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
        <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span> <span class="n">WDModel</span><span class="p">]],</span>
        <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span> <span class="n">WDModel</span><span class="p">]],</span>
        <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span> <span class="n">WDModel</span><span class="p">]],</span>
    <span class="p">]:</span>
        <span class="k">if</span> <span class="n">deeptabular</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deeptabular_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_model_component</span><span class="p">(</span><span class="n">deeptabular</span><span class="p">,</span> <span class="n">is_deeptabular</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">deeptabular_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">deeptext</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deeptext_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_model_component</span><span class="p">(</span><span class="n">deeptext</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">deeptext_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">deepimage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deepimage_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_model_component</span><span class="p">(</span><span class="n">deepimage</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">deepimage_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">deeptabular_</span><span class="p">,</span> <span class="n">deeptext_</span><span class="p">,</span> <span class="n">deepimage_</span>

    <span class="k">def</span> <span class="nf">_forward_wide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wide</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wide</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s2">&quot;wide&quot;</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">first_model_mode</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">first_model_mode</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">first_model_mode</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">first_model_mode</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># type: ignore[union-attr]</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wd_device</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">_forward_deep</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]],</span> <span class="n">wide_out</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span><span class="p">:</span>
                <span class="n">tab_out</span><span class="p">,</span> <span class="n">M_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s2">&quot;deeptabular&quot;</span><span class="p">])</span>
                <span class="n">wide_out</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">tab_out</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">wide_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_component</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span><span class="p">,</span> <span class="s2">&quot;deeptabular&quot;</span><span class="p">,</span> <span class="n">wide_out</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptext</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">wide_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_component</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptext</span><span class="p">,</span> <span class="s2">&quot;deeptext&quot;</span><span class="p">,</span> <span class="n">wide_out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepimage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">wide_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_component</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepimage</span><span class="p">,</span> <span class="s2">&quot;deepimage&quot;</span><span class="p">,</span> <span class="n">wide_out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span><span class="p">:</span>
            <span class="n">res</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span><span class="n">wide_out</span><span class="p">,</span> <span class="n">M_loss</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">wide_out</span>

        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">_forward_deephead</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]],</span> <span class="n">wide_out</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
        <span class="n">deepside</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wd_device</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span><span class="p">:</span>
                <span class="n">deepside</span><span class="p">,</span> <span class="n">M_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s2">&quot;deeptabular&quot;</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">deepside</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_component_with_head</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span><span class="p">,</span> <span class="s2">&quot;deeptabular&quot;</span><span class="p">,</span> <span class="n">deepside</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptext</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deepside</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_component_with_head</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptext</span><span class="p">,</span> <span class="s2">&quot;deeptext&quot;</span><span class="p">,</span> <span class="n">deepside</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepimage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deepside</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_component_with_head</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepimage</span><span class="p">,</span> <span class="s2">&quot;deepimage&quot;</span><span class="p">,</span> <span class="n">deepside</span>
            <span class="p">)</span>

        <span class="c1"># assertion to avoid type issues</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">deepside_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span><span class="p">(</span><span class="n">deepside</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span><span class="p">:</span>
            <span class="n">res</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">wide_out</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">deepside_out</span><span class="p">),</span>
                <span class="n">M_loss</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">wide_out</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">deepside_out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">_forward_component</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">X</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]],</span>
        <span class="n">component</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span> <span class="n">WDModel</span><span class="p">],</span>
        <span class="n">component_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;deeptabular&quot;</span><span class="p">,</span> <span class="s2">&quot;deeptext&quot;</span><span class="p">,</span> <span class="s2">&quot;deepimage&quot;</span><span class="p">],</span>
        <span class="n">wide_out</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">):</span>
            <span class="n">component_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>  <span class="c1"># type: ignore[call-overload]</span>
                <span class="o">*</span><span class="p">[</span><span class="n">cp</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">component_type</span><span class="p">][</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">component</span><span class="p">)]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">component_out</span> <span class="o">=</span> <span class="n">component</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">component_type</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">wide_out</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">component_out</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_forward_component_with_head</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">X</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]],</span>
        <span class="n">component</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span> <span class="n">WDModel</span><span class="p">],</span>
        <span class="n">component_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;deeptabular&quot;</span><span class="p">,</span> <span class="s2">&quot;deeptext&quot;</span><span class="p">,</span> <span class="s2">&quot;deepimage&quot;</span><span class="p">],</span>
        <span class="n">deepside</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">):</span>
            <span class="n">component_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>  <span class="c1"># type: ignore[call-overload]</span>
                <span class="p">[</span><span class="n">cp</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">component_type</span><span class="p">][</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">component</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">component_out</span> <span class="o">=</span> <span class="n">component</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">component_type</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">deepside</span><span class="p">,</span> <span class="n">component_out</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># type: ignore[call-overload]</span>

    <span class="k">def</span> <span class="nf">_set_model_component</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">component</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]],</span>
        <span class="n">is_deeptabular</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span> <span class="n">WDModel</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">component_</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span> <span class="n">WDModel</span><span class="p">]]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">cp</span> <span class="ow">in</span> <span class="n">component</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_deephead</span> <span class="ow">or</span> <span class="n">cp</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">component_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cp</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">component_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">cp</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span><span class="p">))</span>
                    <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_deephead</span> <span class="ow">or</span> <span class="n">component</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">component_</span> <span class="o">=</span> <span class="n">component</span>
        <span class="k">elif</span> <span class="n">is_deeptabular</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span><span class="p">:</span>
            <span class="n">component_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">component</span><span class="p">,</span> <span class="n">TabNetPredLayer</span><span class="p">(</span><span class="n">component</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">component_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">component</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">component</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">component_</span>

    <span class="nd">@staticmethod</span>  <span class="c1"># noqa: C901</span>
    <span class="k">def</span> <span class="nf">_check_inputs</span><span class="p">(</span>  <span class="c1"># noqa: C901</span>
        <span class="n">wide</span><span class="p">,</span>
        <span class="n">deeptabular</span><span class="p">,</span>
        <span class="n">deeptext</span><span class="p">,</span>
        <span class="n">deepimage</span><span class="p">,</span>
        <span class="n">deephead</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">,</span>
        <span class="n">pred_dim</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">wide</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">wide</span><span class="o">.</span><span class="n">wide_linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">pred_dim</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;the &#39;pred_dim&#39; of the wide component (</span><span class="si">{}</span><span class="s2">) must be equal to the &#39;pred_dim&#39; &quot;</span>
                <span class="s2">&quot;of the deep component and the overall model itself (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">wide</span><span class="o">.</span><span class="n">wide_linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">pred_dim</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">deeptabular</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;deeptabular model must have an &#39;output_dim&#39; attribute or property.&quot;</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deeptabular</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">all_have_output_dim</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="nb">hasattr</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">deeptabular</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">all_have_output_dim</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">deeptabular</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>
                <span class="c1"># the following assertion is thought for those cases where we</span>
                <span class="c1"># use fusion with &#39;dot product&#39; so that the output_dim will</span>
                <span class="c1"># be 1 and the pred_dim is not 1</span>
                <span class="k">if</span> <span class="n">deeptabular</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">pred_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;If &#39;output_dim&#39; is 1, &#39;pred_dim&#39; must be 1&quot;</span>

        <span class="k">if</span> <span class="n">deeptabular</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">is_tabnet</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deeptabular</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">is_any_tabnet</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
                    <span class="n">dt</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;TabNet&quot;</span> <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">deeptabular</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">is_any_tabnet</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Currently TabNet is not supported as a component of a multiple &quot;</span>
                        <span class="s2">&quot;tabular component model.&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">is_tabnet</span> <span class="o">=</span> <span class="n">deeptabular</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;TabNet&quot;</span>
            <span class="n">has_wide_text_or_image</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">wide</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">deeptext</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">deepimage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">is_tabnet</span> <span class="ow">and</span> <span class="n">has_wide_text_or_image</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;&#39;WideDeep&#39; is a model comprised by multiple components and the &#39;deeptabular&#39;&quot;</span>
                    <span class="s2">&quot; component is &#39;TabNet&#39;. We recommend using &#39;TabNet&#39; in isolation.&quot;</span>
                    <span class="s2">&quot; The reasons are: i)&#39;TabNet&#39; uses sparse regularization which partially losses&quot;</span>
                    <span class="s2">&quot; its purpose when used in combination with other components.&quot;</span>
                    <span class="s2">&quot; If you still want to use a multiple component model with &#39;TabNet&#39;,&quot;</span>
                    <span class="s2">&quot; consider setting &#39;lambda_sparse&#39; to 0 during training. ii) The feature&quot;</span>
                    <span class="s2">&quot; importances will be computed only for TabNet but the model will comprise multiple&quot;</span>
                    <span class="s2">&quot; components. Therefore, such importances will partially lose their &#39;meaning&#39;.&quot;</span><span class="p">,</span>
                    <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">deeptext</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">err_msg</span> <span class="o">=</span> <span class="s2">&quot;deeptext model must have an &#39;output_dim&#39; attribute or property.&quot;</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deeptext</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">all_have_output_dim</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">deeptext</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">all_have_output_dim</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">deeptext</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">deeptext</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">pred_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;If &#39;output_dim&#39; is 1, &#39;pred_dim&#39; must be 1&quot;</span>

        <span class="k">if</span> <span class="n">deepimage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">err_msg</span> <span class="o">=</span> <span class="s2">&quot;deepimage model must have an &#39;output_dim&#39; attribute or property.&quot;</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deepimage</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">all_have_output_dim</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">di</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">deepimage</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">all_have_output_dim</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">deepimage</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">deepimage</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">pred_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;If &#39;output_dim&#39; is 1, &#39;pred_dim&#39; must be 1&quot;</span>

        <span class="k">if</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;both &#39;deephead&#39; and &#39;head_hidden_dims&#39; are not None. Use one of the other, but not both&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">deeptabular</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">deeptext</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">deepimage</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;if &#39;head_hidden_dims&#39; is not None, at least one deep component must be used&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">deephead</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                    <span class="s2">&quot;As any other custom model passed to &#39;WideDeep&#39;, &#39;deephead&#39; must have an &quot;</span>
                    <span class="s2">&quot;&#39;output_dim&#39; attribute or property. &quot;</span>
                <span class="p">)</span>
            <span class="n">deephead_inp_feat</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">deephead</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">output_dim</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">deeptabular</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deeptabular</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">deeptabular</span><span class="p">:</span>
                        <span class="n">output_dim</span> <span class="o">+=</span> <span class="n">dt</span><span class="o">.</span><span class="n">output_dim</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_dim</span> <span class="o">+=</span> <span class="n">deeptabular</span><span class="o">.</span><span class="n">output_dim</span>
            <span class="k">if</span> <span class="n">deeptext</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deeptext</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">dt</span> <span class="ow">in</span> <span class="n">deeptext</span><span class="p">:</span>
                        <span class="n">output_dim</span> <span class="o">+=</span> <span class="n">dt</span><span class="o">.</span><span class="n">output_dim</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_dim</span> <span class="o">+=</span> <span class="n">deeptext</span><span class="o">.</span><span class="n">output_dim</span>
            <span class="k">if</span> <span class="n">deepimage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deepimage</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">deepimage</span><span class="p">:</span>
                        <span class="n">output_dim</span> <span class="o">+=</span> <span class="n">di</span><span class="o">.</span><span class="n">output_dim</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_dim</span> <span class="o">+=</span> <span class="n">deepimage</span><span class="o">.</span><span class="n">output_dim</span>
            <span class="k">if</span> <span class="n">deephead_inp_feat</span> <span class="o">!=</span> <span class="n">output_dim</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;A custom &#39;deephead&#39; is used and it seems that the input features &quot;</span>
                    <span class="s2">&quot;do not match the output of the deep components&quot;</span><span class="p">,</span>
                    <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>








  <aside class="md-source-file">
    
    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 5.5A3.5 3.5 0 0 1 15.5 9a3.5 3.5 0 0 1-3.5 3.5A3.5 3.5 0 0 1 8.5 9 3.5 3.5 0 0 1 12 5.5M5 8c.56 0 1.08.15 1.53.42-.15 1.43.27 2.85 1.13 3.96C7.16 13.34 6.16 14 5 14a3 3 0 0 1-3-3 3 3 0 0 1 3-3m14 0a3 3 0 0 1 3 3 3 3 0 0 1-3 3c-1.16 0-2.16-.66-2.66-1.62a5.54 5.54 0 0 0 1.13-3.96c.45-.27.97-.42 1.53-.42M5.5 18.25c0-2.07 2.91-3.75 6.5-3.75s6.5 1.68 6.5 3.75V20h-13zM0 20v-1.5c0-1.39 1.89-2.56 4.45-2.9-.59.68-.95 1.62-.95 2.65V20zm24 0h-3.5v-1.75c0-1.03-.36-1.97-.95-2.65 2.56.34 4.45 1.51 4.45 2.9z"/></svg>
      
    </span>
    <nav>
      
        <a href="mailto:jrzaurin@gmail.com">Javier</a>, 
        <a href="mailto:javierrodriguezzaurin@javiers-macbook-pro.local">Javier Rodriguez Zaurin</a>
    </nav>
  </span>

    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Javier Zaurin and Pavol Mulinka
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://jrzaurin.medium.com/" target="_blank" rel="noopener" title="jrzaurin.medium.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262m288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.expand", "toc.integrate"], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../stylesheets/extra.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>