
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch-widedeep.readthedocs.io/pytorch-widedeep/model_components.html">
      
      
        <link rel="prev" href="load_from_folder.html">
      
      
        <link rel="next" href="the_rec_module.html">
      
      
      <link rel="icon" href="../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.33">
    
    
      
        <title>Model Components - pytorch_widedeep</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-models-module" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="pytorch_widedeep" class="md-header__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            pytorch_widedeep
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Model Components
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../installation.html" class="md-tabs__link">
        
  
    
  
  Installation

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../quick_start.html" class="md-tabs__link">
        
  
    
  
  Quick Start

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="utils/index.html" class="md-tabs__link">
          
  
    
  
  Pytorch-widedeep

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../examples/01_preprocessors_and_utils.html" class="md-tabs__link">
          
  
    
  
  Examples

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../contributing.html" class="md-tabs__link">
        
  
    
  
  Contributing

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="pytorch_widedeep" class="md-nav__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    pytorch_widedeep
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quick_start.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Pytorch-widedeep
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Pytorch-widedeep
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="utils/index.html" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/deeptabular_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deeptabular utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/fastai_transforms.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fastai transforms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/image_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/text_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="preprocessing.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Preprocessing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="load_from_folder.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Load From Folder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Model Components
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="model_components.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Model Components
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.linear.wide.Wide" class="md-nav__link">
    <span class="md-ellipsis">
      Wide
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="md-nav__link">
    <span class="md-ellipsis">
      TabMlp
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      TabMlpDecoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="md-nav__link">
    <span class="md-ellipsis">
      TabResnet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      TabResnetDecoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="md-nav__link">
    <span class="md-ellipsis">
      TabNet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      TabNetDecoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="md-nav__link">
    <span class="md-ellipsis">
      ContextAttentionMLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="md-nav__link">
    <span class="md-ellipsis">
      SelfAttentionMLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      TabTransformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="md-nav__link">
    <span class="md-ellipsis">
      SAINT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      FTTransformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="md-nav__link">
    <span class="md-ellipsis">
      TabPerceiver
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="md-nav__link">
    <span class="md-ellipsis">
      TabFastFormer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" class="md-nav__link">
    <span class="md-ellipsis">
      BasicRNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN" class="md-nav__link">
    <span class="md-ellipsis">
      AttentiveRNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN" class="md-nav__link">
    <span class="md-ellipsis">
      StackedAttentiveRNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel" class="md-nav__link">
    <span class="md-ellipsis">
      HFModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.image.vision.Vision" class="md-nav__link">
    <span class="md-ellipsis">
      Vision
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.model_fusion.ModelFuser" class="md-nav__link">
    <span class="md-ellipsis">
      ModelFuser
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.wide_deep.WideDeep" class="md-nav__link">
    <span class="md-ellipsis">
      WideDeep
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="the_rec_module.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Rec Module
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bayesian models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="losses.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Losses
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Metrics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="dataloaders.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataloaders
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="callbacks.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="trainer.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_trainer.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bayesian Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="self_supervised_pretraining.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Self Supervised Pretraining
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="tab2vec.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tab2Vec
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/01_preprocessors_and_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01_preprocessors_and_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/02_model_components.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    02_model_components
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/03_binary_classification_with_defaults.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    03_binary_classification_with_defaults
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/04_regression_with_images_and_text.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    04_regression_with_images_and_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/05_save_and_load_model_and_artifacts.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    05_save_and_load_model_and_artifacts
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/06_finetune_and_warmup.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06_finetune_and_warmup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/07_custom_components.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    07_custom_components
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/08_custom_dataLoader_imbalanced_dataset.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    08_custom_dataLoader_imbalanced_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/09_extracting_embeddings.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    09_extracting_embeddings
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/10_3rd_party_integration-RayTune_WnB.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10_3rd_party_integration-RayTune_WnB
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/11_auc_multiclass.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11_auc_multiclass
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/12_ZILNLoss_origkeras_vs_pytorch_widedeep.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12_ZILNLoss_origkeras_vs_pytorch_widedeep
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/13_model_uncertainty_prediction.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13_model_uncertainty_prediction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/14_bayesian_models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14_bayesian_models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/16_Self_Supervised_Pretraning_pt1.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15_Self-Supervised Pre-Training pt 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/16_Self_Supervised_Pretraning_pt2.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15_Self-Supervised Pre-Training pt 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/17_Usign_a_custom_hugging_face_model.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16_Usign-a-custom-hugging-face-model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/18_feature_importance_via_attention_weights.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17_feature_importance_via_attention_weights
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/19_wide_and_deep_for_recsys_pt1.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18_wide_and_deep_for_recsys_pt1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/19_wide_and_deep_for_recsys_pt2.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18_wide_and_deep_for_recsys_pt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/20_load_from_folder_functionality.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    19_load_from_folder_functionality
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/21_Using_huggingface_within_widedeep.ipynb" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    20-Using-huggingface-within-widedeep
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="the-models-module">The <code>models</code> module<a class="headerlink" href="#the-models-module" title="Permanent link">&para;</a></h1>
<p>This module contains the models that can be used as the four main components
that will comprise a Wide and Deep model (<code>wide</code>, <code>deeptabular</code>,
<code>deeptext</code>, <code>deepimage</code>), as well as the <code>WideDeep</code> "constructor"
class. Note that each of the four components can be used independently. It
also contains all the documentation for the models that can be used for
self-supervised pre-training with tabular data.</p>


<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.linear.wide.Wide" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Wide</span>


<a href="#pytorch_widedeep.models.tabular.linear.wide.Wide" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">Wide</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Defines a <code>Wide</code> (linear) model where the non-linearities are
captured via the so-called crossed-columns. This can be used as the
<code>wide</code> component of a Wide &amp; Deep model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>input_dim</code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>size of the Linear layer (implemented via an Embedding layer).
<code>input_dim</code> is the summation of all the individual values for all the
features that go through the wide model. For example, if the wide
model receives 2 features with 5 individual values each, <code>input_dim =
10</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>pred_dim</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>size of the ouput tensor containing the predictions. Note that unlike
all the other models, the wide model is connected directly to the
output neuron(s) when used to build a Wide and Deep model. Therefore,
it requires the <code>pred_dim</code> parameter.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.linear.wide.Wide.wide_linear">wide_linear</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>the linear layer that comprises the wide branch of the model</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Wide</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">wide</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/linear/wide.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;pred_dim&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;pred_size&quot;</span><span class="p">,</span> <span class="s2">&quot;num_class&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">pred_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Wide</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span> <span class="o">=</span> <span class="n">pred_dim</span>

    <span class="c1"># Embeddings: val + 1 because 0 is reserved for padding/unseen cateogories.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pred_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># (Sum(Embedding) + bias) is equivalent to (OneHotVector + Linear)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">pred_dim</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.models.tabular.linear.wide.Wide.forward" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#pytorch_widedeep.models.tabular.linear.wide.Wide.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Forward pass. Simply connecting the Embedding layer with the ouput
neuron(s)</p>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/models/tabular/linear/wide.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass. Simply connecting the Embedding layer with the ouput</span>
<span class="sd">    neuron(s)&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabMlp</span>


<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabMlp</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>


      <p>Defines a <code>TabMlp</code> model that can be used as the <code>deeptabular</code>
component of a Wide &amp; Deep model or independently by itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features, embedded or not. These are then
passed through a series of dense layers (i.e. a MLP).</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the continuous columns will be embedded using
one of the available methods: <em>'standard'</em>, <em>'periodic'</em>
or <em>'piecewise'</em>. If <code>None</code>, it will default to 'False'.<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This parameter is deprecated and it
 will be removed in future releases. Please, use the
 <code>embed_continuous_method</code> parameter instead.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dim</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Size of the continuous embeddings. If the continuous columns are
embedded, <code>cont_embed_dim</code> must be passed.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                  <code>[200, 100]</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the mlp.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>float or List of floats with the dropout between the dense layers.
e.g: <em>[0.5,0.5]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm_last</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_linear_first</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>mlp model that will receive the concatenation of the embeddings and
the continuous columns</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabMlp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabMlp</span><span class="p">(</span><span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/tab_mlp.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabMlp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Mlp</span>
    <span class="n">mlp_input_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_out_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_out_dim</span>
    <span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">mlp_input_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
        <span class="n">mlp_hidden_dims</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabMlpDecoder</span>


<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabMlpDecoder</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Companion decoder model for the <code>TabMlp</code> model (which can be considered
an encoder itself).</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). The <code>TabMlpDecoder</code> will receive the output from the MLP
and '<em>reconstruct</em>' the embeddings.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>embed_dim</code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Size of the embeddings tensor that needs to be reconstructed.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                  <code>[100, 200]</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the mlp.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>float or List of floats with the dropout between the dense layers.
e.g: <em>[0.5,0.5]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm_last</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_linear_first</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder.decoder">decoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>mlp model that will receive the output of the encoder</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabMlpDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabMlpDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/tab_mlp.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabMlpDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
        <span class="n">mlp_hidden_dims</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">],</span>
        <span class="n">mlp_activation</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabResnet</span>


<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabResnet</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>


      <p>Defines a <code>TabResnet</code> model that can be used as the <code>deeptabular</code>
component of a Wide &amp; Deep model or independently by itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features, embedded or not. These are then
passed through a series of Resnet blocks. See
<code>pytorch_widedeep.models.tab_resnet._layers</code> for details on the
structure of each block.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the continuous columns will be embedded using
one of the available methods: <em>'standard'</em>, <em>'periodic'</em>
or <em>'piecewise'</em>. If <code>None</code>, it will default to 'False'.<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This parameter is deprecated and it
 will be removed in future releases. Please, use the
 <code>embed_continuous_method</code> parameter instead.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dim</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Size of the continuous embeddings. If the continuous columns are
embedded, <code>cont_embed_dim</code> must be passed.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>blocks_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                  <code>[200, 100, 100]</code>
)
          –
          <div class="doc-md-description">
            <p>List of integers that define the input and output units of each block.
For example: <em>[200, 100, 100]</em> will generate 2 blocks. The first will
receive a tensor of size 200 and output a tensor of size 100, and the
second will receive a tensor of size 100 and output a tensor of size
100. See <code>pytorch_widedeep.models.tab_resnet._layers</code> for
details on the structure of each block.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>blocks_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Block's internal dropout.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>simplify_blocks</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the simplest possible residual blocks (<code>X -&gt; [
[LIN, BN, ACT]  + X ]</code>) will be used instead of a standard one
(<code>X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]</code>).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If <code>None</code> the  output of the Resnet Blocks will be
connected directly to the output neuron(s).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm_last</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_linear_first</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>deep dense Resnet model that will receive the concatenation of the
embeddings and the continuous columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.mlp">mlp</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>if <code>mlp_hidden_dims</code> is <code>True</code>, this attribute will be an mlp
model that will receive the results of the concatenation of the
embeddings and the continuous columns -- if present --.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_deep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabResnet</span><span class="p">(</span><span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_deep</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/resnet/tab_resnet.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabResnet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;blocks&#39; must contain at least two elements, e.g. [256, 128]&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span> <span class="o">=</span> <span class="n">blocks_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dropout</span> <span class="o">=</span> <span class="n">blocks_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span> <span class="o">=</span> <span class="n">simplify_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>

    <span class="c1"># Resnet</span>
    <span class="n">dense_resnet_input_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_out_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_out_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
        <span class="n">dense_resnet_input_dim</span><span class="p">,</span> <span class="n">blocks_dims</span><span class="p">,</span> <span class="n">blocks_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span>
    <span class="p">)</span>

    <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
    <span class="c1"># therefore all related params are optional</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabResnetDecoder</span>


<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabResnetDecoder</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Companion decoder model for the <code>TabResnet</code> model (which can be
considered an encoder itself)</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). This class will receive the output from the ResNet blocks or the
MLP(if present) and '<em>reconstruct</em>' the embeddings.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>embed_dim</code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Size of the embeddings tensor to be reconstructed.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>blocks_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                  <code>[100, 100, 200]</code>
)
          –
          <div class="doc-md-description">
            <p>List of integers that define the input and output units of each block.
For example: <em>[200, 100, 100]</em> will generate 2 blocks. The first will
receive a tensor of size 200 and output a tensor of size 100, and the
second will receive a tensor of size 100 and output a tensor of size
100. See <code>pytorch_widedeep.models.tab_resnet._layers</code> for
details on the structure of each block.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>blocks_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Block's internal dropout.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>simplify_blocks</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the simplest possible residual blocks (<code>X -&gt; [
[LIN, BN, ACT]  + X ]</code>) will be used instead of a standard one
(<code>X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]</code>).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If <code>None</code> the  output of the Resnet Blocks will be
connected directly to the output neuron(s).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm_last</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_linear_first</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder.decoder">decoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>deep dense Resnet model that will receive the output of the encoder IF
<code>mlp_hidden_dims</code> is None</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder.mlp">mlp</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>if <code>mlp_hidden_dims</code> is not None, the overall decoder will consist
in an MLP that will receive the output of the encoder followed by the
deep dense Resnet.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnetDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabResnetDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/resnet/tab_resnet.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabResnetDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;blocks&#39; must contain at least two elements, e.g. [256, 128]&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span> <span class="o">=</span> <span class="n">blocks_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dropout</span> <span class="o">=</span> <span class="n">blocks_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span> <span class="o">=</span> <span class="n">simplify_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">blocks_dims</span><span class="p">,</span>
            <span class="n">blocks_dropout</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
            <span class="n">blocks_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">blocks_dims</span><span class="p">,</span> <span class="n">blocks_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabNet</span>


<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabNet</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">attn_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">1.3</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">,</span>
    <span class="n">mask_type</span><span class="o">=</span><span class="s2">&quot;sparsemax&quot;</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>


      <p>Defines a <a href="https://arxiv.org/abs/1908.07442">TabNet model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>The implementation in this library is fully based on that
<a href="https://github.com/dreamquark-ai/tabnet">here</a> by the dreamquark-ai team,
simply adapted so that it can work within the <code>WideDeep</code> frame.
Therefore, <strong>ALL CREDIT TO THE DREAMQUARK-AI TEAM</strong>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the continuous columns will be embedded using
one of the available methods: <em>'standard'</em>, <em>'periodic'</em>
or <em>'piecewise'</em>. If <code>None</code>, it will default to 'False'.<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This parameter is deprecated and it
 will be removed in future releases. Please, use the
 <code>embed_continuous_method</code> parameter instead.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dim</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Size of the continuous embeddings. If the continuous columns are
embedded, <code>cont_embed_dim</code> must be passed.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_steps</code></b>
              (<code>int</code>, default:
                  <code>3</code>
)
          –
          <div class="doc-md-description">
            <p>number of decision steps. For a better understanding of the function
of <code>n_steps</code> and the upcoming parameters, please see the
<a href="https://arxiv.org/abs/1908.07442">paper</a>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>step_dim</code></b>
              (<code>int</code>, default:
                  <code>8</code>
)
          –
          <div class="doc-md-description">
            <p>Step's output dimension. This is the output dimension that
<code>WideDeep</code> will collect and connect to the output neuron(s).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dim</code></b>
              (<code>int</code>, default:
                  <code>8</code>
)
          –
          <div class="doc-md-description">
            <p>Attention dimension</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>dropout</code></b>
              (<code>float</code>, default:
                  <code>0.0</code>
)
          –
          <div class="doc-md-description">
            <p>GLU block's internal dropout</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_glu_step_dependent</code></b>
              (<code>int</code>, default:
                  <code>2</code>
)
          –
          <div class="doc-md-description">
            <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that are step dependent</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_glu_shared</code></b>
              (<code>int</code>, default:
                  <code>2</code>
)
          –
          <div class="doc-md-description">
            <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that will be shared
across decision steps</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ghost_bn</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if <a href="https://arxiv.org/abs/1705.08741">Ghost Batch Normalization</a>
will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>virtual_batch_size</code></b>
              (<code>int</code>, default:
                  <code>128</code>
)
          –
          <div class="doc-md-description">
            <p>Batch size when using Ghost Batch Normalization</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>momentum</code></b>
              (<code>float</code>, default:
                  <code>0.02</code>
)
          –
          <div class="doc-md-description">
            <p>Ghost Batch Normalization's momentum. The dreamquark-ai advises for
very low values. However high values are used in the original
publication. During our tests higher values lead to better results</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>gamma</code></b>
              (<code>float</code>, default:
                  <code>1.3</code>
)
          –
          <div class="doc-md-description">
            <p>Relaxation parameter in the paper. When gamma = 1, a feature is
enforced to be used only at one decision step. As gamma increases,
more flexibility is provided to use a feature at multiple decision
steps</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>epsilon</code></b>
              (<code>float</code>, default:
                  <code>1e-15</code>
)
          –
          <div class="doc-md-description">
            <p>Float to avoid log(0). Always keep low</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mask_type</code></b>
              (<code>str</code>, default:
                  <code>&#39;sparsemax&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Mask function to use. Either <em>'sparsemax'</em> or <em>'entmax'</em></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>the TabNet encoder. For details see the <a href="https://arxiv.org/abs/1908.07442">original publication</a>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabNet</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/tabnet/tab_net.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">attn_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.3</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-15</span><span class="p">,</span>
    <span class="n">mask_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sparsemax&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span> <span class="o">=</span> <span class="n">step_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dim</span> <span class="o">=</span> <span class="n">attn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_step_dependent</span> <span class="o">=</span> <span class="n">n_glu_step_dependent</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_shared</span> <span class="o">=</span> <span class="n">n_glu_shared</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ghost_bn</span> <span class="o">=</span> <span class="n">ghost_bn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">virtual_batch_size</span> <span class="o">=</span> <span class="n">virtual_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_type</span> <span class="o">=</span> <span class="n">mask_type</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_out_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_out_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_out_dim</span>

    <span class="c1"># TabNet</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TabNetEncoder</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_out_dim</span><span class="p">,</span>
        <span class="n">n_steps</span><span class="p">,</span>
        <span class="n">step_dim</span><span class="p">,</span>
        <span class="n">attn_dim</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">,</span>
        <span class="n">n_glu_step_dependent</span><span class="p">,</span>
        <span class="n">n_glu_shared</span><span class="p">,</span>
        <span class="n">ghost_bn</span><span class="p">,</span>
        <span class="n">virtual_batch_size</span><span class="p">,</span>
        <span class="n">momentum</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">,</span>
        <span class="n">mask_type</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabNetDecoder</span>


<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabNetDecoder</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Companion decoder model for the <code>TabNet</code> model (which can be
considered an encoder itself)</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). This class will receive the output from the <code>TabNet</code> encoder
(i.e. the output from the so called 'steps') and '<em>reconstruct</em>' the
embeddings.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>embed_dim</code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Size of the embeddings tensor to be reconstructed.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_steps</code></b>
              (<code>int</code>, default:
                  <code>3</code>
)
          –
          <div class="doc-md-description">
            <p>number of decision steps. For a better understanding of the function
of <code>n_steps</code> and the upcoming parameters, please see the
<a href="https://arxiv.org/abs/1908.07442">paper</a>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>step_dim</code></b>
              (<code>int</code>, default:
                  <code>8</code>
)
          –
          <div class="doc-md-description">
            <p>Step's output dimension. This is the output dimension that
<code>WideDeep</code> will collect and connect to the output neuron(s).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>dropout</code></b>
              (<code>float</code>, default:
                  <code>0.0</code>
)
          –
          <div class="doc-md-description">
            <p>GLU block's internal dropout</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_glu_step_dependent</code></b>
              (<code>int</code>, default:
                  <code>2</code>
)
          –
          <div class="doc-md-description">
            <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that are step dependent</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_glu_shared</code></b>
              (<code>int</code>, default:
                  <code>2</code>
)
          –
          <div class="doc-md-description">
            <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that will be shared
across decision steps</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ghost_bn</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if <a href="https://arxiv.org/abs/1705.08741">Ghost Batch Normalization</a>
will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>virtual_batch_size</code></b>
              (<code>int</code>, default:
                  <code>128</code>
)
          –
          <div class="doc-md-description">
            <p>Batch size when using Ghost Batch Normalization</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>momentum</code></b>
              (<code>float</code>, default:
                  <code>0.02</code>
)
          –
          <div class="doc-md-description">
            <p>Ghost Batch Normalization's momentum. The dreamquark-ai advises for
very low values. However high values are used in the original
publication. During our tests higher values lead to better results</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder.decoder">decoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>decoder that will receive the output from the encoder's steps and will
reconstruct the embeddings</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabNetDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabNetDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/tabnet/tab_net.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabNetDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span> <span class="o">=</span> <span class="n">step_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_step_dependent</span> <span class="o">=</span> <span class="n">n_glu_step_dependent</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_shared</span> <span class="o">=</span> <span class="n">n_glu_shared</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ghost_bn</span> <span class="o">=</span> <span class="n">ghost_bn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">virtual_batch_size</span> <span class="o">=</span> <span class="n">virtual_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>

    <span class="n">shared_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_glu_shared</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">transformer</span> <span class="o">=</span> <span class="n">FeatTransformer</span><span class="p">(</span>
            <span class="n">step_dim</span><span class="p">,</span>
            <span class="n">step_dim</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
            <span class="n">shared_layers</span><span class="p">,</span>
            <span class="n">n_glu_step_dependent</span><span class="p">,</span>
            <span class="n">ghost_bn</span><span class="p">,</span>
            <span class="n">virtual_batch_size</span><span class="p">,</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">initialize_non_glu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span><span class="p">,</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ContextAttentionMLP</span>


<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">ContextAttentionMLP</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="o">=</span><span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


      <p>Defines a <code>ContextAttentionMLP</code> model that can be used as the
<code>deeptabular</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features that are also embedded. These
are then passed through a series of attention blocks. Each attention
block is comprised by a <code>ContextAttentionEncoder</code>. Such encoder is in
part inspired by the attention mechanism described in
<a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf">Hierarchical Attention Networks for Document
Classification</a>.
See <code>pytorch_widedeep.models.tabular.mlp._attention_layers</code> for details.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>add_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>frac_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>&#39;standard&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>input_dim</code></b>
              (<code>int</code>, default:
                  <code>32</code>
)
          –
          <div class="doc-md-description">
            <p>The so-called <em>dimension of the model</em>. Is the number of embeddings
used to encode the categorical and/or continuous columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.2</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for each attention block</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>with_addnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if residual connections will be used in the
attention blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;leaky_relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>String indicating the activation function to be applied to the dense
layer in each attention encoder. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em>
and <em>'gelu'</em> are supported.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_blocks</code></b>
              (<code>int</code>, default:
                  <code>3</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention blocks</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Sequence of attention encoders.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">ContextAttentionMLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ContextAttentionMLP</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/context_attention_mlp.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ContextAttentionMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_activation</span> <span class="o">=</span> <span class="n">attn_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Attention Blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;attention_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">ContextAttentionEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">with_addnorm</span><span class="p">,</span>
                <span class="n">attn_activation</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(F\)</span> is the number of features/columns in the dataset</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">SelfAttentionMLP</span>


<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">SelfAttentionMLP</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="o">=</span><span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


      <p>Defines a <code>SelfAttentionMLP</code> model that can be used as the
deeptabular component of a Wide &amp; Deep model or independently by
itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features that are also embedded. These
are then passed through a series of attention blocks. Each attention
block is comprised by what we would refer as a simplified
<code>SelfAttentionEncoder</code>. See
<code>pytorch_widedeep.models.tabular.mlp._attention_layers</code> for details. The
reason to use a simplified version of self attention is because we
observed that the '<em>standard</em>' attention mechanism used in the
TabTransformer has a notable tendency to overfit.</p>
<p>In more detail, this model only uses Q and K (and not V). If we think
about it as in terms of text (and intuitively), the Softmax(QK^T) is the
attention mechanism that tells us how much, at each position in the input
sentence, each word is represented or 'expressed'. We refer to that
as 'attention weights'. These attention weighst are normally multiplied
by a Value matrix to further strength the focus on the words that each
word should be attending to (again, intuitively).</p>
<p>In this implementation we skip this last multiplication and instead we
multiply the attention weights directly by the input tensor. This is a
simplification that we expect is beneficial in terms of avoiding
overfitting for tabular data.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>add_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>frac_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>&#39;standard&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>input_dim</code></b>
              (<code>int</code>, default:
                  <code>32</code>
)
          –
          <div class="doc-md-description">
            <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.2</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for each attention block</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_heads</code></b>
              (<code>int</code>, default:
                  <code>8</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention heads per attention block.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_bias</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to use bias in the Q, K projection
layers.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>with_addnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if residual connections will be used in the attention blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;leaky_relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>String indicating the activation function to be applied to the dense
layer in each attention encoder. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em>
and <em>'gelu'</em> are supported.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_blocks</code></b>
              (<code>int</code>, default:
                  <code>3</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention blocks</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.cat_and_cont_embed">cat_and_cont_embed</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>This is the module that processes the categorical and continuous columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Sequence of attention encoders.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">SelfAttentionMLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SelfAttentionMLP</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/self_attention_mlp.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttentionMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_activation</span> <span class="o">=</span> <span class="n">attn_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Attention Blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;attention_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">SelfAttentionEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">with_addnorm</span><span class="p">,</span>
                <span class="n">attn_activation</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the WideDeep class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabTransformer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabTransformer</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">use_linear_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_flash_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


      <p>Defines our adptation of the
<a href="https://arxiv.org/abs/2012.06678">TabTransformer model</a>
that can be used as the <code>deeptabular</code> component of a
Wide &amp; Deep model or independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>:
This is an enhanced adaptation of the model described in the paper. It can
be considered as the flagship of our transformer family of models for
tabular data and offers mutiple, additional features relative to the
original publication(and some other models in the library)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>add_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>frac_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>input_dim</code></b>
              (<code>int</code>, default:
                  <code>32</code>
)
          –
          <div class="doc-md-description">
            <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_heads</code></b>
              (<code>int</code>, default:
                  <code>8</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention heads per Transformer block</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_qkv_bias</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_blocks</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Number of Transformer blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.2</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the Multi-Head Attention layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the FeedForward network</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_factor</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>transformer_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;gelu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_linear_attention</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if Linear Attention (from <a href="https://arxiv.org/abs/2006.16236">Transformers are RNNs:
Fast Autoregressive Transformers with Linear Attention</a>)
will be used. The inclusing of this mode of attention is inspired by
<a href="https://www.uber.com/en-GB/blog/deepeta-how-uber-predicts-arrival-times/">this post</a>,
where the Uber team finds that this attention mechanism leads to the
best results for their tabular data.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_flash_attention</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if
<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">Flash Attention</a>
will be used. <br/></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm_last</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_linear_first</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Sequence of Transformer blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.mlp">mlp</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>MLP component in the model</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabTransformer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_transformer.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">use_linear_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_flash_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="c1"># UGLY hack to be able to use the base class __init__ method</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;standard&quot;</span>
            <span class="k">if</span> <span class="n">embed_continuous_method</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">embed_continuous_method</span>
        <span class="p">),</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># we overwrite the embed_continuous_method parameter that was set</span>
    <span class="c1"># to &#39;standard&#39; if embed_continuous_method is None. In other words,</span>
    <span class="c1"># this will already have the right value if the user provided a value</span>
    <span class="c1"># for embed_continuous_method, otherwise will be &quot;standard&quot; when it</span>
    <span class="c1"># should be None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous_method</span> <span class="o">=</span> <span class="n">embed_continuous_method</span>

    <span class="k">if</span> <span class="n">embed_continuous</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span> <span class="o">=</span> <span class="n">embed_continuous</span>
        <span class="k">if</span> <span class="n">embed_continuous</span> <span class="ow">and</span> <span class="n">embed_continuous_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If &#39;embed_continuous&#39; is True, &#39;embed_continuous_method&#39; must be &quot;</span>
                <span class="s2">&quot;one of &#39;standard&#39;, &#39;piecewise&#39; or &#39;periodic&#39;.&quot;</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_linear_attention</span> <span class="o">=</span> <span class="n">use_linear_attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attention</span> <span class="o">=</span> <span class="n">use_flash_attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;If only continuous features are used &#39;embed_continuous&#39; must be set to &#39;True&#39;&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;transformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">TransformerEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">use_qkv_bias</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">ff_factor</span><span class="p">,</span>
                <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="n">use_linear_attention</span><span class="p">,</span>
                <span class="n">use_flash_attention</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_first_hidden_dim</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>:
if flash attention or linear attention
are used, no attention weights are saved during the training process
and calling this property will throw a ValueError</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">SAINT</span>


<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">SAINT</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


      <p>Defines a <a href="https://arxiv.org/abs/2106.01342">SAINT model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This is an slightly modified and enhanced
 version of the model described in the paper,</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>add_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>frac_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>&#39;standard&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>input_dim</code></b>
              (<code>int</code>, default:
                  <code>32</code>
)
          –
          <div class="doc-md-description">
            <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_heads</code></b>
              (<code>int</code>, default:
                  <code>8</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention heads per Transformer block</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_qkv_bias</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_blocks</code></b>
              (<code>int</code>, default:
                  <code>2</code>
)
          –
          <div class="doc-md-description">
            <p>Number of SAINT-Transformer blocks.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the Multi-Head Attention column and
row layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.2</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the FeedForward network</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_factor</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>transformer_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;gelu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm_last</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_linear_first</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.saint.SAINT.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Sequence of SAINT-Transformer blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.saint.SAINT.mlp">mlp</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>MLP component in the model</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">SAINT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SAINT</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/saint.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SAINT</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;saint_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">SaintEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">use_qkv_bias</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">ff_factor</span><span class="p">,</span>
                <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
    <span class="c1"># therefore all related params are optional</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List with the attention weights. Each element of the list is a tuple
where the first and the second elements are the column and row
attention weights respectively</p>
<p>The shape of the attention weights is:</p>
<ul>
<li>
<p>column attention: <span class="arithmatex">\((N, H, F, F)\)</span></p>
</li>
<li>
<p>row attention: <span class="arithmatex">\((1, H, N, N)\)</span></p>
</li>
</ul>
<p>where <span class="arithmatex">\(N\)</span> is the batch size, <span class="arithmatex">\(H\)</span> is the number of heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">FTTransformer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">FTTransformer</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">kv_compression_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">kv_sharing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mf">1.33</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;reglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


      <p>Defines a <a href="https://arxiv.org/abs/2106.11959">FTTransformer model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>add_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>frac_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>&#39;standard&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>input_dim</code></b>
              (<code>int</code>, default:
                  <code>64</code>
)
          –
          <div class="doc-md-description">
            <p>The so-called <em>dimension of the model</em>. Is the number of embeddings used to encode
the categorical and/or continuous columns.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>kv_compression_factor</code></b>
              (<code>float</code>, default:
                  <code>0.5</code>
)
          –
          <div class="doc-md-description">
            <p>By default, the FTTransformer uses Linear Attention
(See <a href="https://arxiv.org/abs/2006.04768&gt;">Linformer: Self-Attention with Linear Complexity</a> ).
The compression factor that will be used to reduce the input sequence
length. If we denote the resulting sequence length as
<span class="arithmatex">\(k = int(kv_{compression \space factor} \times s)\)</span>
where <span class="arithmatex">\(s\)</span> is the input sequence length.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>kv_sharing</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the <span class="arithmatex">\(E\)</span> and <span class="arithmatex">\(F\)</span> projection matrices
will share weights.  See <a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear
Complexity</a> for details</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_heads</code></b>
              (<code>int</code>, default:
                  <code>8</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention heads per FTTransformer block</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_qkv_bias</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_blocks</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Number of FTTransformer blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.2</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the Linear-Attention layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the FeedForward network</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_factor</code></b>
              (<code>float</code>, default:
                  <code>1.33</code>
)
          –
          <div class="doc-md-description">
            <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4, but they use 4/3
in the paper.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>transformer_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;reglu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
FTTransformer block will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm_last</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_linear_first</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Sequence of FTTransformer blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.mlp">mlp</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>MLP component in the model</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">FTTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FTTransformer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/ft_transformer.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">kv_compression_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">kv_sharing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.33</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FTTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">kv_compression_factor</span> <span class="o">=</span> <span class="n">kv_compression_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kv_sharing</span> <span class="o">=</span> <span class="n">kv_sharing</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="n">is_first</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;fttransformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">FTTransformerEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">use_qkv_bias</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">ff_factor</span><span class="p">,</span>
                <span class="n">kv_compression_factor</span><span class="p">,</span>
                <span class="n">kv_sharing</span><span class="p">,</span>
                <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="n">is_first</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">is_first</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
    <span class="c1"># therefore all related params are optional</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is: <span class="arithmatex">\((N, H, F, k)\)</span>, where <span class="arithmatex">\(N\)</span> is
the batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads, <span class="arithmatex">\(F\)</span> is the
number of features/columns and <span class="arithmatex">\(k\)</span> is the reduced sequence length or
dimension, i.e. <span class="arithmatex">\(k = int(kv_{compression \space factor} \times s)\)</span></p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabPerceiver</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabPerceiver</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">n_cross_attns</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_cross_attn_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latents</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">n_latent_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latent_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_perceiver_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;geglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


      <p>Defines an adaptation of a <a href="https://arxiv.org/abs/2103.03206">Perceiver</a>
 that can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model
 or independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: while there are scientific publications for
 the <code>TabTransformer</code>, <code>SAINT</code> and <code>FTTransformer</code>, the <code>TabPerceiver</code>
 and the <code>TabFastFormer</code> are our own adaptations of the
 <a href="https://arxiv.org/abs/2103.03206">Perceiver</a> and the
 <a href="https://arxiv.org/abs/2108.09084">FastFormer</a> for tabular data.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>add_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>frac_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>&#39;standard&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>input_dim</code></b>
              (<code>int</code>, default:
                  <code>32</code>
)
          –
          <div class="doc-md-description">
            <p>The so-called <em>dimension of the model</em>. Is the number of embeddings
used to encode the categorical and/or continuous columns.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_cross_attns</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>Number of times each perceiver block will cross attend to the input
data (i.e. number of cross attention components per perceiver block).
This should normally be 1. However, in the paper they describe some
architectures (normally computer vision-related problems) where the
Perceiver attends multiple times to the input array. Therefore, maybe
multiple cross attention to the input array is also useful in some
cases for tabular data <img alt="🤷" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f937.png" title=":shrug:" /> .</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_cross_attn_heads</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention heads for the cross attention component</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_latents</code></b>
              (<code>int</code>, default:
                  <code>16</code>
)
          –
          <div class="doc-md-description">
            <p>Number of latents. This is the <span class="arithmatex">\(N\)</span> parameter in the paper. As
indicated in the paper, this number should be significantly lower
than <span class="arithmatex">\(M\)</span> (the number of columns in the dataset). Setting <span class="arithmatex">\(N\)</span> closer
to <span class="arithmatex">\(M\)</span> defies the main purpose of the Perceiver, which is to overcome
the transformer quadratic bottleneck</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>latent_dim</code></b>
              (<code>int</code>, default:
                  <code>128</code>
)
          –
          <div class="doc-md-description">
            <p>Latent dimension.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_latent_heads</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention heads per Latent Transformer</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_latent_blocks</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Number of transformer encoder blocks (normalised MHA + normalised FF)
per Latent Transformer</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_perceiver_blocks</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Number of Perceiver blocks defined as [Cross Attention + Latent
Transformer]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_weights</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the weights will be shared between Perceiver
blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the Multi-Head Attention layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the FeedForward network</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_factor</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>transformer_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;geglu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm_last</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_linear_first</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.ModuleDict">ModuleDict</span></code>)
          –
          <div class="doc-md-description">
            <p>ModuleDict with the Perceiver blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.latents">latents</span></code></b>
              (<code><span title="torch.nn.Parameter">Parameter</span></code>)
          –
          <div class="doc-md-description">
            <p>Latents that will be used for prediction</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.mlp">mlp</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>MLP component in the model</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabPerceiver</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabPerceiver</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span> <span class="n">n_latents</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span><span class="n">n_perceiver_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_perceiver.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">n_cross_attns</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n_cross_attn_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latents</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">n_latent_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latent_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_perceiver_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;geglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabPerceiver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attns</span> <span class="o">=</span> <span class="n">n_cross_attns</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attn_heads</span> <span class="o">=</span> <span class="n">n_cross_attn_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_latents</span> <span class="o">=</span> <span class="n">n_latents</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="n">latent_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_latent_heads</span> <span class="o">=</span> <span class="n">n_latent_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_latent_blocks</span> <span class="o">=</span> <span class="n">n_latent_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_perceiver_blocks</span> <span class="o">=</span> <span class="n">n_perceiver_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span> <span class="o">=</span> <span class="n">share_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">latents</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">trunc_normal_</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_latents</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
    <span class="n">first_perceiver_block</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_perceiver_block</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block0&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">first_perceiver_block</span>

    <span class="k">if</span> <span class="n">share_weights</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_perceiver_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="n">first_perceiver_block</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_perceiver_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_perceiver_block</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span>

    <span class="c1"># Mlp</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List with the attention weights. If the weights are not shared
between perceiver blocks each element of the list will be a list
itself containing the Cross Attention and Latent Transformer
attention weights respectively</p>
<p>The shape of the attention weights is:</p>
<ul>
<li>
<p>Cross Attention: <span class="arithmatex">\((N, C, L, F)\)</span></p>
</li>
<li>
<p>Latent Attention: <span class="arithmatex">\((N, T, L, L)\)</span></p>
</li>
</ul>
<p>WHere <span class="arithmatex">\(N\)</span> is the batch size, <span class="arithmatex">\(C\)</span> is the number of Cross Attention
heads, <span class="arithmatex">\(L\)</span> is the number of Latents, <span class="arithmatex">\(F\)</span> is the number of
features/columns in the dataset and <span class="arithmatex">\(T\)</span> is the number of Latent
Attention heads</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TabFastFormer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TabFastFormer</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">share_qv_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>


      <p>Defines an adaptation of a <a href="https://arxiv.org/abs/2108.09084">FastFormer</a>
that can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model
or independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: while there are scientific publications for
 the <code>TabTransformer</code>, <code>SAINT</code> and <code>FTTransformer</code>, the <code>TabPerceiver</code>
 and the <code>TabFastFormer</code> are our own adaptations of the
 <a href="https://arxiv.org/abs/2103.03206">Perceiver</a> and the
 <a href="https://arxiv.org/abs/2108.09084">FastFormer</a> for tabular data.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>column_idx</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
          –
          <div class="doc-md-description">
            <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_input</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cat_bias</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cat_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>add_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>frac_shared_embed</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>continuous_cols</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the name of the numeric (aka continuous) columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_norm_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_continuous_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                  <code>&#39;standard&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>cont_embed_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>quantization_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_frequencies</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>sigma</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_last_layer</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>full_embed_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>input_dim</code></b>
              (<code>int</code>, default:
                  <code>32</code>
)
          –
          <div class="doc-md-description">
            <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_heads</code></b>
              (<code>int</code>, default:
                  <code>8</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention heads per FastFormer block</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_bias</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_blocks</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Number of FastFormer blocks</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the Additive Attention layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.2</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout that will be applied to the FeedForward network</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>ff_factor</code></b>
              (<code>int</code>, default:
                  <code>4</code>
)
          –
          <div class="doc-md-description">
            <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_qv_weights</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Following the paper, this is a boolean indicating if the Value (<span class="arithmatex">\(V\)</span>) and
the Query (<span class="arithmatex">\(Q\)</span>) transformation parameters will be shared.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>share_weights</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>In addition to sharing the <span class="arithmatex">\(V\)</span> and <span class="arithmatex">\(Q\)</span> transformation parameters, the
parameters across different Fastformer layers can also be shared.
Please, see
<code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code> for
details</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>transformer_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>MLP hidden dimensions. If not provided no MLP on top of the final
FTTransformer block will be used</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_batchnorm_last</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>mlp_linear_first</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.encoder">encoder</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Sequence of FasFormer blocks.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.mlp">mlp</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>MLP component in the model</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabFastFormer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabFastFormer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">share_qv_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabFastFormer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">share_qv_weights</span> <span class="o">=</span> <span class="n">share_qv_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span> <span class="o">=</span> <span class="n">share_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">first_fastformer_block</span> <span class="o">=</span> <span class="n">FastFormerEncoder</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">,</span>
        <span class="n">ff_dropout</span><span class="p">,</span>
        <span class="n">ff_factor</span><span class="p">,</span>
        <span class="n">share_qv_weights</span><span class="p">,</span>
        <span class="n">transformer_activation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;fastformer_block0&quot;</span><span class="p">,</span> <span class="n">first_fastformer_block</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">share_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;fastformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">first_fastformer_block</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;fastformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">FastFormerEncoder</span><span class="p">(</span>
                    <span class="n">input_dim</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">,</span>
                    <span class="n">use_bias</span><span class="p">,</span>
                    <span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="n">ff_dropout</span><span class="p">,</span>
                    <span class="n">ff_factor</span><span class="p">,</span>
                    <span class="n">share_qv_weights</span><span class="p">,</span>
                    <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
    <span class="c1"># therefore all related params are optional</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List with the attention weights. Each element of the list is a
tuple where the first and second elements are the <span class="arithmatex">\(\alpha\)</span>
and <span class="arithmatex">\(\beta\)</span> attention weights in the paper.</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F)\)</span> where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">BasicRNN</span>


<a href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">BasicRNN</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


      <p>Standard text classifier/regressor comprised by a stack of RNNs
(LSTMs or GRUs) that can be used as the <code>deeptext</code> component of a Wide &amp;
Deep model or independently by itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of
dense layers on top of the stack of RNNs</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>vocab_size</code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Number of words in the vocabulary</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_dim</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_matrix</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Pretrained word embeddings</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_trainable</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the pretrained embeddings are trainable</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>rnn_type</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[lstm, gru]</code>, default:
                  <code>&#39;lstm&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>String indicating the type of RNN to use. One of <em>'lstm'</em> or <em>'gru'</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>hidden_dim</code></b>
              (<code>int</code>, default:
                  <code>64</code>
)
          –
          <div class="doc-md-description">
            <p>Hidden dim of the RNN</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_layers</code></b>
              (<code>int</code>, default:
                  <code>3</code>
)
          –
          <div class="doc-md-description">
            <p>Number of recurrent layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>rnn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.0</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for each RNN layer except the last layer</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>bidirectional</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the staked RNNs are bidirectional</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_hidden_state</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether to use the final hidden state or the RNN's
output as predicting features. Typically the former is used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>padding_idx</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's tokenizer
where the token index 0 is reserved for the <em>'unknown'</em> word token.
Therefore, the default value is set to 1.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm_last</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_linear_first</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.word_embed">word_embed</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>word embedding matrix</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.rnn">rnn</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Stack of RNNs</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.rnn_mlp">rnn_mlp</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not None</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">BasicRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/text/rnns/basic_rnn.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BasicRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">embed_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;If no &#39;embed_matrix&#39; is passed, the embedding dimension must&quot;</span>
            <span class="s2">&quot;be specified with &#39;embed_dim&#39;&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;the input embedding dimension </span><span class="si">{}</span><span class="s2"> and the dimension of the &quot;</span>
            <span class="s2">&quot;pretrained embeddings </span><span class="si">{}</span><span class="s2"> do not match. The pretrained embeddings &quot;</span>
            <span class="s2">&quot;dimension (</span><span class="si">{}</span><span class="s2">) will be used&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">),</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span> <span class="o">=</span> <span class="n">embed_trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_dropout</span> <span class="o">=</span> <span class="n">rnn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_hidden_state</span> <span class="o">=</span> <span class="n">use_hidden_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

    <span class="c1"># Embeddings</span>
    <span class="k">if</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_embeddings</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
        <span class="p">)</span>

    <span class="c1"># RNN</span>
    <span class="n">rnn_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="s2">&quot;num_layers&quot;</span><span class="p">:</span> <span class="n">n_layers</span><span class="p">,</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">,</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">rnn_dropout</span><span class="p">,</span>
        <span class="s2">&quot;batch_first&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bidirectional</span> <span class="k">else</span> <span class="n">hidden_dim</span>

    <span class="c1"># FC-Head (Mlp)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MLP</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># simple hack to add readability in the forward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">AttentiveRNN</span>


<a href="#pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">AttentiveRNN</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN">BasicRNN</a></code></p>


      <p>Text classifier/regressor comprised by a stack of RNNs
(LSTMs or GRUs) plus an attention layer. This model can be used as the
<code>deeptext</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of dense
layers on top of attention layer</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>vocab_size</code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Number of words in the vocabulary</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_dim</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_matrix</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Pretrained word embeddings</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_trainable</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the pretrained embeddings are trainable</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>rnn_type</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[lstm, gru]</code>, default:
                  <code>&#39;lstm&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>String indicating the type of RNN to use. One of <em>'lstm'</em> or <em>'gru'</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>hidden_dim</code></b>
              (<code>int</code>, default:
                  <code>64</code>
)
          –
          <div class="doc-md-description">
            <p>Hidden dim of the RNN</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_layers</code></b>
              (<code>int</code>, default:
                  <code>3</code>
)
          –
          <div class="doc-md-description">
            <p>Number of recurrent layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>rnn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout for each RNN layer except the last layer</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>bidirectional</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the staked RNNs are bidirectional</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_hidden_state</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether to use the final hidden state or the RNN's
output as predicting features. Typically the former is used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>padding_idx</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's
tokenizer where the token index 0 is reserved for the <em>'unknown'</em>
word token. Therefore, the default value is set to 1.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_concatenate</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the input to the attention mechanism will be the
output of the RNN or the output of the RNN concatenated with the last
hidden state.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Internal dropout for the attention mechanism</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm_last</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_linear_first</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.word_embed">word_embed</span></code></b>
              (<code><span title="nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>word embedding matrix</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.rnn">rnn</span></code></b>
              (<code><span title="nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Stack of RNNs</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.rnn_mlp">rnn_mlp</span></code></b>
              (<code><span title="nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not <code>None</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">AttentiveRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AttentiveRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/text/rnns/attentive_rnn.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">embed_matrix</span><span class="o">=</span><span class="n">embed_matrix</span><span class="p">,</span>
        <span class="n">embed_trainable</span><span class="o">=</span><span class="n">embed_trainable</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
        <span class="n">rnn_dropout</span><span class="o">=</span><span class="n">rnn_dropout</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">use_hidden_state</span><span class="o">=</span><span class="n">use_hidden_state</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="o">=</span><span class="n">head_hidden_dims</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="o">=</span><span class="n">head_activation</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="o">=</span><span class="n">head_dropout</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="o">=</span><span class="n">head_batchnorm</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="n">head_batchnorm_last</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="o">=</span><span class="n">head_linear_first</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Embeddings and RNN defined in the BasicRNN inherited class</span>

    <span class="c1"># Attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_concatenate</span> <span class="o">=</span> <span class="n">attn_concatenate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>

    <span class="k">if</span> <span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="k">elif</span> <span class="n">bidirectional</span> <span class="ow">or</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">ContextAttention</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="p">,</span> <span class="n">sum_along_seq</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># FC-Head (Mlp)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List with the attention weights</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, S)\)</span>, where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(S\)</span> is the length of the sequence</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">StackedAttentiveRNN</span>


<a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">StackedAttentiveRNN</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


      <p>Text classifier/regressor comprised by a stack of blocks:
<code>[RNN + Attention]</code>. This can be used as the <code>deeptext</code> component of a
Wide &amp; Deep model or independently by itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of
dense layers on top of the attentiob blocks</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>vocab_size</code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Number of words in the vocabulary</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_dim</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_matrix</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Pretrained word embeddings</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>embed_trainable</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the pretrained embeddings are trainable</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>rnn_type</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[lstm, gru]</code>, default:
                  <code>&#39;lstm&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>String indicating the type of RNN to use. One of 'lstm' or 'gru'</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>hidden_dim</code></b>
              (<code>int</code>, default:
                  <code>64</code>
)
          –
          <div class="doc-md-description">
            <p>Hidden dim of the RNN</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>bidirectional</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the staked RNNs are bidirectional</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>padding_idx</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's
tokenizer where the token index 0 is reserved for the <em>'unknown'</em>
word token. Therefore, the default value is set to 1.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_blocks</code></b>
              (<code>int</code>, default:
                  <code>3</code>
)
          –
          <div class="doc-md-description">
            <p>Number of attention blocks. Each block is comprised by an RNN and a
Context Attention Encoder</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_concatenate</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the input to the attention mechanism will be the
output of the RNN or the output of the RNN concatenated with the last
hidden state or simply</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>attn_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Internal dropout for the attention mechanism</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>with_addnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the output of each block will be added to the
input and normalised</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the sizes of the dense layers in the head e.g: [128, 64]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm_last</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_linear_first</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.word_embed">word_embed</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>word embedding matrix</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.rnn">rnn</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Stack of RNNs</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.rnn_mlp">rnn_mlp</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not <code>None</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">StackedAttentiveRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">StackedAttentiveRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/text/rnns/stacked_attentive_rnn.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">StackedAttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;the input embedding dimension </span><span class="si">{}</span><span class="s2"> and the dimension of the &quot;</span>
            <span class="s2">&quot;pretrained embeddings </span><span class="si">{}</span><span class="s2"> do not match. The pretrained embeddings &quot;</span>
            <span class="s2">&quot;dimension (</span><span class="si">{}</span><span class="s2">) will be used&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">),</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span> <span class="o">=</span> <span class="n">embed_trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_concatenate</span> <span class="o">=</span> <span class="n">attn_concatenate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

    <span class="c1"># Embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_embeddings</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">)</span>

    <span class="c1"># Linear Projection: if embed_dim is different that the input of the</span>
    <span class="c1"># attention blocks we add a linear projection</span>
    <span class="k">if</span> <span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="k">elif</span> <span class="n">bidirectional</span> <span class="ow">or</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_proj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="c1"># RNN</span>
    <span class="n">rnn_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">,</span>
        <span class="s2">&quot;batch_first&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>

    <span class="c1"># FC-Head (Mlp)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ContextAttentionEncoder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">attn_concatenate</span><span class="p">,</span>
                <span class="n">with_addnorm</span><span class="o">=</span><span class="n">with_addnorm</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">sum_along_seq</span><span class="o">=</span><span class="n">i</span> <span class="o">==</span> <span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Mlp</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MLP</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># simple hack to add readability in the forward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.attention_weights" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weights</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, S)\)</span> Where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(S\)</span> is the length of the sequence</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">HFModel</span>


<a href="#pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">HFModel</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">use_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">trainable_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


      <p>This class is a wrapper around the Hugging Face transformers library. It
can be used as the text component of a Wide &amp; Deep model or independently
by itself.</p>
<p>At the moment only models from the families BERT, RoBERTa, DistilBERT,
ALBERT and ELECTRA are supported. This is because this library is
designed to address classification and regression tasks and these are the
most 'popular' encoder-only models, which have proved to be those that
work best for these tasks.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>model_name</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>The model name from the transformers library e.g. 'bert-base-uncased'.
Currently supported models are those from the families: BERT, RoBERTa,
DistilBERT, ALBERT and ELECTRA.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>use_cls_token</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether to use the [CLS] token or the mean of the
sequence of hidden states as the sentence embedding</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>trainable_parameters</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the names of the model parameters that will be trained. If
None, none of the parameters will be trainable</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to include batch normalization in the
dense layers that form the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm_last</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_linear_first</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>verbose</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>If True, it will print information about the model</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>**kwargs</code></b>
          –
          <div class="doc-md-description">
            <p>Additional kwargs to be passed to the model</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel.head">head</span></code></b>
              (<code><span title="nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>Stack of dense layers on top of the transformer. This will only exists
if <code>head_layers_dim</code> is not None</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">HFModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">HFModel</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/text/huggingface_transformers/hf_model.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;use_cls_token&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;use_special_token&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">use_cls_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">trainable_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># TO DO: add warning regarging ELECTRA as ELECTRA does not have a cls</span>
    <span class="c1"># token.  Research what happens with ELECTRA</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_cls_token</span> <span class="o">=</span> <span class="n">use_cls_token</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span> <span class="o">=</span> <span class="n">trainable_parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cls_token</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;The model will use the [CLS] token. Make sure the tokenizer &quot;</span>
            <span class="s2">&quot;was run with add_special_tokens=True&quot;</span><span class="p">,</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model_class</span> <span class="o">=</span> <span class="n">get_model_class</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">get_config_and_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">output_attention_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="nb">any</span><span class="p">([</span><span class="n">tl</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">tl</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span><span class="p">])</span>

    <span class="c1"># FC-Head (Mlp). Note that the FC head will always be trainable</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel.attention_weight" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">attention_weight</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel.attention_weight" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weight</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Returns the attention weights if the model was created with the
output_attention_weights=True argument. If not, it will raise an
AttributeError.</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
sequence length.</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.image.vision.Vision" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Vision</span>


<a href="#pytorch_widedeep.models.image.vision.Vision" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">Vision</span><span class="p">(</span>
    <span class="n">pretrained_model_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">channel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


      <p>Defines a standard image classifier/regressor using a pretrained
network or a sequence of convolution layers that can be used as the
<code>deepimage</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: this class represents the integration
 between <code>pytorch-widedeep</code> and <code>torchvision</code>. New architectures will be
 available as they are added to <code>torchvision</code>. In a distant future we aim
 to bring transformer-based architectures as well. However, simple
 CNN-based architectures (and even MLP-based) seem to produce SoTA
 results. For the time being, we describe below the options available
 through this class</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>pretrained_model_setup</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[str, <span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.Union">Union</span>[str, <span title="pytorch_widedeep.wdtypes.WeightsEnum">WeightsEnum</span>]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Name of the pretrained model. Should be a variant of the following
architectures: <em>'resnet'</em>, <em>'shufflenet'</em>, <em>'resnext'</em>,
<em>'wide_resnet'</em>, <em>'regnet'</em>, <em>'densenet'</em>, <em>'mobilenetv3'</em>,
<em>'mobilenetv2'</em>, <em>'mnasnet'</em>, <em>'efficientnet'</em> and <em>'squeezenet'</em>. if
<code>pretrained_model_setup = None</code> a basic, fully trainable CNN will be
used. Alternatively, since Torchvision 0.13 one can use pretrained
models with different weigths. Therefore, <code>pretrained_model_setup</code> can
also be dictionary with the name of the model and the weights (e.g.
<code>{'resnet50': ResNet50_Weights.DEFAULT}</code> or
<code>{'resnet50': "IMAGENET1K_V2"}</code>). <br/> Aliased as <code>pretrained_model_name</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>n_trainable</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Number of trainable layers starting from the layer closer to the
output neuron(s). Note that this number DOES NOT take into account
the so-called <em>'head'</em> which is ALWAYS trainable. If
<code>trainable_params</code> is not None this parameter will be ignored</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>trainable_params</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List of strings containing the names (or substring within the name) of
the parameters that will be trained. For example, if we use a
<em>'resnet18'</em> pretrained model and we set <code>trainable_params =
['layer4']</code> only the parameters of <em>'layer4'</em> of the network
(and the head, as mentioned before) will be trained. Note that
setting this or the previous parameter involves some knowledge of
the architecture used.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>channel_sizes</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                  <code>[64, 128, 256, 512]</code>
)
          –
          <div class="doc-md-description">
            <p>List of integers with the channel sizes of a CNN in case we choose not
to use a pretrained model</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>kernel_sizes</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>[7, 3, 3, 3]</code>
)
          –
          <div class="doc-md-description">
            <p>List of integers with the kernel sizes of a CNN in case we choose not
to use a pretrained model. Must be of length equal to <code>len(channel_sizes) - 1</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>strides</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>[2, 1, 1, 1]</code>
)
          –
          <div class="doc-md-description">
            <p>List of integers with the stride sizes of a CNN in case we choose not
to use a pretrained model. Must be of length equal to <code>len(channel_sizes) - 1</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per dense layer in the head. e.g: <em>[64,32]</em></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>float indicating the dropout between the dense layers.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm_last</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_linear_first</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.image.vision.Vision.features">features</span></code></b>
              (<code><span title="torch.nn.Module">Module</span></code>)
          –
          <div class="doc-md-description">
            <p>The pretrained model or Standard CNN plus the optional head</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Vision</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Vision</span><span class="p">(</span><span class="n">channel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">kernel_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">head_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_img</span><span class="p">)</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/image/vision.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;pretrained_model_setup&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;pretrained_model_name&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_setup</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">WeightsEnum</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_trainable</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">channel_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">kernel_sizes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">strides</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Vision</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_check_pretrained_model_setup</span><span class="p">(</span>
        <span class="n">pretrained_model_setup</span><span class="p">,</span> <span class="n">n_trainable</span><span class="p">,</span> <span class="n">trainable_params</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span> <span class="o">=</span> <span class="n">pretrained_model_setup</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_trainable</span> <span class="o">=</span> <span class="n">n_trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainable_params</span> <span class="o">=</span> <span class="n">trainable_params</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">channel_sizes</span> <span class="o">=</span> <span class="n">channel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_sizes</span> <span class="o">=</span> <span class="n">kernel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">strides</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_features</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">pretrained_model_setup</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vision_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.image.vision.Vision.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.image.vision.Vision.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.model_fusion.ModelFuser" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ModelFuser</span>


<a href="#pytorch_widedeep.models.model_fusion.ModelFuser" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">ModelFuser</span><span class="p">(</span>
    <span class="n">models</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">fusion_method</span><span class="p">,</span>
    <span class="n">projection_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>


      <p>This class is a wrapper around a list of models that are associated to the
different text and/or image columns (and datasets) The class is designed
to 'fuse' the models using a variety of methods.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>models</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]</code>)
          –
          <div class="doc-md-description">
            <p>List of models whose outputs will be fused</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>fusion_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[concatenate, mean, max, sum, mult, dot, <span title="pytorch_widedeep.models.model_fusion.ModelFuser.head">head</span>], <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[concatenate, mean, max, sum, mult, <span title="pytorch_widedeep.models.model_fusion.ModelFuser.head">head</span>]]]</code>)
          –
          <div class="doc-md-description">
            <p>Method to fuse the output of the models. It can be one of
['concatenate', 'mean', 'max', 'sum', 'mult', 'dot', 'head'] or a
list of those, but 'dot'. If a list is provided the output of the
models will be fused using all the methods in the list and the final
output will be the concatenation of the outputs of each method</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>projection_method</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[min, max, mean]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>If the fusion_method is not 'concatenate', this parameter will
determine how to project the output of the models to a common
dimension. It can be one of ['min', 'max', 'mean']. Default is None</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>custom_head</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>, <span title="torch.nn.Module">Module</span>]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Custom head to be used to fuse the output of the models. If provided,
this will take precedence over head_hidden_dims. Also, if
provided, 'projection_method' will be ignored.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the number of neurons per layer in the custom head. If
custom_head is provided, this parameter will be ignored</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_activation</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function to be used in the custom head. Default is None</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_dropout</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout to be used in the custom head. Default is None</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Whether to use batchnorm in the custom head. Default is None</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm_last</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Whether or not batch normalization will be applied to the last of the
dense layers</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_linear_first</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="pytorch_widedeep.models.model_fusion.ModelFuser.head">head</span></code></b>
              (<code><span title="torch.nn.Module">Module</span> or <span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code>)
          –
          <div class="doc-md-description">
            <p>Custom head to be used to fuse the output of the models. If
custom_head is provided, this will take precedence over
head_hidden_dims</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">TextPreprocessor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">BasicRNN</span><span class="p">,</span> <span class="n">ModelFuser</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text_col1&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;hello world&#39;</span><span class="p">,</span> <span class="s1">&#39;this is a test&#39;</span><span class="p">],</span>
<span class="gp">... </span><span class="s1">&#39;text_col2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;goodbye world&#39;</span><span class="p">,</span> <span class="s1">&#39;this is another test&#39;</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_preprocessor_1</span> <span class="o">=</span> <span class="n">TextPreprocessor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">text_col</span><span class="o">=</span><span class="s2">&quot;text_col1&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_vocab</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">maxlen</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_preprocessor_2</span> <span class="o">=</span> <span class="n">TextPreprocessor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">text_col</span><span class="o">=</span><span class="s2">&quot;text_col2&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_vocab</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">maxlen</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text1</span> <span class="o">=</span> <span class="n">text_preprocessor_1</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text2</span> <span class="o">=</span> <span class="n">text_preprocessor_2</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text1_tnsr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_text1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text2_tnsr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_text2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rnn1</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">text_preprocessor_1</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rnn2</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">text_preprocessor_2</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fused_model</span> <span class="o">=</span> <span class="n">ModelFuser</span><span class="p">(</span><span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="n">rnn1</span><span class="p">,</span> <span class="n">rnn2</span><span class="p">],</span> <span class="n">fusion_method</span><span class="o">=</span><span class="s1">&#39;concatenate&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">fused_model</span><span class="p">([</span><span class="n">X_text1_tnsr</span><span class="p">,</span> <span class="n">X_text2_tnsr</span><span class="p">])</span>
</code></pre></div>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/model_fusion.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">fusion_method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span>
            <span class="s2">&quot;concatenate&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mult&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dot&quot;</span><span class="p">,</span>
            <span class="s2">&quot;head&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">List</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;concatenate&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="s2">&quot;mult&quot;</span><span class="p">,</span> <span class="s2">&quot;head&quot;</span><span class="p">]],</span>
    <span class="p">],</span>
    <span class="n">projection_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_head</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ModelFuser</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="o">=</span> <span class="n">fusion_method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">=</span> <span class="n">projection_method</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">all_output_dim_equal</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">check_input_parameters</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;head&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">custom_head</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;When using &#39;head&#39; as fusion_method, either head_hidden_dims or custom_head must be provided&quot;</span>
        <span class="k">if</span> <span class="n">custom_head</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># custom_head takes precedence over head_hidden_dims (in case</span>
            <span class="c1"># both are provided)</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span>
                <span class="n">custom_head</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span>
            <span class="p">),</span> <span class="s2">&quot;custom_head must have an &#39;output_dim&#39; property&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">custom_head</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
                <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="nb">sum</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">])]</span>
                <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span>
                <span class="p">),</span>
                <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span><span class="p">,</span>
                <span class="n">batchnorm</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span>
                <span class="p">),</span>
                <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">False</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span>
                <span class="p">),</span>
                <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                    <span class="kc">True</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span>
                <span class="p">),</span>
            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.model_fusion.ModelFuser.output_dim" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_dim</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.model_fusion.ModelFuser.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Returns the output dimension of the model.</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h3 id="pytorch_widedeep.models.model_fusion.ModelFuser.project" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">project</span>


<a href="#pytorch_widedeep.models.model_fusion.ModelFuser.project" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">project</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Projects the output of the models to a common dimension.</p>

            <details class="quote">
              <summary>Source code in <code>pytorch_widedeep/models/model_fusion.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">project</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Projects the output of the models to a common dimension.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_output_dim_equal</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="n">output_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span><span class="p">:</span>
        <span class="n">proj_dim</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">output_dims</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">output_dims</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">proj_dim</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
        <span class="n">proj_dim</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">output_dims</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">output_dims</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">proj_dim</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_method</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
        <span class="n">proj_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">output_dims</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_dims</span><span class="p">))</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;projection_method must be one of [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;]&quot;</span><span class="p">)</span>

    <span class="n">x_proj</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">idx</span><span class="p">:</span>
            <span class="n">x_proj</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_proj</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">output_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">proj_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">x_proj</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.wide_deep.WideDeep" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">WideDeep</span>


<a href="#pytorch_widedeep.models.wide_deep.WideDeep" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">WideDeep</span><span class="p">(</span>
    <span class="n">wide</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptabular</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptext</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deepimage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deephead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">enforce_positive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">enforce_positive_activation</span><span class="o">=</span><span class="s2">&quot;softplus&quot;</span><span class="p">,</span>
    <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Main collector class that combines all <code>wide</code>, <code>deeptabular</code>
<code>deeptext</code> and <code>deepimage</code> models.</p>
<p>Note that all models described so far in this library must be passed to
the <code>WideDeep</code> class once constructed. This is because the models output
the last layer before the prediction layer. Such prediction layer is
added by the <code>WideDeep</code> class as it collects the components for every
data mode.</p>
<p>There are two options to combine these models that correspond to the
two main architectures that <code>pytorch-widedeep</code> can build.</p>
<ul>
<li>
<p>Directly connecting the output of the model components to an ouput neuron(s).</p>
</li>
<li>
<p>Adding a <code>Fully-Connected Head</code> (FC-Head) on top of the deep models.
  This FC-Head will combine the output form the <code>deeptabular</code>, <code>deeptext</code> and
  <code>deepimage</code> and will be then connected to the output neuron(s).</p>
</li>
</ul>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code>wide</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="torch.nn.Module">Module</span>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p><code>Wide</code> model. This is a linear model where the non-linearities are
captured via crossed-columns.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>deeptabular</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>, <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Currently this library implements a number of possible architectures
for the <code>deeptabular</code> component. See the documenation of the
package. Note that <code>deeptabular</code> can be a list of models. This is
useful when using multiple tabular inputs (e.g. for example in the
context of a two-tower model for recommendation systems)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>deeptext</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>, <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Currently this library implements a number of possible architectures
for the <code>deeptext</code> component. See the documenation of the
package. Note that <code>deeptext</code> can be a list of models. This is useful
when using multiple text inputs.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>deepimage</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Union">Union</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>, <span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Currently this library uses <code>torchvision</code> and implements a number of
possible architectures for the <code>deepimage</code> component. See the
documenation of the package. Note that <code>deepimage</code> can be a list of
models. This is useful when using multiple image inputs.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>deephead</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>Alternatively, the user can pass a custom model that will receive the
output of the deep component. If <code>deephead</code> is not None all the
previous fc-head parameters will be ignored</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_hidden_dims</code></b>
              (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>List with the sizes of the dense layers in the head e.g: [128, 64]</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;relu&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function for the dense layers in the head. Currently
<code>'tanh'</code>, <code>'relu'</code>, <code>'leaky_relu'</code> and <code>'gelu'</code> are supported</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_dropout</code></b>
              (<code>float</code>, default:
                  <code>0.1</code>
)
          –
          <div class="doc-md-description">
            <p>Dropout of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <code>'rnn_mlp'</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_batchnorm_last</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>head_linear_first</code></b>
              (<code>bool</code>, default:
                  <code>True</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>enforce_positive</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Boolean indicating if the output from the final layer must be
positive. This is important if you are using loss functions with
non-negative input restrictions, e.g. RMSLE, or if you know your
predictions are bounded in between 0 and inf</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>enforce_positive_activation</code></b>
              (<code>str</code>, default:
                  <code>&#39;softplus&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>Activation function to enforce that the final layer has a positive
output. <code>'softplus'</code> or <code>'relu'</code> are supported.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code>pred_dim</code></b>
              (<code>int</code>, default:
                  <code>1</code>
)
          –
          <div class="doc-md-description">
            <p>Size of the final wide and deep output layer containing the
predictions. <code>1</code> for regression and binary classification or number
of classes for multiclass classification.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnet</span><span class="p">,</span> <span class="n">Vision</span><span class="p">,</span> <span class="n">BasicRNN</span><span class="p">,</span> <span class="n">Wide</span><span class="p">,</span> <span class="n">WideDeep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">][:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deeptabular</span> <span class="o">=</span> <span class="n">TabResnet</span><span class="p">(</span><span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">embed_input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deeptext</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deepimage</span> <span class="o">=</span> <span class="n">Vision</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">WideDeep</span><span class="p">(</span><span class="n">wide</span><span class="o">=</span><span class="n">wide</span><span class="p">,</span> <span class="n">deeptabular</span><span class="o">=</span><span class="n">deeptabular</span><span class="p">,</span> <span class="n">deeptext</span><span class="o">=</span><span class="n">deeptext</span><span class="p">,</span> <span class="n">deepimage</span><span class="o">=</span><span class="n">deepimage</span><span class="p">)</span>
</code></pre></div>
    <p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: It is possible to use custom components to
 build Wide &amp; Deep models. Simply, build them and pass them as the
 corresponding parameters. Note that the custom models MUST return a last
 layer of activations(i.e. not the final prediction) so that  these
 activations are collected by <code>WideDeep</code> and combined accordingly. In
 addition, the models MUST also contain an attribute <code>output_dim</code> with
 the size of these last layers of activations. See for example
 <code>pytorch_widedeep.models.tab_mlp.TabMlp</code></p>

                  <details class="quote">
                    <summary>Source code in <code>pytorch_widedeep/models/wide_deep.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span>  <span class="c1"># noqa: C901</span>
    <span class="s2">&quot;pred_dim&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;num_class&quot;</span><span class="p">,</span> <span class="s2">&quot;pred_size&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">wide</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptabular</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptext</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deepimage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deephead</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">enforce_positive</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">enforce_positive_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;softplus&quot;</span><span class="p">,</span>
    <span class="n">pred_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">WideDeep</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span>
        <span class="n">wide</span><span class="p">,</span>
        <span class="n">deeptabular</span><span class="p">,</span>
        <span class="n">deeptext</span><span class="p">,</span>
        <span class="n">deepimage</span><span class="p">,</span>
        <span class="n">deephead</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">,</span>
        <span class="n">pred_dim</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># this attribute will be eventually over-written by the Trainer&#39;s</span>
    <span class="c1"># device. Acts here as a &#39;placeholder&#39;.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wd_device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># required as attribute just in case we pass a deephead</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span> <span class="o">=</span> <span class="n">pred_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">enforce_positive</span> <span class="o">=</span> <span class="n">enforce_positive</span>

    <span class="c1"># better to set this attribute already here</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">deeptabular</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span> <span class="o">=</span> <span class="n">deeptabular</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;TabNet&quot;</span>

    <span class="c1"># The main 5 components of the wide and deep assemble: wide,</span>
    <span class="c1"># deeptabular, deeptext, deepimage and deephead</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_deephead</span> <span class="o">=</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_deephead</span><span class="p">(</span>
            <span class="n">deeptabular</span><span class="p">,</span>
            <span class="n">deeptext</span><span class="p">,</span>
            <span class="n">deepimage</span><span class="p">,</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">deephead</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">deephead</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># for consistency with other components we default to None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">wide</span> <span class="o">=</span> <span class="n">wide</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptext</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepimage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_model_components</span><span class="p">(</span>
        <span class="n">deeptabular</span><span class="p">,</span> <span class="n">deeptext</span><span class="p">,</span> <span class="n">deepimage</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enforce_positive</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enf_pos</span> <span class="o">=</span> <span class="n">get_activation_fn</span><span class="p">(</span><span class="n">enforce_positive_activation</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>








  <aside class="md-source-file">
    
    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 5.5A3.5 3.5 0 0 1 15.5 9a3.5 3.5 0 0 1-3.5 3.5A3.5 3.5 0 0 1 8.5 9 3.5 3.5 0 0 1 12 5.5M5 8c.56 0 1.08.15 1.53.42-.15 1.43.27 2.85 1.13 3.96C7.16 13.34 6.16 14 5 14a3 3 0 0 1-3-3 3 3 0 0 1 3-3m14 0a3 3 0 0 1 3 3 3 3 0 0 1-3 3c-1.16 0-2.16-.66-2.66-1.62a5.536 5.536 0 0 0 1.13-3.96c.45-.27.97-.42 1.53-.42M5.5 18.25c0-2.07 2.91-3.75 6.5-3.75s6.5 1.68 6.5 3.75V20h-13v-1.75M0 20v-1.5c0-1.39 1.89-2.56 4.45-2.9-.59.68-.95 1.62-.95 2.65V20H0m24 0h-3.5v-1.75c0-1.03-.36-1.97-.95-2.65 2.56.34 4.45 1.51 4.45 2.9V20Z"/></svg>
      
    </span>
    <nav>
      
        <a href="mailto:jrzaurin@gmail.com">Javier</a>, 
        <a href="mailto:javierrodriguezzaurin@javiers-macbook-pro.local">Javier Rodriguez Zaurin</a>
    </nav>
  </span>

    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Javier Zaurin and Pavol Mulinka
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://jrzaurin.medium.com/" target="_blank" rel="noopener" title="jrzaurin.medium.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.expand", "toc.integrate"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.af256bd8.min.js"></script>
      
        <script src="../stylesheets/extra.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>