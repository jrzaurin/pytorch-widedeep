
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch-widedeep.readthedocs.io/pytorch-widedeep/model_components.html">
      
      
        <link rel="prev" href="load_from_folder.html">
      
      
        <link rel="next" href="bayesian_models.html">
      
      
      <link rel="icon" href="../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.13">
    
    
      
        <title>Model Components - pytorch_widedeep</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-models-module" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="pytorch_widedeep" class="md-header__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            pytorch_widedeep
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Model Components
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../installation.html" class="md-tabs__link">
        
  
    
  
  Installation

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../quick_start.html" class="md-tabs__link">
        
  
    
  
  Quick Start

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="utils/index.html" class="md-tabs__link">
          
  
    
  
  Pytorch-widedeep

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../examples/01_preprocessors_and_utils.html" class="md-tabs__link">
          
  
    
  
  Examples

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../contributing.html" class="md-tabs__link">
        
  
    
  
  Contributing

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="pytorch_widedeep" class="md-nav__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    pytorch_widedeep
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quick_start.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Pytorch-widedeep
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Pytorch-widedeep
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="utils/index.html" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/deeptabular_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deeptabular utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/fastai_transforms.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fastai transforms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/image_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/text_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="preprocessing.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Preprocessing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="load_from_folder.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Load From Folder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Model Components
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="model_components.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Model Components
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.linear.wide.Wide" class="md-nav__link">
    <span class="md-ellipsis">
      Wide
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="md-nav__link">
    <span class="md-ellipsis">
      TabMlp
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      TabMlpDecoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="md-nav__link">
    <span class="md-ellipsis">
      TabResnet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      TabResnetDecoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="md-nav__link">
    <span class="md-ellipsis">
      TabNet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="md-nav__link">
    <span class="md-ellipsis">
      TabNetDecoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="md-nav__link">
    <span class="md-ellipsis">
      ContextAttentionMLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="md-nav__link">
    <span class="md-ellipsis">
      SelfAttentionMLP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      TabTransformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="md-nav__link">
    <span class="md-ellipsis">
      SAINT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="md-nav__link">
    <span class="md-ellipsis">
      FTTransformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="md-nav__link">
    <span class="md-ellipsis">
      TabPerceiver
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="md-nav__link">
    <span class="md-ellipsis">
      TabFastFormer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" class="md-nav__link">
    <span class="md-ellipsis">
      BasicRNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN" class="md-nav__link">
    <span class="md-ellipsis">
      AttentiveRNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN" class="md-nav__link">
    <span class="md-ellipsis">
      StackedAttentiveRNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.miscellaneous.basic_transformer.Transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel" class="md-nav__link">
    <span class="md-ellipsis">
      HFModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.image.vision.Vision" class="md-nav__link">
    <span class="md-ellipsis">
      Vision
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.wide_deep.WideDeep" class="md-nav__link">
    <span class="md-ellipsis">
      WideDeep
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.fds_layer.FDSLayer" class="md-nav__link">
    <span class="md-ellipsis">
      FDSLayer
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bayesian models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="losses.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Losses
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Metrics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="dataloaders.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataloaders
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="callbacks.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="trainer.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_trainer.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bayesian Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="self_supervised_pretraining.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Self Supervised Pretraining
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="tab2vec.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tab2Vec
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/01_preprocessors_and_utils.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01_preprocessors_and_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/02_model_components.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    02_model_components
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/03_binary_classification_with_defaults.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    03_binary_classification_with_defaults
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/04_regression_with_images_and_text.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    04_regression_with_images_and_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/05_save_and_load_model_and_artifacts.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    05_save_and_load_model_and_artifacts
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/06_finetune_and_warmup.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    06_finetune_and_warmup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/07_custom_components.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    07_custom_components
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/08_custom_dataLoader_imbalanced_dataset.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    08_custom_dataLoader_imbalanced_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/09_extracting_embeddings.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    09_extracting_embeddings
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/10_3rd_party_integration-RayTune_WnB.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10_3rd_party_integration-RayTune_WnB
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/11_auc_multiclass.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11_auc_multiclass
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/12_ZILNLoss_origkeras_vs_pytorch_widedeep.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12_ZILNLoss_origkeras_vs_pytorch_widedeep
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/13_model_uncertainty_prediction.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13_model_uncertainty_prediction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/14_bayesian_models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14_bayesian_models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/15_DIR-LDS_and_FDS.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15_DIR-LDS_and_FDS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/16_Self_Supervised_Pretraning_pt1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16_Self-Supervised Pre-Training pt 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/16_Self_Supervised_Pretraning_pt2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16_Self-Supervised Pre-Training pt 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/17_Usign_a_custom_hugging_face_model.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17_Usign-a-custom-hugging-face-model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/18_feature_importance_via_attention_weights.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18_feature_importance_via_attention_weights
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/19_wide_and_deep_for_recsys_pt1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    19_wide_and_deep_for_recsys_pt1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/19_wide_and_deep_for_recsys_pt2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    19_wide_and_deep_for_recsys_pt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/20_load_from_folder_functionality.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    20_load_from_folder_functionality
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/21_Using_huggingface_within_widedeep.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    21-Using-huggingface-within-widedeep
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="the-models-module">The <code>models</code> module<a class="headerlink" href="#the-models-module" title="Permanent link">&para;</a></h1>
<p>This module contains the models that can be used as the four main components
that will comprise a Wide and Deep model (<code>wide</code>, <code>deeptabular</code>,
<code>deeptext</code>, <code>deepimage</code>), as well as the <code>WideDeep</code> "constructor"
class. Note that each of the four components can be used independently. It
also contains all the documentation for the models that can be used for
self-supervised pre-training with tabular data.</p>


<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.linear.wide.Wide" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">Wide</span>


<a href="#pytorch_widedeep.models.tabular.linear.wide.Wide" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">Wide</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Defines a <code>Wide</code> (linear) model where the non-linearities are
captured via the so-called crossed-columns. This can be used as the
<code>wide</code> component of a Wide &amp; Deep model.</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>input_dim</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>size of the Linear layer (implemented via an Embedding layer).
<code>input_dim</code> is the summation of all the individual values for all the
features that go through the wide model. For example, if the wide
model receives 2 features with 5 individual values each, <code>input_dim =
10</code></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>pred_dim</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>size of the ouput tensor containing the predictions. Note that unlike
all the other models, the wide model is connected directly to the
output neuron(s) when used to build a Wide and Deep model. Therefore,
it requires the <code>pred_dim</code> parameter.</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.linear.wide.Wide.wide_linear">wide_linear</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>the linear layer that comprises the wide branch of the model</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Wide</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">wide</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/linear/wide.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;pred_dim&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;pred_size&quot;</span><span class="p">,</span> <span class="s2">&quot;num_class&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">pred_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Wide</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span> <span class="o">=</span> <span class="n">pred_dim</span>

    <span class="c1"># Embeddings: val + 1 because 0 is reserved for padding/unseen cateogories.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pred_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># (Sum(Embedding) + bias) is equivalent to (OneHotVector + Linear)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">pred_dim</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.linear.wide.Wide.forward" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">forward</span>


<a href="#pytorch_widedeep.models.tabular.linear.wide.Wide.forward" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Forward pass. Simply connecting the Embedding layer with the ouput
neuron(s)</p>

          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/linear/wide.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pass. Simply connecting the Embedding layer with the ouput</span>
<span class="sd">    neuron(s)&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">TabMlp</span>


<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">TabMlp</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>

  
      <p>Defines a <code>TabMlp</code> model that can be used as the <code>deeptabular</code>
component of a Wide &amp; Deep model or independently by itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features, embedded or not. These are then
passed through a series of dense layers (i.e. a MLP).</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the continuous columns will be embedded using
one of the available methods: <em>'standard'</em>, <em>'periodic'</em>
or <em>'piecewise'</em>. If <code>None</code>, it will default to 'False'.<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This parameter is deprecated and it
 will be removed in future releases. Please, use the
 <code>embed_continuous_method</code> parameter instead.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dim</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Size of the continuous embeddings. If the continuous columns are
embedded, <code>cont_embed_dim</code> must be passed.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                <code>[200, 100]</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the mlp.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>float or List of floats with the dropout between the dense layers.
e.g: <em>[0.5,0.5]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm_last</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_linear_first</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>mlp model that will receive the concatenation of the embeddings and
the continuous columns</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabMlp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabMlp</span><span class="p">(</span><span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/tab_mlp.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabMlp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Mlp</span>
    <span class="n">mlp_input_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_out_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_out_dim</span>
    <span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">mlp_input_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
        <span class="n">mlp_hidden_dims</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">TabMlpDecoder</span>


<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">TabMlpDecoder</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Companion decoder model for the <code>TabMlp</code> model (which can be considered
an encoder itself).</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). The <code>TabMlpDecoder</code> will receive the output from the MLP
and '<em>reconstruct</em>' the embeddings.</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>embed_dim</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Size of the embeddings tensor that needs to be reconstructed.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                <code>[100, 200]</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the mlp.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>float or List of floats with the dropout between the dense layers.
e.g: <em>[0.5,0.5]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm_last</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_linear_first</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder.decoder">decoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>mlp model that will receive the output of the encoder</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabMlpDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabMlpDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/tab_mlp.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabMlpDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
        <span class="n">mlp_hidden_dims</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">],</span>
        <span class="n">mlp_activation</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">TabResnet</span>


<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">TabResnet</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>

  
      <p>Defines a <code>TabResnet</code> model that can be used as the <code>deeptabular</code>
component of a Wide &amp; Deep model or independently by itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features, embedded or not. These are then
passed through a series of Resnet blocks. See
<code>pytorch_widedeep.models.tab_resnet._layers</code> for details on the
structure of each block.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the continuous columns will be embedded using
one of the available methods: <em>'standard'</em>, <em>'periodic'</em>
or <em>'piecewise'</em>. If <code>None</code>, it will default to 'False'.<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This parameter is deprecated and it
 will be removed in future releases. Please, use the
 <code>embed_continuous_method</code> parameter instead.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dim</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Size of the continuous embeddings. If the continuous columns are
embedded, <code>cont_embed_dim</code> must be passed.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>blocks_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                <code>[200, 100, 100]</code>
)
        –
        <div class="doc-md-description">
          <p>List of integers that define the input and output units of each block.
For example: <em>[200, 100, 100]</em> will generate 2 blocks. The first will
receive a tensor of size 200 and output a tensor of size 100, and the
second will receive a tensor of size 100 and output a tensor of size
100. See <code>pytorch_widedeep.models.tab_resnet._layers</code> for
details on the structure of each block.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>blocks_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Block's internal dropout.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>simplify_blocks</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the simplest possible residual blocks (<code>X -&gt; [
[LIN, BN, ACT]  + X ]</code>) will be used instead of a standard one
(<code>X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]</code>).</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If <code>None</code> the  output of the Resnet Blocks will be
connected directly to the output neuron(s).</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm_last</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_linear_first</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>deep dense Resnet model that will receive the concatenation of the
embeddings and the continuous columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.mlp">mlp</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>if <code>mlp_hidden_dims</code> is <code>True</code>, this attribute will be an mlp
model that will receive the results of the concatenation of the
embeddings and the continuous columns -- if present --.</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_deep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabResnet</span><span class="p">(</span><span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_deep</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/resnet/tab_resnet.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabResnet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;blocks&#39; must contain at least two elements, e.g. [256, 128]&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span> <span class="o">=</span> <span class="n">blocks_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dropout</span> <span class="o">=</span> <span class="n">blocks_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span> <span class="o">=</span> <span class="n">simplify_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>

    <span class="c1"># Resnet</span>
    <span class="n">dense_resnet_input_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_out_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_out_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
        <span class="n">dense_resnet_input_dim</span><span class="p">,</span> <span class="n">blocks_dims</span><span class="p">,</span> <span class="n">blocks_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span>
    <span class="p">)</span>

    <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
    <span class="c1"># therefore all related params are optional</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">TabResnetDecoder</span>


<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">TabResnetDecoder</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Companion decoder model for the <code>TabResnet</code> model (which can be
considered an encoder itself)</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). This class will receive the output from the ResNet blocks or the
MLP(if present) and '<em>reconstruct</em>' the embeddings.</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>embed_dim</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Size of the embeddings tensor to be reconstructed.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>blocks_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                <code>[100, 100, 200]</code>
)
        –
        <div class="doc-md-description">
          <p>List of integers that define the input and output units of each block.
For example: <em>[200, 100, 100]</em> will generate 2 blocks. The first will
receive a tensor of size 200 and output a tensor of size 100, and the
second will receive a tensor of size 100 and output a tensor of size
100. See <code>pytorch_widedeep.models.tab_resnet._layers</code> for
details on the structure of each block.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>blocks_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Block's internal dropout.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>simplify_blocks</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the simplest possible residual blocks (<code>X -&gt; [
[LIN, BN, ACT]  + X ]</code>) will be used instead of a standard one
(<code>X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]</code>).</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If <code>None</code> the  output of the Resnet Blocks will be
connected directly to the output neuron(s).</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm_last</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_linear_first</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder.decoder">decoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>deep dense Resnet model that will receive the output of the encoder IF
<code>mlp_hidden_dims</code> is None</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder.mlp">mlp</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>if <code>mlp_hidden_dims</code> is not None, the overall decoder will consist
in an MLP that will receive the output of the encoder followed by the
deep dense Resnet.</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnetDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabResnetDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/resnet/tab_resnet.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabResnetDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;blocks&#39; must contain at least two elements, e.g. [256, 128]&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span> <span class="o">=</span> <span class="n">blocks_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dropout</span> <span class="o">=</span> <span class="n">blocks_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span> <span class="o">=</span> <span class="n">simplify_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">blocks_dims</span><span class="p">,</span>
            <span class="n">blocks_dropout</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
            <span class="n">blocks_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">blocks_dims</span><span class="p">,</span> <span class="n">blocks_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">TabNet</span>


<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">TabNet</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">attn_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">1.3</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">,</span>
    <span class="n">mask_type</span><span class="o">=</span><span class="s2">&quot;sparsemax&quot;</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>

  
      <p>Defines a <a href="https://arxiv.org/abs/1908.07442">TabNet model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>The implementation in this library is fully based on that
<a href="https://github.com/dreamquark-ai/tabnet">here</a> by the dreamquark-ai team,
simply adapted so that it can work within the <code>WideDeep</code> frame.
Therefore, <strong>ALL CREDIT TO THE DREAMQUARK-AI TEAM</strong>.</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the continuous columns will be embedded using
one of the available methods: <em>'standard'</em>, <em>'periodic'</em>
or <em>'piecewise'</em>. If <code>None</code>, it will default to 'False'.<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This parameter is deprecated and it
 will be removed in future releases. Please, use the
 <code>embed_continuous_method</code> parameter instead.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dim</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Size of the continuous embeddings. If the continuous columns are
embedded, <code>cont_embed_dim</code> must be passed.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_steps</code></b>
            (<code>int</code>, default:
                <code>3</code>
)
        –
        <div class="doc-md-description">
          <p>number of decision steps. For a better understanding of the function
of <code>n_steps</code> and the upcoming parameters, please see the
<a href="https://arxiv.org/abs/1908.07442">paper</a>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>step_dim</code></b>
            (<code>int</code>, default:
                <code>8</code>
)
        –
        <div class="doc-md-description">
          <p>Step's output dimension. This is the output dimension that
<code>WideDeep</code> will collect and connect to the output neuron(s).</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dim</code></b>
            (<code>int</code>, default:
                <code>8</code>
)
        –
        <div class="doc-md-description">
          <p>Attention dimension</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>dropout</code></b>
            (<code>float</code>, default:
                <code>0.0</code>
)
        –
        <div class="doc-md-description">
          <p>GLU block's internal dropout</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_glu_step_dependent</code></b>
            (<code>int</code>, default:
                <code>2</code>
)
        –
        <div class="doc-md-description">
          <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that are step dependent</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_glu_shared</code></b>
            (<code>int</code>, default:
                <code>2</code>
)
        –
        <div class="doc-md-description">
          <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that will be shared
across decision steps</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ghost_bn</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if <a href="https://arxiv.org/abs/1705.08741">Ghost Batch Normalization</a>
will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>virtual_batch_size</code></b>
            (<code>int</code>, default:
                <code>128</code>
)
        –
        <div class="doc-md-description">
          <p>Batch size when using Ghost Batch Normalization</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>momentum</code></b>
            (<code>float</code>, default:
                <code>0.02</code>
)
        –
        <div class="doc-md-description">
          <p>Ghost Batch Normalization's momentum. The dreamquark-ai advises for
very low values. However high values are used in the original
publication. During our tests higher values lead to better results</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>gamma</code></b>
            (<code>float</code>, default:
                <code>1.3</code>
)
        –
        <div class="doc-md-description">
          <p>Relaxation parameter in the paper. When gamma = 1, a feature is
enforced to be used only at one decision step. As gamma increases,
more flexibility is provided to use a feature at multiple decision
steps</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>epsilon</code></b>
            (<code>float</code>, default:
                <code>1e-15</code>
)
        –
        <div class="doc-md-description">
          <p>Float to avoid log(0). Always keep low</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mask_type</code></b>
            (<code>str</code>, default:
                <code>&#39;sparsemax&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Mask function to use. Either <em>'sparsemax'</em> or <em>'entmax'</em></p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>the TabNet encoder. For details see the <a href="https://arxiv.org/abs/1908.07442">original publication</a>.</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabNet</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/tabnet/tab_net.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">attn_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.3</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-15</span><span class="p">,</span>
    <span class="n">mask_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sparsemax&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span> <span class="o">=</span> <span class="n">step_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dim</span> <span class="o">=</span> <span class="n">attn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_step_dependent</span> <span class="o">=</span> <span class="n">n_glu_step_dependent</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_shared</span> <span class="o">=</span> <span class="n">n_glu_shared</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ghost_bn</span> <span class="o">=</span> <span class="n">ghost_bn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">virtual_batch_size</span> <span class="o">=</span> <span class="n">virtual_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_type</span> <span class="o">=</span> <span class="n">mask_type</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_out_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_out_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cont_out_dim</span>

    <span class="c1"># TabNet</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TabNetEncoder</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_out_dim</span><span class="p">,</span>
        <span class="n">n_steps</span><span class="p">,</span>
        <span class="n">step_dim</span><span class="p">,</span>
        <span class="n">attn_dim</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">,</span>
        <span class="n">n_glu_step_dependent</span><span class="p">,</span>
        <span class="n">n_glu_shared</span><span class="p">,</span>
        <span class="n">ghost_bn</span><span class="p">,</span>
        <span class="n">virtual_batch_size</span><span class="p">,</span>
        <span class="n">momentum</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">,</span>
        <span class="n">mask_type</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">TabNetDecoder</span>


<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">TabNetDecoder</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Companion decoder model for the <code>TabNet</code> model (which can be
considered an encoder itself)</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). This class will receive the output from the <code>TabNet</code> encoder
(i.e. the output from the so called 'steps') and '<em>reconstruct</em>' the
embeddings.</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>embed_dim</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Size of the embeddings tensor to be reconstructed.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_steps</code></b>
            (<code>int</code>, default:
                <code>3</code>
)
        –
        <div class="doc-md-description">
          <p>number of decision steps. For a better understanding of the function
of <code>n_steps</code> and the upcoming parameters, please see the
<a href="https://arxiv.org/abs/1908.07442">paper</a>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>step_dim</code></b>
            (<code>int</code>, default:
                <code>8</code>
)
        –
        <div class="doc-md-description">
          <p>Step's output dimension. This is the output dimension that
<code>WideDeep</code> will collect and connect to the output neuron(s).</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>dropout</code></b>
            (<code>float</code>, default:
                <code>0.0</code>
)
        –
        <div class="doc-md-description">
          <p>GLU block's internal dropout</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_glu_step_dependent</code></b>
            (<code>int</code>, default:
                <code>2</code>
)
        –
        <div class="doc-md-description">
          <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that are step dependent</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_glu_shared</code></b>
            (<code>int</code>, default:
                <code>2</code>
)
        –
        <div class="doc-md-description">
          <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that will be shared
across decision steps</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ghost_bn</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if <a href="https://arxiv.org/abs/1705.08741">Ghost Batch Normalization</a>
will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>virtual_batch_size</code></b>
            (<code>int</code>, default:
                <code>128</code>
)
        –
        <div class="doc-md-description">
          <p>Batch size when using Ghost Batch Normalization</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>momentum</code></b>
            (<code>float</code>, default:
                <code>0.02</code>
)
        –
        <div class="doc-md-description">
          <p>Ghost Batch Normalization's momentum. The dreamquark-ai advises for
very low values. However high values are used in the original
publication. During our tests higher values lead to better results</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder.decoder">decoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>decoder that will receive the output from the encoder's steps and will
reconstruct the embeddings</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabNetDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabNetDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/tabnet/tab_net.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabNetDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span> <span class="o">=</span> <span class="n">step_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_step_dependent</span> <span class="o">=</span> <span class="n">n_glu_step_dependent</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_shared</span> <span class="o">=</span> <span class="n">n_glu_shared</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ghost_bn</span> <span class="o">=</span> <span class="n">ghost_bn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">virtual_batch_size</span> <span class="o">=</span> <span class="n">virtual_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>

    <span class="n">shared_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_glu_shared</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">transformer</span> <span class="o">=</span> <span class="n">FeatTransformer</span><span class="p">(</span>
            <span class="n">step_dim</span><span class="p">,</span>
            <span class="n">step_dim</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
            <span class="n">shared_layers</span><span class="p">,</span>
            <span class="n">n_glu_step_dependent</span><span class="p">,</span>
            <span class="n">ghost_bn</span><span class="p">,</span>
            <span class="n">virtual_batch_size</span><span class="p">,</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">initialize_non_glu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span><span class="p">,</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">ContextAttentionMLP</span>


<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">ContextAttentionMLP</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="o">=</span><span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines a <code>ContextAttentionMLP</code> model that can be used as the
<code>deeptabular</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features that are also embedded. These
are then passed through a series of attention blocks. Each attention
block is comprised by a <code>ContextAttentionEncoder</code>. Such encoder is in
part inspired by the attention mechanism described in
<a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf">Hierarchical Attention Networks for Document
Classification</a>.
See <code>pytorch_widedeep.models.tabular.mlp._attention_layers</code> for details.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>add_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>frac_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>&#39;standard&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>input_dim</code></b>
            (<code>int</code>, default:
                <code>32</code>
)
        –
        <div class="doc-md-description">
          <p>The so-called <em>dimension of the model</em>. Is the number of embeddings
used to encode the categorical and/or continuous columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.2</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for each attention block</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>with_addnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if residual connections will be used in the
attention blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;leaky_relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>String indicating the activation function to be applied to the dense
layer in each attention encoder. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em>
and <em>'gelu'</em> are supported.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_blocks</code></b>
            (<code>int</code>, default:
                <code>3</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention blocks</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Sequence of attention encoders.</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">ContextAttentionMLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ContextAttentionMLP</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/context_attention_mlp.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ContextAttentionMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_activation</span> <span class="o">=</span> <span class="n">attn_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Attention Blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;attention_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">ContextAttentionEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">with_addnorm</span><span class="p">,</span>
                <span class="n">attn_activation</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.attention_weights" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(F\)</span> is the number of features/columns in the dataset</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">SelfAttentionMLP</span>


<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">SelfAttentionMLP</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="o">=</span><span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines a <code>SelfAttentionMLP</code> model that can be used as the
deeptabular component of a Wide &amp; Deep model or independently by
itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features that are also embedded. These
are then passed through a series of attention blocks. Each attention
block is comprised by what we would refer as a simplified
<code>SelfAttentionEncoder</code>. See
<code>pytorch_widedeep.models.tabular.mlp._attention_layers</code> for details. The
reason to use a simplified version of self attention is because we
observed that the '<em>standard</em>' attention mechanism used in the
TabTransformer has a notable tendency to overfit.</p>
<p>In more detail, this model only uses Q and K (and not V). If we think
about it as in terms of text (and intuitively), the Softmax(QK^T) is the
attention mechanism that tells us how much, at each position in the input
sentence, each word is represented or 'expressed'. We refer to that
as 'attention weights'. These attention weighst are normally multiplied
by a Value matrix to further strength the focus on the words that each
word should be attending to (again, intuitively).</p>
<p>In this implementation we skip this last multiplication and instead we
multiply the attention weights directly by the input tensor. This is a
simplification that we expect is beneficial in terms of avoiding
overfitting for tabular data.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>add_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>frac_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>&#39;standard&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>input_dim</code></b>
            (<code>int</code>, default:
                <code>32</code>
)
        –
        <div class="doc-md-description">
          <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.2</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for each attention block</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_heads</code></b>
            (<code>int</code>, default:
                <code>8</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention heads per attention block.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_bias</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to use bias in the Q, K projection
layers.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>with_addnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if residual connections will be used in the attention blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;leaky_relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>String indicating the activation function to be applied to the dense
layer in each attention encoder. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em>
and <em>'gelu'</em> are supported.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_blocks</code></b>
            (<code>int</code>, default:
                <code>3</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention blocks</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.cat_and_cont_embed">cat_and_cont_embed</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>This is the module that processes the categorical and continuous columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Sequence of attention encoders.</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">SelfAttentionMLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SelfAttentionMLP</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/self_attention_mlp.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttentionMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_activation</span> <span class="o">=</span> <span class="n">attn_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Attention Blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;attention_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">SelfAttentionEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">with_addnorm</span><span class="p">,</span>
                <span class="n">attn_activation</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the WideDeep class</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.attention_weights" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">TabTransformer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">TabTransformer</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">use_linear_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_flash_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines our adptation of the
<a href="https://arxiv.org/abs/2012.06678">TabTransformer model</a>
that can be used as the <code>deeptabular</code> component of a
Wide &amp; Deep model or independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>:
This is an enhanced adaptation of the model described in the paper. It can
be considered as the flagship of our transformer family of models for
tabular data and offers mutiple, additional features relative to the
original publication(and some other models in the library)</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>add_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>frac_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>input_dim</code></b>
            (<code>int</code>, default:
                <code>32</code>
)
        –
        <div class="doc-md-description">
          <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_heads</code></b>
            (<code>int</code>, default:
                <code>8</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention heads per Transformer block</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_qkv_bias</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_blocks</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Number of Transformer blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.2</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the Multi-Head Attention layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the FeedForward network</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_factor</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>transformer_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;gelu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_linear_attention</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if Linear Attention (from <a href="https://arxiv.org/abs/2006.16236">Transformers are RNNs:
Fast Autoregressive Transformers with Linear Attention</a>)
will be used. The inclusing of this mode of attention is inspired by
<a href="https://www.uber.com/en-GB/blog/deepeta-how-uber-predicts-arrival-times/">this post</a>,
where the Uber team finds that this attention mechanism leads to the
best results for their tabular data.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_flash_attention</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if
<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">Flash Attention</a>
will be used. <br/></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm_last</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_linear_first</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Sequence of Transformer blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.mlp">mlp</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>MLP component in the model</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabTransformer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_transformer.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">use_linear_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_flash_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_linear_attention</span> <span class="o">=</span> <span class="n">use_linear_attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attention</span> <span class="o">=</span> <span class="n">use_flash_attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;If only continuous features are used &#39;embed_continuous&#39; must be set to &#39;True&#39;&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;transformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">TransformerEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">use_qkv_bias</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">ff_factor</span><span class="p">,</span>
                <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="n">use_linear_attention</span><span class="p">,</span>
                <span class="n">use_flash_attention</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_first_hidden_dim</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.attention_weights" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>:
if flash attention or linear attention
are used, no attention weights are saved during the training process
and calling this property will throw a ValueError</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">SAINT</span>


<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">SAINT</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines a <a href="https://arxiv.org/abs/2106.01342">SAINT model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This is an slightly modified and enhanced
 version of the model described in the paper,</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>add_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>frac_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>&#39;standard&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>input_dim</code></b>
            (<code>int</code>, default:
                <code>32</code>
)
        –
        <div class="doc-md-description">
          <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_heads</code></b>
            (<code>int</code>, default:
                <code>8</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention heads per Transformer block</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_qkv_bias</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_blocks</code></b>
            (<code>int</code>, default:
                <code>2</code>
)
        –
        <div class="doc-md-description">
          <p>Number of SAINT-Transformer blocks.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the Multi-Head Attention column and
row layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_dropout</code></b>
            (<code>float</code>, default:
                <code>0.2</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the FeedForward network</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_factor</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>transformer_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;gelu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm_last</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_linear_first</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.saint.SAINT.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Sequence of SAINT-Transformer blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.saint.SAINT.mlp">mlp</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>MLP component in the model</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">SAINT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SAINT</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/saint.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SAINT</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;saint_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">SaintEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">use_qkv_bias</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">ff_factor</span><span class="p">,</span>
                <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
    <span class="c1"># therefore all related params are optional</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT.attention_weights" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights. Each element of the list is a tuple
where the first and the second elements are the column and row
attention weights respectively</p>
<p>The shape of the attention weights is:</p>
<ul>
<li>
<p>column attention: <span class="arithmatex">\((N, H, F, F)\)</span></p>
</li>
<li>
<p>row attention: <span class="arithmatex">\((1, H, N, N)\)</span></p>
</li>
</ul>
<p>where <span class="arithmatex">\(N\)</span> is the batch size, <span class="arithmatex">\(H\)</span> is the number of heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">FTTransformer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">FTTransformer</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">kv_compression_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">kv_sharing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mf">1.33</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;reglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines a <a href="https://arxiv.org/abs/2106.11959">FTTransformer model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>add_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>frac_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>&#39;standard&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>input_dim</code></b>
            (<code>int</code>, default:
                <code>64</code>
)
        –
        <div class="doc-md-description">
          <p>The so-called <em>dimension of the model</em>. Is the number of embeddings used to encode
the categorical and/or continuous columns.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>kv_compression_factor</code></b>
            (<code>float</code>, default:
                <code>0.5</code>
)
        –
        <div class="doc-md-description">
          <p>By default, the FTTransformer uses Linear Attention
(See <a href="https://arxiv.org/abs/2006.04768&gt;">Linformer: Self-Attention with Linear Complexity</a> ).
The compression factor that will be used to reduce the input sequence
length. If we denote the resulting sequence length as
<span class="arithmatex">\(k = int(kv_{compression \space factor} \times s)\)</span>
where <span class="arithmatex">\(s\)</span> is the input sequence length.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>kv_sharing</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the <span class="arithmatex">\(E\)</span> and <span class="arithmatex">\(F\)</span> projection matrices
will share weights.  See <a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear
Complexity</a> for details</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_heads</code></b>
            (<code>int</code>, default:
                <code>8</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention heads per FTTransformer block</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_qkv_bias</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_blocks</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Number of FTTransformer blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.2</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the Linear-Attention layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the FeedForward network</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_factor</code></b>
            (<code>float</code>, default:
                <code>1.33</code>
)
        –
        <div class="doc-md-description">
          <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4, but they use 4/3
in the paper.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>transformer_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;reglu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
FTTransformer block will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm_last</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_linear_first</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Sequence of FTTransformer blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.mlp">mlp</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>MLP component in the model</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">FTTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FTTransformer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/ft_transformer.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">kv_compression_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">kv_sharing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.33</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FTTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">kv_compression_factor</span> <span class="o">=</span> <span class="n">kv_compression_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kv_sharing</span> <span class="o">=</span> <span class="n">kv_sharing</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="n">is_first</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;fttransformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">FTTransformerEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">use_qkv_bias</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">ff_factor</span><span class="p">,</span>
                <span class="n">kv_compression_factor</span><span class="p">,</span>
                <span class="n">kv_sharing</span><span class="p">,</span>
                <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="n">is_first</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">is_first</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
    <span class="c1"># therefore all related params are optional</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.attention_weights" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is: <span class="arithmatex">\((N, H, F, k)\)</span>, where <span class="arithmatex">\(N\)</span> is
the batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads, <span class="arithmatex">\(F\)</span> is the
number of features/columns and <span class="arithmatex">\(k\)</span> is the reduced sequence length or
dimension, i.e. <span class="arithmatex">\(k = int(kv_{compression \space factor} \times s)\)</span></p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">TabPerceiver</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">TabPerceiver</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">n_cross_attns</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_cross_attn_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latents</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">n_latent_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latent_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_perceiver_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;geglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines an adaptation of a <a href="https://arxiv.org/abs/2103.03206">Perceiver</a>
 that can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model
 or independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: while there are scientific publications for
 the <code>TabTransformer</code>, <code>SAINT</code> and <code>FTTransformer</code>, the <code>TabPerceiver</code>
 and the <code>TabFastFormer</code> are our own adaptations of the
 <a href="https://arxiv.org/abs/2103.03206">Perceiver</a> and the
 <a href="https://arxiv.org/abs/2108.09084">FastFormer</a> for tabular data.</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>add_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>frac_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>&#39;standard&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>input_dim</code></b>
            (<code>int</code>, default:
                <code>32</code>
)
        –
        <div class="doc-md-description">
          <p>The so-called <em>dimension of the model</em>. Is the number of embeddings
used to encode the categorical and/or continuous columns.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_cross_attns</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Number of times each perceiver block will cross attend to the input
data (i.e. number of cross attention components per perceiver block).
This should normally be 1. However, in the paper they describe some
architectures (normally computer vision-related problems) where the
Perceiver attends multiple times to the input array. Therefore, maybe
multiple cross attention to the input array is also useful in some
cases for tabular data <img alt="🤷" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f937.png" title=":shrug:" /> .</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_cross_attn_heads</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention heads for the cross attention component</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_latents</code></b>
            (<code>int</code>, default:
                <code>16</code>
)
        –
        <div class="doc-md-description">
          <p>Number of latents. This is the <span class="arithmatex">\(N\)</span> parameter in the paper. As
indicated in the paper, this number should be significantly lower
than <span class="arithmatex">\(M\)</span> (the number of columns in the dataset). Setting <span class="arithmatex">\(N\)</span> closer
to <span class="arithmatex">\(M\)</span> defies the main purpose of the Perceiver, which is to overcome
the transformer quadratic bottleneck</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>latent_dim</code></b>
            (<code>int</code>, default:
                <code>128</code>
)
        –
        <div class="doc-md-description">
          <p>Latent dimension.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_latent_heads</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention heads per Latent Transformer</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_latent_blocks</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Number of transformer encoder blocks (normalised MHA + normalised FF)
per Latent Transformer</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_perceiver_blocks</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Number of Perceiver blocks defined as [Cross Attention + Latent
Transformer]</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_weights</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the weights will be shared between Perceiver
blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the Multi-Head Attention layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the FeedForward network</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_factor</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>transformer_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;geglu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm_last</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_linear_first</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.ModuleDict">ModuleDict</span></code>)
        –
        <div class="doc-md-description">
          <p>ModuleDict with the Perceiver blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.latents">latents</span></code></b>
            (<code><span title="torch.nn.Parameter">Parameter</span></code>)
        –
        <div class="doc-md-description">
          <p>Latents that will be used for prediction</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.mlp">mlp</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>MLP component in the model</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabPerceiver</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabPerceiver</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span> <span class="n">n_latents</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span><span class="n">n_perceiver_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_perceiver.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">n_cross_attns</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n_cross_attn_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latents</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">n_latent_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latent_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_perceiver_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;geglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabPerceiver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attns</span> <span class="o">=</span> <span class="n">n_cross_attns</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attn_heads</span> <span class="o">=</span> <span class="n">n_cross_attn_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_latents</span> <span class="o">=</span> <span class="n">n_latents</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="n">latent_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_latent_heads</span> <span class="o">=</span> <span class="n">n_latent_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_latent_blocks</span> <span class="o">=</span> <span class="n">n_latent_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_perceiver_blocks</span> <span class="o">=</span> <span class="n">n_perceiver_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span> <span class="o">=</span> <span class="n">share_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">latents</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">trunc_normal_</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_latents</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
    <span class="n">first_perceiver_block</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_perceiver_block</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block0&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">first_perceiver_block</span>

    <span class="k">if</span> <span class="n">share_weights</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_perceiver_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="n">first_perceiver_block</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_perceiver_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_perceiver_block</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span>

    <span class="c1"># Mlp</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.attention_weights" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights. If the weights are not shared
between perceiver blocks each element of the list will be a list
itself containing the Cross Attention and Latent Transformer
attention weights respectively</p>
<p>The shape of the attention weights is:</p>
<ul>
<li>
<p>Cross Attention: <span class="arithmatex">\((N, C, L, F)\)</span></p>
</li>
<li>
<p>Latent Attention: <span class="arithmatex">\((N, T, L, L)\)</span></p>
</li>
</ul>
<p>WHere <span class="arithmatex">\(N\)</span> is the batch size, <span class="arithmatex">\(C\)</span> is the number of Cross Attention
heads, <span class="arithmatex">\(L\)</span> is the number of Latents, <span class="arithmatex">\(F\)</span> is the number of
features/columns in the dataset and <span class="arithmatex">\(T\)</span> is the number of Latent
Attention heads</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">TabFastFormer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">TabFastFormer</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="o">=</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">share_qv_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines an adaptation of a <a href="https://arxiv.org/abs/2108.09084">FastFormer</a>
that can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model
or independently by itself.</p>
<p>Most of the parameters for this class are <code>Optional</code> since the use of
categorical or continuous is in fact optional (i.e. one can use
categorical features only, continuous features only or both).</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: while there are scientific publications for
 the <code>TabTransformer</code>, <code>SAINT</code> and <code>FTTransformer</code>, the <code>TabPerceiver</code>
 and the <code>TabFastFormer</code> are our own adaptations of the
 <a href="https://arxiv.org/abs/2103.03206">Perceiver</a> and the
 <a href="https://arxiv.org/abs/2108.09084">FastFormer</a> for tabular data.</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>column_idx</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        –
        <div class="doc-md-description">
          <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_input</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Categorical embeddings dropout. If <code>None</code>, it will default
to 0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cat_bias</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if bias will be used for the categorical embeddings.
If <code>None</code>, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cat_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the embeddings will be "shared". The idea behind <code>shared_embed</code> is
described in the Appendix A in the <a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>:
<em>'The goal of having column embedding is to enable the model to
distinguish the classes in one column from those in the other
columns'</em>. In other words, the idea is to let the model learn which
column is embedded at the time. See: <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>add_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code>
If 'None' is passed, it will default to 'False'.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>frac_shared_embed</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column. If 'None' is passed, it will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>continuous_cols</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the name of the numeric (aka continuous) columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_norm_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[batchnorm, layernorm]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Type of normalization layer applied to the continuous features.
Options are: <em>'layernorm'</em> and <em>'batchnorm'</em>. if <code>None</code>, no
normalization layer will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_continuous_method</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[standard, piecewise, periodic]]</code>, default:
                <code>&#39;standard&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Method to use to embed the continuous features. Options are:
<em>'standard'</em>, <em>'periodic'</em> or <em>'piecewise'</em>. The <em>'standard'</em>
embedding method is based on the FT-Transformer implementation
presented in the paper: <a href="https://arxiv.org/abs/2106.11959v5">Revisiting Deep Learning Models for
Tabular Data</a>. The <em>'periodic'</em>
and_'piecewise'_ methods were presented in the paper: <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>.
Please, read the papers for details.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for the continuous embeddings. If <code>None</code>, it will default to 0.0</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>cont_embed_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.
If <code>None</code>, no activation function will be applied.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>quantization_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is used when the <em>'piecewise'</em> method is used to embed
the continuous cols. It is a dict where keys are the name of the continuous
columns and values are lists with the boundaries for the quantization
of the continuous_cols. See the examples for details. If
If the <em>'piecewise'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_frequencies</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the so called <em>'k'</em> in their paper <a href="https://arxiv.org/abs/2203.05556">On Embeddings for
Numerical Features in Tabular Deep Learning</a>,
and is the number of 'frequencies' that will be used to represent each
continuous column. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This is the sigma parameter in the paper mentioned when describing the
previous parameters and it is used to initialise the 'frequency
weights'. See their Eq 2 in the paper for details. If
the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_last_layer</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This parameter is not present in the before mentioned paper but it is implemented in
the <a href="https://github.com/yandex-research/rtdl-num-embeddings/tree/main">official repo</a>.
If <code>True</code> the linear layer that turns the frequencies into embeddings
will be shared across the continuous columns. If <code>False</code> a different
linear layer will be used for each continuous column.
If the <em>'periodic'</em> method is used, this parameter is required.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>full_embed_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>If <code>True</code>, the full embedding corresponding to a column will be masked
out/dropout. If <code>None</code>, it will default to <code>False</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>input_dim</code></b>
            (<code>int</code>, default:
                <code>32</code>
)
        –
        <div class="doc-md-description">
          <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_heads</code></b>
            (<code>int</code>, default:
                <code>8</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention heads per FastFormer block</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_bias</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_blocks</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Number of FastFormer blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the Additive Attention layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_dropout</code></b>
            (<code>float</code>, default:
                <code>0.2</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the FeedForward network</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_factor</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_qv_weights</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Following the paper, this is a boolean indicating if the Value (<span class="arithmatex">\(V\)</span>) and
the Query (<span class="arithmatex">\(Q\)</span>) transformation parameters will be shared.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>share_weights</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>In addition to sharing the <span class="arithmatex">\(V\)</span> and <span class="arithmatex">\(Q\)</span> transformation parameters, the
parameters across different Fastformer layers can also be shared.
Please, see
<code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code> for
details</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>transformer_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>MLP hidden dimensions. If not provided no MLP on top of the final
FTTransformer block will be used</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If not provided no MLP on top of the final
Transformer block will be used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_activation</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu' and _'gelu'</em> are supported.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <em>'relu'</em>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>float with the dropout between the dense layers of the MLP.
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to 0.0.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_batchnorm_last</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to False.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>mlp_linear_first</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[bool]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code>
If 'mlp_hidden_dims' is not <code>None</code> and this parameter is <code>None</code>, it
will default to <code>True</code>.</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Sequence of FasFormer blocks.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.mlp">mlp</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>MLP component in the model</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabFastFormer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabFastFormer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;piecewise&quot;</span><span class="p">,</span> <span class="s2">&quot;periodic&quot;</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;standard&quot;</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantization_setup</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_frequencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">share_last_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">share_qv_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabFastFormer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_continuous_method</span><span class="o">=</span><span class="n">embed_continuous_method</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">quantization_setup</span><span class="o">=</span><span class="n">quantization_setup</span><span class="p">,</span>
        <span class="n">n_frequencies</span><span class="o">=</span><span class="n">n_frequencies</span><span class="p">,</span>
        <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
        <span class="n">share_last_layer</span><span class="o">=</span><span class="n">share_last_layer</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">share_qv_weights</span> <span class="o">=</span> <span class="n">share_qv_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span> <span class="o">=</span> <span class="n">share_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">first_fastformer_block</span> <span class="o">=</span> <span class="n">FastFormerEncoder</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">,</span>
        <span class="n">ff_dropout</span><span class="p">,</span>
        <span class="n">ff_factor</span><span class="p">,</span>
        <span class="n">share_qv_weights</span><span class="p">,</span>
        <span class="n">transformer_activation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;fastformer_block0&quot;</span><span class="p">,</span> <span class="n">first_fastformer_block</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">share_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;fastformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">first_fastformer_block</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;fastformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">FastFormerEncoder</span><span class="p">(</span>
                    <span class="n">input_dim</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">,</span>
                    <span class="n">use_bias</span><span class="p">,</span>
                    <span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="n">ff_dropout</span><span class="p">,</span>
                    <span class="n">ff_factor</span><span class="p">,</span>
                    <span class="n">share_qv_weights</span><span class="p">,</span>
                    <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Mlp: adding an MLP on top of the Resnet blocks is optional and</span>
    <span class="c1"># therefore all related params are optional</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">d_hidden</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="p">(</span>
                <span class="s2">&quot;relu&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span>
            <span class="p">),</span>
            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">batchnorm</span><span class="o">=</span><span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">batchnorm_last</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span>
            <span class="p">),</span>
            <span class="n">linear_first</span><span class="o">=</span><span class="p">(</span>
                <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.attention_weights" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights. Each element of the list is a
tuple where the first and second elements are the <span class="arithmatex">\(\alpha\)</span>
and <span class="arithmatex">\(\beta\)</span> attention weights in the paper.</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F)\)</span> where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">BasicRNN</span>


<a href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">BasicRNN</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>

  
      <p>Standard text classifier/regressor comprised by a stack of RNNs
(LSTMs or GRUs) that can be used as the <code>deeptext</code> component of a Wide &amp;
Deep model or independently by itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of
dense layers on top of the stack of RNNs</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>vocab_size</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Number of words in the vocabulary</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_dim</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_matrix</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Pretrained word embeddings</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_trainable</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the pretrained embeddings are trainable</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>rnn_type</code></b>
            (<code>str</code>, default:
                <code>&#39;lstm&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>String indicating the type of RNN to use. One of <em>'lstm'</em> or <em>'gru'</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>hidden_dim</code></b>
            (<code>int</code>, default:
                <code>64</code>
)
        –
        <div class="doc-md-description">
          <p>Hidden dim of the RNN</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_layers</code></b>
            (<code>int</code>, default:
                <code>3</code>
)
        –
        <div class="doc-md-description">
          <p>Number of recurrent layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>rnn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for each RNN layer except the last layer</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>bidirectional</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether the staked RNNs are bidirectional</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_hidden_state</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether to use the final hidden state or the RNN's
output as predicting features. Typically the former is used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>padding_idx</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's tokenizer
where the token index 0 is reserved for the <em>'unknown'</em> word token.
Therefore, the default value is set to 1.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm_last</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_linear_first</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.word_embed">word_embed</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>word embedding matrix</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.rnn">rnn</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Stack of RNNs</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.rnn_mlp">rnn_mlp</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not None</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">BasicRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/text/rnns/basic_rnn.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BasicRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">embed_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;If no &#39;embed_matrix&#39; is passed, the embedding dimension must&quot;</span>
            <span class="s2">&quot;be specified with &#39;embed_dim&#39;&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;the input embedding dimension </span><span class="si">{}</span><span class="s2"> and the dimension of the &quot;</span>
            <span class="s2">&quot;pretrained embeddings </span><span class="si">{}</span><span class="s2"> do not match. The pretrained embeddings &quot;</span>
            <span class="s2">&quot;dimension (</span><span class="si">{}</span><span class="s2">) will be used&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">),</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span> <span class="o">=</span> <span class="n">embed_trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_dropout</span> <span class="o">=</span> <span class="n">rnn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_hidden_state</span> <span class="o">=</span> <span class="n">use_hidden_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

    <span class="c1"># Embeddings</span>
    <span class="k">if</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_embeddings</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
        <span class="p">)</span>

    <span class="c1"># RNN</span>
    <span class="n">rnn_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="s2">&quot;num_layers&quot;</span><span class="p">:</span> <span class="n">n_layers</span><span class="p">,</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">,</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">rnn_dropout</span><span class="p">,</span>
        <span class="s2">&quot;batch_first&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bidirectional</span> <span class="k">else</span> <span class="n">hidden_dim</span>

    <span class="c1"># FC-Head (Mlp)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MLP</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># simple hack to add readability in the forward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">AttentiveRNN</span>


<a href="#pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">AttentiveRNN</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN" href="#pytorch_widedeep.models.text.rnns.basic_rnn.BasicRNN">BasicRNN</a></code></p>

  
      <p>Text classifier/regressor comprised by a stack of RNNs
(LSTMs or GRUs) plus an attention layer. This model can be used as the
<code>deeptext</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of dense
layers on top of attention layer</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>vocab_size</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Number of words in the vocabulary</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_dim</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_matrix</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Pretrained word embeddings</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_trainable</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the pretrained embeddings are trainable</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>rnn_type</code></b>
            (<code>str</code>, default:
                <code>&#39;lstm&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>String indicating the type of RNN to use. One of <em>'lstm'</em> or <em>'gru'</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>hidden_dim</code></b>
            (<code>int</code>, default:
                <code>64</code>
)
        –
        <div class="doc-md-description">
          <p>Hidden dim of the RNN</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_layers</code></b>
            (<code>int</code>, default:
                <code>3</code>
)
        –
        <div class="doc-md-description">
          <p>Number of recurrent layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>rnn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout for each RNN layer except the last layer</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>bidirectional</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether the staked RNNs are bidirectional</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_hidden_state</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether to use the final hidden state or the RNN's
output as predicting features. Typically the former is used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>padding_idx</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's
tokenizer where the token index 0 is reserved for the <em>'unknown'</em>
word token. Therefore, the default value is set to 1.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_concatenate</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the input to the attention mechanism will be the
output of the RNN or the output of the RNN concatenated with the last
hidden state.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Internal dropout for the attention mechanism</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm_last</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_linear_first</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.word_embed">word_embed</span></code></b>
            (<code><span title="nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>word embedding matrix</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.rnn">rnn</span></code></b>
            (<code><span title="nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Stack of RNNs</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.rnn_mlp">rnn_mlp</span></code></b>
            (<code><span title="nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not <code>None</code></p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">AttentiveRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AttentiveRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/text/rnns/attentive_rnn.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">embed_matrix</span><span class="o">=</span><span class="n">embed_matrix</span><span class="p">,</span>
        <span class="n">embed_trainable</span><span class="o">=</span><span class="n">embed_trainable</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
        <span class="n">rnn_dropout</span><span class="o">=</span><span class="n">rnn_dropout</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">use_hidden_state</span><span class="o">=</span><span class="n">use_hidden_state</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="o">=</span><span class="n">head_hidden_dims</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="o">=</span><span class="n">head_activation</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="o">=</span><span class="n">head_dropout</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="o">=</span><span class="n">head_batchnorm</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="n">head_batchnorm_last</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="o">=</span><span class="n">head_linear_first</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Embeddings and RNN defined in the BasicRNN inherited class</span>

    <span class="c1"># Attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_concatenate</span> <span class="o">=</span> <span class="n">attn_concatenate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>

    <span class="k">if</span> <span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="k">elif</span> <span class="n">bidirectional</span> <span class="ow">or</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">ContextAttention</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="p">,</span> <span class="n">sum_along_seq</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># FC-Head (Mlp)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.attention_weights" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.attentive_rnn.AttentiveRNN.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, S)\)</span>, where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(S\)</span> is the length of the sequence</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">StackedAttentiveRNN</span>


<a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">StackedAttentiveRNN</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>

  
      <p>Text classifier/regressor comprised by a stack of blocks:
<code>[RNN + Attention]</code>. This can be used as the <code>deeptext</code> component of a
Wide &amp; Deep model or independently by itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of
dense layers on top of the attentiob blocks</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>vocab_size</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Number of words in the vocabulary</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_dim</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_matrix</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Pretrained word embeddings</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>embed_trainable</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the pretrained embeddings are trainable</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>rnn_type</code></b>
            (<code>str</code>, default:
                <code>&#39;lstm&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>String indicating the type of RNN to use. One of 'lstm' or 'gru'</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>hidden_dim</code></b>
            (<code>int</code>, default:
                <code>64</code>
)
        –
        <div class="doc-md-description">
          <p>Hidden dim of the RNN</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>bidirectional</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether the staked RNNs are bidirectional</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>padding_idx</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's
tokenizer where the token index 0 is reserved for the <em>'unknown'</em>
word token. Therefore, the default value is set to 1.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_blocks</code></b>
            (<code>int</code>, default:
                <code>3</code>
)
        –
        <div class="doc-md-description">
          <p>Number of attention blocks. Each block is comprised by an RNN and a
Context Attention Encoder</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_concatenate</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the input to the attention mechanism will be the
output of the RNN or the output of the RNN concatenated with the last
hidden state or simply</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Internal dropout for the attention mechanism</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>with_addnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the output of each block will be added to the
input and normalised</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the sizes of the dense layers in the head e.g: [128, 64]</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm_last</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_linear_first</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.word_embed">word_embed</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>word embedding matrix</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.rnn">rnn</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Stack of RNNs</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.rnn_mlp">rnn_mlp</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not <code>None</code></p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">StackedAttentiveRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">StackedAttentiveRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/text/rnns/stacked_attentive_rnn.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">StackedAttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;the input embedding dimension </span><span class="si">{}</span><span class="s2"> and the dimension of the &quot;</span>
            <span class="s2">&quot;pretrained embeddings </span><span class="si">{}</span><span class="s2"> do not match. The pretrained embeddings &quot;</span>
            <span class="s2">&quot;dimension (</span><span class="si">{}</span><span class="s2">) will be used&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">),</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span> <span class="o">=</span> <span class="n">embed_trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_concatenate</span> <span class="o">=</span> <span class="n">attn_concatenate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

    <span class="c1"># Embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_embeddings</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">)</span>

    <span class="c1"># Linear Projection: if embed_dim is different that the input of the</span>
    <span class="c1"># attention blocks we add a linear projection</span>
    <span class="k">if</span> <span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="k">elif</span> <span class="n">bidirectional</span> <span class="ow">or</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_proj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="c1"># RNN</span>
    <span class="n">rnn_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">,</span>
        <span class="s2">&quot;batch_first&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>

    <span class="c1"># FC-Head (Mlp)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ContextAttentionEncoder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">attn_concatenate</span><span class="p">,</span>
                <span class="n">with_addnorm</span><span class="o">=</span><span class="n">with_addnorm</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">sum_along_seq</span><span class="o">=</span><span class="n">i</span> <span class="o">==</span> <span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Mlp</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MLP</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># simple hack to add readability in the forward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.attention_weights" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.rnns.stacked_attentive_rnn.StackedAttentiveRNN.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weights</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, S)\)</span> Where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(S\)</span> is the length of the sequence</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.miscellaneous.basic_transformer.Transformer" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">Transformer</span>


<a href="#pytorch_widedeep.models.text.miscellaneous.basic_transformer.Transformer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">Transformer</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">seq_length</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">use_linear_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_flash_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">with_cls_token</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">with_pos_encoding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">pos_encoding_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">pos_encoder</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Basic Encoder-Only Transformer Model for text classification/regression.
As all other models in the library this model can be used as the
<code>deeptext</code> component of a Wide &amp; Deep model or independently by itself.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>:
This model is introduced in the context of recommendation systems and
thought for sequences of any nature (e.g. items). It can, of course,
still be used for text. However, at this stage, we have decided to not
include the possibility of loading pretrained word vectors since we aim
to integrate the library wit Huggingface in the (hopefully) near future</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>vocab_size</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Number of words in the vocabulary</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>input_dim</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Dimension of the token embeddings</p>
<p>Param aliases: <code>embed_dim</code>, <code>d_model</code>. <br/></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>seq_length</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Input sequence length</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_heads</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Number of attention heads per Transformer block</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_blocks</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>Number of Transformer blocks</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>attn_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the Multi-Head Attention layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout that will be applied to the FeedForward network</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ff_factor</code></b>
            (<code>int</code>, default:
                <code>4</code>
)
        –
        <div class="doc-md-description">
          <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>activation</code></b>
            (<code>str</code>, default:
                <code>&#39;gelu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>padding_idx</code></b>
            (<code>int</code>, default:
                <code>0</code>
)
        –
        <div class="doc-md-description">
          <p>index of the padding token in the padded-tokenised sequences.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>with_cls_token</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if a <code>'[CLS]'</code> token is included in the tokenized
sequences. If present, the final hidden state corresponding to this
token is used as the aggregated representation for classification and
regression tasks. <strong>NOTE</strong>: if included in the tokenized sequences it
must be inserted as the first token in the sequences.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>with_pos_encoding</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if positional encoding will be used</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>pos_encoding_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Positional encoding dropout</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>pos_encoder</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="torch.nn.Module">Module</span>]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>This model uses by default a standard positional encoding approach.
However, any custom positional encoder can also be used and pass to
the Transformer model via the 'pos_encoder' parameter</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.miscellaneous.basic_transformer.Transformer.embedding">embedding</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Standard token embedding layer</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.miscellaneous.basic_transformer.Transformer.pos_encoder">pos_encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Positional Encoder</p>
        </div>
      </li>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.miscellaneous.basic_transformer.Transformer.encoder">encoder</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Sequence of Transformer blocks</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Transformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_blocks</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/text/miscellaneous/basic_transformer.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;input_dim&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;embed_dim&quot;</span><span class="p">,</span> <span class="s2">&quot;d_model&quot;</span><span class="p">])</span>
<span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;seq_length&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="s2">&quot;maxlen&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">use_linear_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_flash_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">with_cls_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>  <span class="c1"># from here on pos encoding args</span>
    <span class="n">with_pos_encoding</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">pos_encoding_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">pos_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_linear_attention</span> <span class="o">=</span> <span class="n">use_linear_attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attention</span> <span class="o">=</span> <span class="n">use_flash_attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="n">with_cls_token</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_pos_encoding</span> <span class="o">=</span> <span class="n">with_pos_encoding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding_dropout</span> <span class="o">=</span> <span class="n">pos_encoding_dropout</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">with_pos_encoding</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pos_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span> <span class="n">PositionalEncoding</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">pos_encoder</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span> <span class="n">pos_encoding_dropout</span><span class="p">,</span> <span class="n">seq_length</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;transformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">TransformerEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>  <span class="c1"># use_qkv_bias</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">ff_factor</span><span class="p">,</span>
                <span class="n">activation</span><span class="p">,</span>
                <span class="n">use_linear_attention</span><span class="p">,</span>
                <span class="n">use_flash_attention</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">HFModel</span>


<a href="#pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">HFModel</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">use_cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">trainable_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>

  
      <p>This class is a wrapper around the Hugging Face transformers library. It
can be used as the text component of a Wide &amp; Deep model or independently
by itself.</p>
<p>At the moment only models from the families BERT, RoBERTa, DistilBERT,
ALBERT and ELECTRA are supported. This is because this library is
designed to address classification and regression tasks and these are the
most 'popular' encoder-only models, which have proved to be those that
work best for these tasks.</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>model_name</code></b>
            (<code>str</code>)
        –
        <div class="doc-md-description">
          <p>The model name from the transformers library e.g. 'bert-base-uncased'.
Currently supported models are those from the families: BERT, RoBERTa,
DistilBERT, ALBERT and ELECTRA.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>use_cls_token</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether to use the [CLS] token or the mean of the
sequence of hidden states as the sentence embedding</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>trainable_parameters</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the names of the model parameters that will be trained. If
None, none of the parameters will be trainable</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to include batch normalization in the
dense layers that form the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm_last</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_linear_first</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>verbose</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>If True, it will print information about the model</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>**kwargs</code></b>
        –
        <div class="doc-md-description">
          <p>Additional kwargs to be passed to the model</p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel.head">head</span></code></b>
            (<code><span title="nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>Stack of dense layers on top of the transformer. This will only exists
if <code>head_layers_dim</code> is not None</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">HFModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">HFModel</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/text/huggingface_transformers/hf_model.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;use_cls_token&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;use_special_token&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">use_cls_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">trainable_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># TO DO: add warning regarging ELECTRA as ELECTRA does not have a cls</span>
    <span class="c1"># token.  Research what happens with ELECTRA</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_cls_token</span> <span class="o">=</span> <span class="n">use_cls_token</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span> <span class="o">=</span> <span class="n">trainable_parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cls_token</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;The model will use the [CLS] token. Make sure the tokenizer &quot;</span>
            <span class="s2">&quot;was run with add_special_tokens=True&quot;</span><span class="p">,</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model_class</span> <span class="o">=</span> <span class="n">get_model_class</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">get_config_and_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">output_attention_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="nb">any</span><span class="p">([</span><span class="n">tl</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">tl</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span><span class="p">])</span>

    <span class="c1"># FC-Head (Mlp). Note that the FC head will always be trainable</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel.attention_weight" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">attention_weight</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.huggingface_transformers.hf_model.HFModel.attention_weight" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">attention_weight</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Returns the attention weights if the model was created with the
output_attention_weights=True argument. If not, it will raise an
AttributeError.</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
sequence length.</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.image.vision.Vision" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">Vision</span>


<a href="#pytorch_widedeep.models.image.vision.Vision" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">Vision</span><span class="p">(</span>
    <span class="n">pretrained_model_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">channel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span></code></p>

  
      <p>Defines a standard image classifier/regressor using a pretrained
network or a sequence of convolution layers that can be used as the
<code>deepimage</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: this class represents the integration
 between <code>pytorch-widedeep</code> and <code>torchvision</code>. New architectures will be
 available as they are added to <code>torchvision</code>. In a distant future we aim
 to bring transformer-based architectures as well. However, simple
 CNN-based architectures (and even MLP-based) seem to produce SoTA
 results. For the time being, we describe below the options available
 through this class</p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>pretrained_model_setup</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[str, <span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.Union">Union</span>[str, <span title="pytorch_widedeep.wdtypes.WeightsEnum">WeightsEnum</span>]]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Name of the pretrained model. Should be a variant of the following
architectures: <em>'resnet'</em>, <em>'shufflenet'</em>, <em>'resnext'</em>,
<em>'wide_resnet'</em>, <em>'regnet'</em>, <em>'densenet'</em>, <em>'mobilenetv3'</em>,
<em>'mobilenetv2'</em>, <em>'mnasnet'</em>, <em>'efficientnet'</em> and <em>'squeezenet'</em>. if
<code>pretrained_model_setup = None</code> a basic, fully trainable CNN will be
used. Alternatively, since Torchvision 0.13 one can use pretrained
models with different weigths. Therefore, <code>pretrained_model_setup</code> can
also be dictionary with the name of the model and the weights (e.g.
<code>{'resnet50': ResNet50_Weights.DEFAULT}</code> or
<code>{'resnet50': "IMAGENET1K_V2"}</code>). <br/> Aliased as <code>pretrained_model_name</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>n_trainable</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Number of trainable layers starting from the layer closer to the
output neuron(s). Note that this number DOES NOT take into account
the so-called <em>'head'</em> which is ALWAYS trainable. If
<code>trainable_params</code> is not None this parameter will be ignored</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>trainable_params</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List of strings containing the names (or substring within the name) of
the parameters that will be trained. For example, if we use a
<em>'resnet18'</em> pretrained model and we set <code>trainable_params =
['layer4']</code> only the parameters of <em>'layer4'</em> of the network
(and the head, as mentioned before) will be trained. Note that
setting this or the previous parameter involves some knowledge of
the architecture used.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>channel_sizes</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>, default:
                <code>[64, 128, 256, 512]</code>
)
        –
        <div class="doc-md-description">
          <p>List of integers with the channel sizes of a CNN in case we choose not
to use a pretrained model</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>kernel_sizes</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>[7, 3, 3, 3]</code>
)
        –
        <div class="doc-md-description">
          <p>List of integers with the kernel sizes of a CNN in case we choose not
to use a pretrained model. Must be of length equal to <code>len(channel_sizes) - 1</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>strides</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>[2, 1, 1, 1]</code>
)
        –
        <div class="doc-md-description">
          <p>List of integers with the stride sizes of a CNN in case we choose not
to use a pretrained model. Must be of length equal to <code>len(channel_sizes) - 1</code>.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the number of neurons per dense layer in the head. e.g: <em>[64,32]</em></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_dropout</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>float indicating the dropout between the dense layers.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm_last</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_linear_first</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
        </div>
      </li>
  </ul>



  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b><code><span title="pytorch_widedeep.models.image.vision.Vision.features">features</span></code></b>
            (<code><span title="torch.nn.Module">Module</span></code>)
        –
        <div class="doc-md-description">
          <p>The pretrained model or Standard CNN plus the optional head</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Vision</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Vision</span><span class="p">(</span><span class="n">channel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">kernel_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">head_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_img</span><span class="p">)</span>
</code></pre></div>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/image/vision.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span><span class="s2">&quot;pretrained_model_setup&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;pretrained_model_name&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_setup</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">WeightsEnum</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_trainable</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">channel_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">kernel_sizes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">strides</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Vision</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_check_pretrained_model_setup</span><span class="p">(</span>
        <span class="n">pretrained_model_setup</span><span class="p">,</span> <span class="n">n_trainable</span><span class="p">,</span> <span class="n">trainable_params</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span> <span class="o">=</span> <span class="n">pretrained_model_setup</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_trainable</span> <span class="o">=</span> <span class="n">n_trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainable_params</span> <span class="o">=</span> <span class="n">trainable_params</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">channel_sizes</span> <span class="o">=</span> <span class="n">channel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_sizes</span> <span class="o">=</span> <span class="n">kernel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">strides</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_features</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">pretrained_model_setup</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vision_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pytorch_widedeep.models.image.vision.Vision.output_dim" class="doc doc-heading">
          <span class="doc doc-object-name doc-attribute-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.image.vision.Vision.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">output_dim</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>
  </div>

</div>





  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.wide_deep.WideDeep" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">WideDeep</span>


<a href="#pytorch_widedeep.models.wide_deep.WideDeep" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">WideDeep</span><span class="p">(</span>
    <span class="n">wide</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptabular</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptext</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deepimage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deephead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">enforce_positive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">enforce_positive_activation</span><span class="o">=</span><span class="s2">&quot;softplus&quot;</span><span class="p">,</span>
    <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">with_fds</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">fds_config</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Main collector class that combines all <code>wide</code>, <code>deeptabular</code>
<code>deeptext</code> and <code>deepimage</code> models.</p>
<p>Note that all models described so far in this library must be passed to
the <code>WideDeep</code> class once constructed. This is because the models output
the last layer before the prediction layer. Such prediction layer is
added by the <code>WideDeep</code> class as it collects the components for every
data mode.</p>
<p>There are two options to combine these models that correspond to the
two main architectures that <code>pytorch-widedeep</code> can build.</p>
<ul>
<li>
<p>Directly connecting the output of the model components to an ouput neuron(s).</p>
</li>
<li>
<p>Adding a <code>Fully-Connected Head</code> (FC-Head) on top of the deep models.
  This FC-Head will combine the output form the <code>deeptabular</code>, <code>deeptext</code> and
  <code>deepimage</code> and will be then connected to the output neuron(s).</p>
</li>
</ul>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>wide</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="torch.nn.Module">Module</span>]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p><code>Wide</code> model. This is a linear model where the non-linearities are
captured via crossed-columns.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>deeptabular</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Currently this library implements a number of possible architectures
for the <code>deeptabular</code> component. See the documenation of the
package.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>deeptext</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Currently this library implements a number of possible architectures
for the <code>deeptext</code> component. See the documenation of the
package.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>deepimage</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Currently this library uses <code>torchvision</code> and implements a number of
possible architectures for the <code>deepimage</code> component. See the
documenation of the package.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>deephead</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.models._base_wd_model_component.BaseWDModelComponent">BaseWDModelComponent</span>]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Alternatively, the user can pass a custom model that will receive the
output of the deep component. If <code>deephead</code> is not None all the
previous fc-head parameters will be ignored</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_hidden_dims</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>List with the sizes of the dense layers in the head e.g: [128, 64]</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;relu&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function for the dense layers in the head. Currently
<code>'tanh'</code>, <code>'relu'</code>, <code>'leaky_relu'</code> and <code>'gelu'</code> are supported</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_dropout</code></b>
            (<code>float</code>, default:
                <code>0.1</code>
)
        –
        <div class="doc-md-description">
          <p>Dropout of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <code>'rnn_mlp'</code></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_batchnorm_last</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>head_linear_first</code></b>
            (<code>bool</code>, default:
                <code>True</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
        </div>
      </li>
      <li class="field-body">
        <b><code>enforce_positive</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if the output from the final layer must be
positive. This is important if you are using loss functions with
non-negative input restrictions, e.g. RMSLE, or if you know your
predictions are bounded in between 0 and inf</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>enforce_positive_activation</code></b>
            (<code>str</code>, default:
                <code>&#39;softplus&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>Activation function to enforce that the final layer has a positive
output. <code>'softplus'</code> or <code>'relu'</code> are supported.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>pred_dim</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Size of the final wide and deep output layer containing the
predictions. <code>1</code> for regression and binary classification or number
of classes for multiclass classification.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>with_fds</code></b>
            (<code>bool</code>, default:
                <code>False</code>
)
        –
        <div class="doc-md-description">
          <p>Boolean indicating if Feature Distribution Smoothing (FDS) will be
applied before the final prediction layer. Only available for
regression problems.
See <a href="https://arxiv.org/abs/2102.09554">Delving into Deep Imbalanced Regression</a> for details.</p>
        </div>
      </li>
  </ul>



  <p>Other Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>**fds_config</code></b>
        –
        <div class="doc-md-description">
          <p>Dictionary with the parameters to be used when using Feature
Distribution Smoothing. Please, see the docs for the <code>FDSLayer</code>.
<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: Feature Distribution Smoothing
 is available when using <strong>ONLY</strong> a <code>deeptabular</code> component
<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: We consider this feature absolutely
experimental and we recommend the user to not use it unless the
corresponding <a href="https://arxiv.org/abs/2102.09554">publication</a> is
well understood</p>
        </div>
      </li>
  </ul>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnet</span><span class="p">,</span> <span class="n">Vision</span><span class="p">,</span> <span class="n">BasicRNN</span><span class="p">,</span> <span class="n">Wide</span><span class="p">,</span> <span class="n">WideDeep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">][:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deeptabular</span> <span class="o">=</span> <span class="n">TabResnet</span><span class="p">(</span><span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">embed_input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deeptext</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deepimage</span> <span class="o">=</span> <span class="n">Vision</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">WideDeep</span><span class="p">(</span><span class="n">wide</span><span class="o">=</span><span class="n">wide</span><span class="p">,</span> <span class="n">deeptabular</span><span class="o">=</span><span class="n">deeptabular</span><span class="p">,</span> <span class="n">deeptext</span><span class="o">=</span><span class="n">deeptext</span><span class="p">,</span> <span class="n">deepimage</span><span class="o">=</span><span class="n">deepimage</span><span class="p">)</span>
</code></pre></div>
    <p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: It is possible to use custom components to
 build Wide &amp; Deep models. Simply, build them and pass them as the
 corresponding parameters. Note that the custom models MUST return a last
 layer of activations(i.e. not the final prediction) so that  these
 activations are collected by <code>WideDeep</code> and combined accordingly. In
 addition, the models MUST also contain an attribute <code>output_dim</code> with
 the size of these last layers of activations. See for example
 <code>pytorch_widedeep.models.tab_mlp.TabMlp</code></p>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/wide_deep.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@alias</span><span class="p">(</span>  <span class="c1"># noqa: C901</span>
    <span class="s2">&quot;pred_dim&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;num_class&quot;</span><span class="p">,</span> <span class="s2">&quot;pred_size&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">wide</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptabular</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptext</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deepimage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deephead</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BaseWDModelComponent</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">enforce_positive</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">enforce_positive_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;softplus&quot;</span><span class="p">,</span>
    <span class="n">pred_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">with_fds</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">fds_config</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">WideDeep</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span>
        <span class="n">wide</span><span class="p">,</span>
        <span class="n">deeptabular</span><span class="p">,</span>
        <span class="n">deeptext</span><span class="p">,</span>
        <span class="n">deepimage</span><span class="p">,</span>
        <span class="n">deephead</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">,</span>
        <span class="n">pred_dim</span><span class="p">,</span>
        <span class="n">with_fds</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># this attribute will be eventually over-written by the Trainer&#39;s</span>
    <span class="c1"># device. Acts here as a &#39;placeholder&#39;.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wd_device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># required as attribute just in case we pass a deephead</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span> <span class="o">=</span> <span class="n">pred_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_fds</span> <span class="o">=</span> <span class="n">with_fds</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">enforce_positive</span> <span class="o">=</span> <span class="n">enforce_positive</span>

    <span class="c1"># The main 5 components of the wide and deep assemble: wide,</span>
    <span class="c1"># deeptabular, deeptext, deepimage and deephead</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_deephead</span> <span class="o">=</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_deephead</span><span class="p">(</span>
            <span class="n">deeptabular</span><span class="p">,</span>
            <span class="n">deeptext</span><span class="p">,</span>
            <span class="n">deepimage</span><span class="p">,</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">deephead</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">deephead</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># for consistency with other components we default to None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">wide</span> <span class="o">=</span> <span class="n">wide</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptext</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepimage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_model_components</span><span class="p">(</span>
        <span class="n">deeptabular</span><span class="p">,</span> <span class="n">deeptext</span><span class="p">,</span> <span class="n">deepimage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_deephead</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_fds</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fds_layer</span> <span class="o">=</span> <span class="n">FDSLayer</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">fds_config</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enforce_positive</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enf_pos</span> <span class="o">=</span> <span class="n">get_activation_fn</span><span class="p">(</span><span class="n">enforce_positive_activation</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.fds_layer.FDSLayer" class="doc doc-heading">
          <span class="doc doc-object-name doc-class-name">FDSLayer</span>


<a href="#pytorch_widedeep.models.fds_layer.FDSLayer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><span class="n">FDSLayer</span><span class="p">(</span>
    <span class="n">feature_dim</span><span class="p">,</span>
    <span class="n">granularity</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">y_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">y_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">start_update</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">start_smooth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="n">ks</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">clip_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.Module">Module</span></code></p>

  
      <p>Feature Distribution Smoothing layer. Please, see
<a href="https:/arxiv.org/abs/2102.09554">Delving into Deep Imbalanced Regression</a>
for details.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: this is NOT an available model per se,
 but more a utility that can be used as we run a <code>WideDeep</code> model.
 The parameters of this extra layers can be set as the class
 <code>WideDeep</code> is instantiated via the keyword arguments <code>fds_config</code>.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: Feature Distribution Smoothing is
 available when using ONLY a <code>deeptabular</code> component</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: We consider this feature absolutely
experimental and we recommend the user to not use it unless the
corresponding <a href="https://arxiv.org/abs/2102.09554">publication</a> is
well understood</p>
<p>The code here is based on the code at the
<a href="https://github.com/YyzHarry/imbalanced-regression">official repo</a></p>



  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b><code>feature_dim</code></b>
            (<code>int</code>)
        –
        <div class="doc-md-description">
          <p>input dimension size, i.e. output size of previous layer. This
will be the dimension of the output from the <code>deeptabular</code>
component</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>granularity</code></b>
            (<code>int</code>, default:
                <code>100</code>
)
        –
        <div class="doc-md-description">
          <p>number of bins that the target <span class="arithmatex">\(y\)</span> is divided into and that will
be used to compute the features' statistics (mean and variance)</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>y_max</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p><span class="arithmatex">\(y\)</span> upper limit to be considered when binning</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>y_min</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p><span class="arithmatex">\(y\)</span> lower limit to be considered when binning</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>start_update</code></b>
            (<code>int</code>, default:
                <code>0</code>
)
        –
        <div class="doc-md-description">
          <p>number of _'waiting epochs' after which the FDS layer will start
to update its statistics</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>start_smooth</code></b>
            (<code>int</code>, default:
                <code>2</code>
)
        –
        <div class="doc-md-description">
          <p>number of _'waiting epochs' after which the FDS layer will start
smoothing the feature distributions</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>kernel</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[gaussian, triang, laplace]</code>, default:
                <code>&#39;gaussian&#39;</code>
)
        –
        <div class="doc-md-description">
          <p>choice of smoothing kernel</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>ks</code></b>
            (<code>int</code>, default:
                <code>5</code>
)
        –
        <div class="doc-md-description">
          <p>kernel window size</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>sigma</code></b>
            (<code>float</code>, default:
                <code>2</code>
)
        –
        <div class="doc-md-description">
          <p>if a <em>'gaussian'</em> or <em>'laplace'</em> kernels are used, this is the
corresponding standard deviation</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>momentum</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>0.9</code>
)
        –
        <div class="doc-md-description">
          <p>to train the layer the authors used a momentum update of the running
statistics across each epoch. Set to 0.9 in the paper.</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>clip_min</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>this parameter is used to clip the ratio between the so called
running variance and the smoothed variance, and is introduced for
numerical stability. We leave it as optional as we did not find a
notable improvement in our experiments. The authors used a value
of 0.1</p>
        </div>
      </li>
      <li class="field-body">
        <b><code>clip_max</code></b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>same as <code>clip_min</code> but for the upper limit.We leave it as optional
as we did not find a notable improvement in our experiments. The
authors used a value of 10.</p>
        </div>
      </li>
  </ul>

                <details class="quote">
                  <summary>Source code in <code>pytorch_widedeep/models/fds_layer.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">y_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">start_update</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">start_smooth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s2">&quot;triang&quot;</span><span class="p">,</span> <span class="s2">&quot;laplace&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="n">ks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">momentum</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
    <span class="n">clip_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Feature Distribution Smoothing layer. Please, see</span>
<span class="sd">    [Delving into Deep Imbalanced Regression](https:/arxiv.org/abs/2102.09554)</span>
<span class="sd">    for details.</span>

<span class="sd">    :information_source: **NOTE**: this is NOT an available model per se,</span>
<span class="sd">     but more a utility that can be used as we run a `WideDeep` model.</span>
<span class="sd">     The parameters of this extra layers can be set as the class</span>
<span class="sd">     `WideDeep` is instantiated via the keyword arguments `fds_config`.</span>

<span class="sd">    :information_source: **NOTE**: Feature Distribution Smoothing is</span>
<span class="sd">     available when using ONLY a `deeptabular` component</span>

<span class="sd">    :information_source: **NOTE**: We consider this feature absolutely</span>
<span class="sd">    experimental and we recommend the user to not use it unless the</span>
<span class="sd">    corresponding [publication](https://arxiv.org/abs/2102.09554) is</span>
<span class="sd">    well understood</span>

<span class="sd">    The code here is based on the code at the</span>
<span class="sd">    [official repo](https://github.com/YyzHarry/imbalanced-regression)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    feature_dim: int,</span>
<span class="sd">        input dimension size, i.e. output size of previous layer. This</span>
<span class="sd">        will be the dimension of the output from the `deeptabular`</span>
<span class="sd">        component</span>
<span class="sd">    granularity: int = 100,</span>
<span class="sd">        number of bins that the target $y$ is divided into and that will</span>
<span class="sd">        be used to compute the features&#39; statistics (mean and variance)</span>
<span class="sd">    y_max: Optional[float] = None,</span>
<span class="sd">        $y$ upper limit to be considered when binning</span>
<span class="sd">    y_min: Optional[float] = None,</span>
<span class="sd">        $y$ lower limit to be considered when binning</span>
<span class="sd">    start_update: int = 0,</span>
<span class="sd">        number of _&#39;waiting epochs&#39; after which the FDS layer will start</span>
<span class="sd">        to update its statistics</span>
<span class="sd">    start_smooth: int = 1,</span>
<span class="sd">        number of _&#39;waiting epochs&#39; after which the FDS layer will start</span>
<span class="sd">        smoothing the feature distributions</span>
<span class="sd">    kernel: Literal[&quot;gaussian&quot;, &quot;triang&quot;, &quot;laplace&quot;, None] = &quot;gaussian&quot;,</span>
<span class="sd">        choice of smoothing kernel</span>
<span class="sd">    ks: int = 5,</span>
<span class="sd">        kernel window size</span>
<span class="sd">    sigma: Union[int, float] = 2,</span>
<span class="sd">        if a _&#39;gaussian&#39;_ or _&#39;laplace&#39;_ kernels are used, this is the</span>
<span class="sd">        corresponding standard deviation</span>
<span class="sd">    momentum: float = 0.9,</span>
<span class="sd">        to train the layer the authors used a momentum update of the running</span>
<span class="sd">        statistics across each epoch. Set to 0.9 in the paper.</span>
<span class="sd">    clip_min: Optional[float] = None,</span>
<span class="sd">        this parameter is used to clip the ratio between the so called</span>
<span class="sd">        running variance and the smoothed variance, and is introduced for</span>
<span class="sd">        numerical stability. We leave it as optional as we did not find a</span>
<span class="sd">        notable improvement in our experiments. The authors used a value</span>
<span class="sd">        of 0.1</span>
<span class="sd">    clip_max: Optional[float] = None,</span>
<span class="sd">        same as `clip_min` but for the upper limit.We leave it as optional</span>
<span class="sd">        as we did not find a notable improvement in our experiments. The</span>
<span class="sd">        authors used a value of 10.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FDSLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">start_update</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">start_smooth</span>
    <span class="p">),</span> <span class="s2">&quot;initial update must start at least 2 epoch before smoothing&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">feature_dim</span> <span class="o">=</span> <span class="n">feature_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span> <span class="o">=</span> <span class="n">granularity</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">y_max</span> <span class="o">=</span> <span class="n">y_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">y_min</span> <span class="o">=</span> <span class="n">y_min</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_window</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">get_kernel_window</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">ks</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">half_ks</span> <span class="o">=</span> <span class="p">(</span><span class="n">ks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">start_update</span> <span class="o">=</span> <span class="n">start_update</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">start_smooth</span> <span class="o">=</span> <span class="n">start_smooth</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clip_min</span> <span class="o">=</span> <span class="n">clip_min</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clip_max</span> <span class="o">=</span> <span class="n">clip_max</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pred_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_register_buffers</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>








  <aside class="md-source-file">
    
    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 5.5A3.5 3.5 0 0 1 15.5 9a3.5 3.5 0 0 1-3.5 3.5A3.5 3.5 0 0 1 8.5 9 3.5 3.5 0 0 1 12 5.5M5 8c.56 0 1.08.15 1.53.42-.15 1.43.27 2.85 1.13 3.96C7.16 13.34 6.16 14 5 14a3 3 0 0 1-3-3 3 3 0 0 1 3-3m14 0a3 3 0 0 1 3 3 3 3 0 0 1-3 3c-1.16 0-2.16-.66-2.66-1.62a5.536 5.536 0 0 0 1.13-3.96c.45-.27.97-.42 1.53-.42M5.5 18.25c0-2.07 2.91-3.75 6.5-3.75s6.5 1.68 6.5 3.75V20h-13v-1.75M0 20v-1.5c0-1.39 1.89-2.56 4.45-2.9-.59.68-.95 1.62-.95 2.65V20H0m24 0h-3.5v-1.75c0-1.03-.36-1.97-.95-2.65 2.56.34 4.45 1.51 4.45 2.9V20Z"/></svg>
      
    </span>
    <nav>
      
        <a href="mailto:jrzaurin@gmail.com">Javier</a>, 
        <a href="mailto:javierrodriguezzaurin@javiers-macbook-pro.local">Javier Rodriguez Zaurin</a>
    </nav>
  </span>

    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Javier Zaurin and Pavol Mulinka
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://jrzaurin.medium.com/" target="_blank" rel="noopener" title="jrzaurin.medium.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.expand", "toc.integrate"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
        <script src="../stylesheets/extra.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>