
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch-widedeep.readthedocs.io/pytorch-widedeep/model_components.html">
      
      <link rel="icon" href="../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.4.2">
    
    
      
        <title>Model Components - pytorch_widedeep</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.69437709.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-models-module" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="pytorch_widedeep" class="md-header__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            pytorch_widedeep
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Model Components
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../index.html" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../installation.html" class="md-tabs__link">
      Installation
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../quick_start.html" class="md-tabs__link">
      Quick Start
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="utils/index.html" class="md-tabs__link md-tabs__link--active">
        Pytorch-widedeep
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../examples/01_Preprocessors_and_utils.html" class="md-tabs__link">
        Examples
      </a>
    </li>
  

      
        
  
  


  <li class="md-tabs__item">
    <a href="../contributing.html" class="md-tabs__link">
      Contributing
    </a>
  </li>

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="pytorch_widedeep" class="md-nav__button md-logo" aria-label="pytorch_widedeep" data-md-component="logo">
      
  <img src="../assets/images/widedeep_logo.png" alt="logo">

    </a>
    pytorch_widedeep
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/jrzaurin/pytorch-widedeep" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch_widedeep
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../installation.html" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../quick_start.html" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4">
          Pytorch-widedeep
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Pytorch-widedeep" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Pytorch-widedeep
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4_1" type="checkbox" id="__nav_4_1" checked>
      
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="utils/index.html">Utils</a>
          
            <label for="__nav_4_1">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Utils" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          Utils
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="utils/deeptabular_utils.html" class="md-nav__link">
        Deeptabular utils
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="utils/fastai_transforms.html" class="md-nav__link">
        Fastai transforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="utils/image_utils.html" class="md-nav__link">
        Image utils
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="utils/text_utils.html" class="md-nav__link">
        Text utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="preprocessing.html" class="md-nav__link">
        Preprocessing
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Model Components
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="model_components.html" class="md-nav__link md-nav__link--active">
        Model Components
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.linear.wide.Wide" class="md-nav__link">
    Wide
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="md-nav__link">
    TabMlp
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="md-nav__link">
    TabMlpDecoder
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="md-nav__link">
    TabResnet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="md-nav__link">
    TabResnetDecoder
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="md-nav__link">
    TabNet
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="md-nav__link">
    TabNetDecoder
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="md-nav__link">
    ContextAttentionMLP
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="md-nav__link">
    SelfAttentionMLP
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="md-nav__link">
    TabTransformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="md-nav__link">
    SAINT
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="md-nav__link">
    FTTransformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="md-nav__link">
    TabPerceiver
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="md-nav__link">
    TabFastFormer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.attentive_rnn.BasicRNN" class="md-nav__link">
    BasicRNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.attentive_rnn.AttentiveRNN" class="md-nav__link">
    AttentiveRNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.text.stacked_attentive_rnn.StackedAttentiveRNN" class="md-nav__link">
    StackedAttentiveRNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.image.vision.Vision" class="md-nav__link">
    Vision
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.wide_deep.WideDeep" class="md-nav__link">
    WideDeep
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_widedeep.models.fds_layer.FDSLayer" class="md-nav__link">
    FDSLayer
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_models.html" class="md-nav__link">
        Bayesian models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="losses.html" class="md-nav__link">
        Losses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="metrics.html" class="md-nav__link">
        Metrics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="dataloaders.html" class="md-nav__link">
        Dataloaders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="callbacks.html" class="md-nav__link">
        Callbacks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="trainer.html" class="md-nav__link">
        Trainer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="bayesian_trainer.html" class="md-nav__link">
        Bayesian Trainer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="self_supervised_pretraining.html" class="md-nav__link">
        Self Supervised Pretraining
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="tab2vec.html" class="md-nav__link">
        Tab2Vec
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5">
          Examples
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Examples" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Examples
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/01_Preprocessors_and_utils.html" class="md-nav__link">
        01_Preprocessors_and_utils
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/02_model_components.html" class="md-nav__link">
        02_model_components
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/03_Binary_Classification_with_Defaults.html" class="md-nav__link">
        03_Binary_Classification_with_Defaults
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/04_regression_with_images_and_text.html" class="md-nav__link">
        04_regression_with_images_and_text
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/05_save_and_load_model_and_artifacts.html" class="md-nav__link">
        05_save_and_load_model_and_artifacts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/06_fineTune_and_warmup.html" class="md-nav__link">
        06_fineTune_and_warmup
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/07_Custom_Components.html" class="md-nav__link">
        07_Custom_Components
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/08_custom_dataLoader_imbalanced_dataset.html" class="md-nav__link">
        08_custom_dataLoader_imbalanced_dataset
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/09_extracting_embeddings.html" class="md-nav__link">
        09_extracting_embeddings
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/11_auc_multiclass.html" class="md-nav__link">
        11_auc_multiclass
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/12_ZILNLoss_origkeras_vs_pytorch_multimodal.html" class="md-nav__link">
        12_ZILNLoss_origkeras_vs_pytorch_multimodal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/13_Model_Uncertainty_prediction.html" class="md-nav__link">
        13_Model_Uncertainty_prediction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/14_bayesian_models.html" class="md-nav__link">
        14_bayesian_models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/15_DIR-LDS_and_FDS.html" class="md-nav__link">
        15_DIR-LDS_and_FDS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/16_Self_Supervised_Pretraning_pt1.html" class="md-nav__link">
        16_Self-Supervised Pre-Training pt 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../examples/16_Self_Supervised_Pretraning_pt2.html" class="md-nav__link">
        16_Self-Supervised Pre-Training pt 2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../contributing.html" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
  <a href="https://github.com/jrzaurin/pytorch-widedeep/edit/master/mkdocs/sources/pytorch-widedeep/model_components.md" title="Edit this page" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>



<h1 id="the-models-module">The <code>models</code> module<a class="headerlink" href="#the-models-module" title="Permanent link">&para;</a></h1>
<p>This module contains the models that can be used as the four main components
that will comprise a Wide and Deep model (<code>wide</code>, <code>deeptabular</code>,
<code>deeptext</code>, <code>deepimage</code>), as well as the <code>WideDeep</code> "constructor"
class. Note that each of the four components can be used independently. It
also contains all the documentation for the models that can be used for
self-supervised pre-training with tabular data.</p>


<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.linear.wide.Wide" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">Wide</span>


<a href="#pytorch_widedeep.models.tabular.linear.wide.Wide" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">Wide</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Defines a <code>Wide</code> (linear) model where the non-linearities are
captured via the so-called crossed-columns. This can be used as the
<code>wide</code> component of a Wide &amp; Deep model.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>input_dim</b>
            (<code>int</code>)
        – <p>size of the Embedding layer. <code>input_dim</code> is the summation of all the
individual values for all the features that go through the wide
model. For example, if the wide model receives 2 features with
5 individual values each, <code>input_dim = 10</code></p>
      </li>
      <li class="field-body">
        <b>pred_dim</b>
            (<code>int</code>)
        – <p>size of the ouput tensor containing the predictions. Note that unlike
all the other models, the wide model is connected directly to the
output neuron(s) when used to build a Wide and Deep model. Therefore,
it requires the <code>pred_dim</code> parameter.</p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>wide_linear</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>the linear layer that comprises the wide branch of the model</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Wide</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">wide</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/linear/wide.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">pred_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Wide</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span> <span class="o">=</span> <span class="n">pred_dim</span>

    <span class="c1"># Embeddings: val + 1 because 0 is reserved for padding/unseen cateogories.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wide_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pred_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># (Sum(Embedding) + bias) is equivalent to (OneHotVector + Linear)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">pred_dim</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">TabMlp</span>


<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">TabMlp</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>

  
      <p>Defines a <code>TabMlp</code> model that can be used as the <code>deeptabular</code>
component of a Wide &amp; Deep model or independently by itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features, embedded or not. These are then
passed through a series of dense layers (i.e. a MLP).</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the <code>TabMlp</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em>.</p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>)
        – <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: <em>'layernorm'</em>, <em>'batchnorm'</em> or None.</p>
      </li>
      <li class="field-body">
        <b>embed_continuous</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the continuous columns will be embedded
(i.e. passed each through a linear layer with or without activation)</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dim</b>
            (<code>int</code>)
        – <p>Size of the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Dropout for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the continuous embeddings if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>)
        – <p>List with the number of neurons per dense layer in the mlp.</p>
      </li>
      <li class="field-body">
        <b>mlp_activation</b>
            (<code>str</code>)
        – <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_dropout</b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>)
        – <p>float or List of floats with the dropout between the dense layers.
e.g: <em>[0.5,0.5]</em></p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>mlp model that will receive the concatenation of the embeddings and
the continuous columns</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabMlp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabMlp</span><span class="p">(</span><span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/tab_mlp.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabMlp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Mlp</span>
    <span class="n">mlp_input_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_and_cont_embed</span><span class="o">.</span><span class="n">output_dim</span>
    <span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">mlp_input_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
        <span class="n">mlp_hidden_dims</span><span class="p">,</span>
        <span class="n">mlp_activation</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/tab_mlp.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">TabMlpDecoder</span>


<a href="#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlpDecoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">TabMlpDecoder</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Companion decoder model for the <code>TabMlp</code> model (which can be considered
an encoder itself).</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). The <code>TabMlpDecoder</code> will receive the output from the MLP
and '<em>reconstruct</em>' the embeddings.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>embed_dim</b>
            (<code>int</code>)
        – <p>Size of the embeddings tensor that needs to be reconstructed.</p>
      </li>
      <li class="field-body">
        <b>mlp_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>)
        – <p>List with the number of neurons per dense layer in the mlp.</p>
      </li>
      <li class="field-body">
        <b>mlp_activation</b>
            (<code>str</code>)
        – <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_dropout</b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>)
        – <p>float or List of floats with the dropout between the dense layers.
e.g: <em>[0.5,0.5]</em></p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>decoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>mlp model that will receive the output of the encoder</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabMlpDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabMlpDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/tab_mlp.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabMlpDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
        <span class="n">mlp_hidden_dims</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">],</span>
        <span class="n">mlp_activation</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">,</span>
        <span class="n">mlp_batchnorm</span><span class="p">,</span>
        <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
        <span class="n">mlp_linear_first</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">TabResnet</span>


<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">TabResnet</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>

  
      <p>Defines a <code>TabResnet</code> model that can be used as the <code>deeptabular</code>
component of a Wide &amp; Deep model or independently by itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features, embedded or not. These are then
passed through a series of Resnet blocks. See
<code>pytorch_widedeep.models.tab_resnet._layers</code> for details on the
structure of each block.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the model. Required to slice the tensors. e.g.
<em>{'education': 0, 'relationship': 1, 'workclass': 2, ...}</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>)
        – <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em>.</p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu` and _'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: <em>'layernorm'</em>, <em>'batchnorm'</em> or <code>None</code>.</p>
      </li>
      <li class="field-body">
        <b>embed_continuous</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the continuous columns will be embedded
(i.e. passed each through a linear layer with or without activation)</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dim</b>
            (<code>int</code>)
        – <p>Size of the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Continuous embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the continuous embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu` and _'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>blocks_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>)
        – <p>List of integers that define the input and output units of each block.
For example: <em>[200, 100, 100]</em> will generate 2 blocks. The first will
receive a tensor of size 200 and output a tensor of size 100, and the
second will receive a tensor of size 100 and output a tensor of size
100. See <code>pytorch_widedeep.models.tab_resnet._layers</code> for
details on the structure of each block.</p>
      </li>
      <li class="field-body">
        <b>blocks_dropout</b>
            (<code>float</code>)
        – <p>Block's internal dropout.</p>
      </li>
      <li class="field-body">
        <b>simplify_blocks</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the simplest possible residual blocks (<code>X -&gt; [
[LIN, BN, ACT]  + X ]</code>) will be used instead of a standard one
(<code>X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]</code>).</p>
      </li>
      <li class="field-body">
        <b>mlp_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If <code>None</code> the  output of the Resnet Blocks will be
connected directly to the output neuron(s).</p>
      </li>
      <li class="field-body">
        <b>mlp_activation</b>
            (<code>str</code>)
        – <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu` and _'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_dropout</b>
            (<code>float</code>)
        – <p>float with the dropout between the dense layers of the MLP.</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>deep dense Resnet model that will receive the concatenation of the
embeddings and the continuous columns</p>
      </li>
      <li class="field-body">
        <b>mlp</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>if <code>mlp_hidden_dims</code> is <code>True</code>, this attribute will be an mlp
model that will receive the results of the concatenation of the
embeddings and the continuous columns -- if present --.</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_deep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabResnet</span><span class="p">(</span><span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_deep</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/resnet/tab_resnet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabResnet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;blocks&#39; must contain at least two elements, e.g. [256, 128]&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span> <span class="o">=</span> <span class="n">blocks_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dropout</span> <span class="o">=</span> <span class="n">blocks_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span> <span class="o">=</span> <span class="n">simplify_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="n">cat_out_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_and_cont_embed</span><span class="o">.</span><span class="n">cat_out_dim</span>
    <span class="n">cont_out_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_and_cont_embed</span><span class="o">.</span><span class="n">cont_out_dim</span>

    <span class="c1"># Resnet</span>
    <span class="n">dense_resnet_input_dim</span> <span class="o">=</span> <span class="n">cat_out_dim</span> <span class="o">+</span> <span class="n">cont_out_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
        <span class="n">dense_resnet_input_dim</span><span class="p">,</span> <span class="n">blocks_dims</span><span class="p">,</span> <span class="n">blocks_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span>
    <span class="p">)</span>

    <span class="c1"># Mlp</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">mlp_activation</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
            <span class="n">mlp_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnet.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/resnet/tab_resnet.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">TabResnetDecoder</span>


<a href="#pytorch_widedeep.models.tabular.resnet.tab_resnet.TabResnetDecoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">TabResnetDecoder</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Companion decoder model for the <code>TabResnet</code> model (which can be
considered an encoder itself)</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). This class will receive the output from the ResNet blocks or the
MLP(if present) and '<em>reconstruct</em>' the embeddings.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>embed_dim</b>
            (<code>int</code>)
        – <p>Size of the embeddings tensor to be reconstructed.</p>
      </li>
      <li class="field-body">
        <b>blocks_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>)
        – <p>List of integers that define the input and output units of each block.
For example: <em>[200, 100, 100]</em> will generate 2 blocks. The first will
receive a tensor of size 200 and output a tensor of size 100, and the
second will receive a tensor of size 100 and output a tensor of size
100. See <code>pytorch_widedeep.models.tab_resnet._layers</code> for
details on the structure of each block.</p>
      </li>
      <li class="field-body">
        <b>blocks_dropout</b>
            (<code>float</code>)
        – <p>Block's internal dropout.</p>
      </li>
      <li class="field-body">
        <b>simplify_blocks</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the simplest possible residual blocks (<code>X -&gt; [
[LIN, BN, ACT]  + X ]</code>) will be used instead of a standard one
(<code>X -&gt; [ [LIN1, BN1, ACT1] -&gt; [LIN2, BN2]  + X ]</code>).</p>
      </li>
      <li class="field-body">
        <b>mlp_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>List with the number of neurons per dense layer in the MLP. e.g:
<em>[64, 32]</em>. If <code>None</code> the  output of the Resnet Blocks will be
connected directly to the output neuron(s).</p>
      </li>
      <li class="field-body">
        <b>mlp_activation</b>
            (<code>str</code>)
        – <p>Activation function for the dense layers of the MLP. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky'_relu` and _'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_dropout</b>
            (<code>float</code>)
        – <p>float with the dropout between the dense layers of the MLP.</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>decoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>deep dense Resnet model that will receive the output of the encoder IF
<code>mlp_hidden_dims</code> is None</p>
      </li>
      <li class="field-body">
        <b>mlp</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>if <code>mlp_hidden_dims</code> is not None, the overall decoder will consist
in an MLP that will receive the output of the encoder followed by the
deep dense Resnet.</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnetDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabResnetDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/resnet/tab_resnet.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">blocks_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="n">blocks_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">simplify_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabResnetDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;&#39;blocks&#39; must contain at least two elements, e.g. [256, 128]&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dims</span> <span class="o">=</span> <span class="n">blocks_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">blocks_dropout</span> <span class="o">=</span> <span class="n">blocks_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span> <span class="o">=</span> <span class="n">simplify_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">mlp_activation</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
            <span class="n">mlp_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
            <span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">blocks_dims</span><span class="p">,</span> <span class="n">blocks_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">DenseResnet</span><span class="p">(</span>
            <span class="n">blocks_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">blocks_dims</span><span class="p">,</span> <span class="n">blocks_dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_blocks</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">blocks_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">TabNet</span>


<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">TabNet</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">attn_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">1.3</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">,</span>
    <span class="n">mask_type</span><span class="o">=</span><span class="s2">&quot;sparsemax&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithoutAttention">BaseTabularModelWithoutAttention</span></code></p>

  
      <p>Defines a <a href="https://arxiv.org/abs/1908.07442">TabNet model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p>The implementation in this library is fully based on that
<a href="https://github.com/dreamquark-ai/tabnet">here</a> by the dreamquark-ai team,
simply adapted so that it can work within the <code>WideDeep</code> frame.
Therefore, <strong>ALL CREDIT TO THE DREAMQUARK-AI TEAM</strong>.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the <code>TabNet</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int, int]]]</code>)
        – <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings, if any. <em>'tanh'</em>,
<em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: <em>'layernorm'</em>, <em>'batchnorm'</em> or <code>None</code>.</p>
      </li>
      <li class="field-body">
        <b>embed_continuous</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the continuous columns will be embedded
(i.e. passed each through a linear layer with or without activation)</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dim</b>
            (<code>int</code>)
        – <p>Size of the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Dropout for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the continuous embeddings, if any. <em>'tanh'</em>,
<em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>n_steps</b>
            (<code>int</code>)
        – <p>number of decision steps. For a better understanding of the function
of <code>n_steps</code> and the upcoming parameters, please see the
<a href="https://arxiv.org/abs/1908.07442">paper</a>.</p>
      </li>
      <li class="field-body">
        <b>step_dim</b>
            (<code>int</code>)
        – <p>Step's output dimension. This is the output dimension that
<code>WideDeep</code> will collect and connect to the output neuron(s).</p>
      </li>
      <li class="field-body">
        <b>attn_dim</b>
            (<code>int</code>)
        – <p>Attention dimension</p>
      </li>
      <li class="field-body">
        <b>dropout</b>
            (<code>float</code>)
        – <p>GLU block's internal dropout</p>
      </li>
      <li class="field-body">
        <b>n_glu_step_dependent</b>
            (<code>int</code>)
        – <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that are step dependent</p>
      </li>
      <li class="field-body">
        <b>n_glu_shared</b>
            (<code>int</code>)
        – <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that will be shared
across decision steps</p>
      </li>
      <li class="field-body">
        <b>ghost_bn</b>
            (<code>bool</code>)
        – <p>Boolean indicating if <a href="https://arxiv.org/abs/1705.08741">Ghost Batch Normalization</a>
will be used.</p>
      </li>
      <li class="field-body">
        <b>virtual_batch_size</b>
            (<code>int</code>)
        – <p>Batch size when using Ghost Batch Normalization</p>
      </li>
      <li class="field-body">
        <b>momentum</b>
            (<code>float</code>)
        – <p>Ghost Batch Normalization's momentum. The dreamquark-ai advises for
very low values. However high values are used in the original
publication. During our tests higher values lead to better results</p>
      </li>
      <li class="field-body">
        <b>gamma</b>
            (<code>float</code>)
        – <p>Relaxation parameter in the paper. When gamma = 1, a feature is
enforced to be used only at one decision step. As gamma increases,
more flexibility is provided to use a feature at multiple decision
steps</p>
      </li>
      <li class="field-body">
        <b>epsilon</b>
            (<code>float</code>)
        – <p>Float to avoid log(0). Always keep low</p>
      </li>
      <li class="field-body">
        <b>mask_type</b>
            (<code>str</code>)
        – <p>Mask function to use. Either <em>'sparsemax'</em> or <em>'entmax'</em></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>the TabNet encoder. For details see the <a href="https://arxiv.org/abs/1908.07442">original publication</a>.</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabNet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabNet</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/tabnet/tab_net.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cont_embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">attn_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.3</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-15</span><span class="p">,</span>
    <span class="n">mask_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sparsemax&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">cont_embed_dim</span><span class="o">=</span><span class="n">cont_embed_dim</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span> <span class="o">=</span> <span class="n">step_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dim</span> <span class="o">=</span> <span class="n">attn_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_step_dependent</span> <span class="o">=</span> <span class="n">n_glu_step_dependent</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_shared</span> <span class="o">=</span> <span class="n">n_glu_shared</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ghost_bn</span> <span class="o">=</span> <span class="n">ghost_bn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">virtual_batch_size</span> <span class="o">=</span> <span class="n">virtual_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_type</span> <span class="o">=</span> <span class="n">mask_type</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_out_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_and_cont_embed</span><span class="o">.</span><span class="n">output_dim</span>

    <span class="c1"># TabNet</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TabNetEncoder</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_out_dim</span><span class="p">,</span>
        <span class="n">n_steps</span><span class="p">,</span>
        <span class="n">step_dim</span><span class="p">,</span>
        <span class="n">attn_dim</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">,</span>
        <span class="n">n_glu_step_dependent</span><span class="p">,</span>
        <span class="n">n_glu_shared</span><span class="p">,</span>
        <span class="n">ghost_bn</span><span class="p">,</span>
        <span class="n">virtual_batch_size</span><span class="p">,</span>
        <span class="n">momentum</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">,</span>
        <span class="n">mask_type</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNet.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/tabnet/tab_net.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">TabNetDecoder</span>


<a href="#pytorch_widedeep.models.tabular.tabnet.tab_net.TabNetDecoder" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">TabNetDecoder</span><span class="p">(</span>
    <span class="n">embed_dim</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Companion decoder model for the <code>TabNet</code> model (which can be
considered an encoder itself)</p>
<p>This class is designed to be used with the <code>EncoderDecoderTrainer</code> when
using self-supervised pre-training (see the corresponding section in the
docs). This class will receive the output from the <code>TabNet</code> encoder
(i.e. the output from the so called 'steps') and '<em>reconstruct</em>' the
embeddings.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>embed_dim</b>
            (<code>int</code>)
        – <p>Size of the embeddings tensor to be reconstructed.</p>
      </li>
      <li class="field-body">
        <b>n_steps</b>
            (<code>int</code>)
        – <p>number of decision steps. For a better understanding of the function
of <code>n_steps</code> and the upcoming parameters, please see the
<a href="https://arxiv.org/abs/1908.07442">paper</a>.</p>
      </li>
      <li class="field-body">
        <b>step_dim</b>
            (<code>int</code>)
        – <p>Step's output dimension. This is the output dimension that
<code>WideDeep</code> will collect and connect to the output neuron(s).</p>
      </li>
      <li class="field-body">
        <b>dropout</b>
            (<code>float</code>)
        – <p>GLU block's internal dropout</p>
      </li>
      <li class="field-body">
        <b>n_glu_step_dependent</b>
            (<code>int</code>)
        – <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that are step dependent</p>
      </li>
      <li class="field-body">
        <b>n_glu_shared</b>
            (<code>int</code>)
        – <p>number of GLU Blocks (<code>[FC -&gt; BN -&gt; GLU]</code>) that will be shared
across decision steps</p>
      </li>
      <li class="field-body">
        <b>ghost_bn</b>
            (<code>bool</code>)
        – <p>Boolean indicating if <a href="https://arxiv.org/abs/1705.08741">Ghost Batch Normalization</a>
will be used.</p>
      </li>
      <li class="field-body">
        <b>virtual_batch_size</b>
            (<code>int</code>)
        – <p>Batch size when using Ghost Batch Normalization</p>
      </li>
      <li class="field-body">
        <b>momentum</b>
            (<code>float</code>)
        – <p>Ghost Batch Normalization's momentum. The dreamquark-ai advises for
very low values. However high values are used in the original
publication. During our tests higher values lead to better results</p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>decoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>decoder that will receive the output from the encoder's steps and will
reconstruct the embeddings</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabNetDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_inp</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TabNetDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">ghost_bn</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">x_inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 32])</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/tabnet/tab_net.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">step_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">n_glu_step_dependent</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">n_glu_shared</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">ghost_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">virtual_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabNetDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step_dim</span> <span class="o">=</span> <span class="n">step_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_step_dependent</span> <span class="o">=</span> <span class="n">n_glu_step_dependent</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_glu_shared</span> <span class="o">=</span> <span class="n">n_glu_shared</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ghost_bn</span> <span class="o">=</span> <span class="n">ghost_bn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">virtual_batch_size</span> <span class="o">=</span> <span class="n">virtual_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>

    <span class="n">shared_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_glu_shared</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">transformer</span> <span class="o">=</span> <span class="n">FeatTransformer</span><span class="p">(</span>
            <span class="n">step_dim</span><span class="p">,</span>
            <span class="n">step_dim</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
            <span class="n">shared_layers</span><span class="p">,</span>
            <span class="n">n_glu_step_dependent</span><span class="p">,</span>
            <span class="n">ghost_bn</span><span class="p">,</span>
            <span class="n">virtual_batch_size</span><span class="p">,</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">step_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">initialize_non_glu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_layer</span><span class="p">,</span> <span class="n">step_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">ContextAttentionMLP</span>


<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">ContextAttentionMLP</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="o">=</span><span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines a <code>ContextAttentionMLP</code> model that can be used as the
<code>deeptabular</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features that are also embedded. These
are then passed through a series of attention blocks. Each attention
block is comprised by a <code>ContextAttentionEncoder</code>. Such encoder is in
part inspired by the attention mechanism described in
<a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf">Hierarchical Attention Networks for Document
Classification</a>.
See <code>pytorch_widedeep.models.tabular.mlp._attention_layers</code> for details.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the model. Required to slice the tensors. e.g.
<em>{'education': 0, 'relationship': 1, 'workclass': 2, ...}</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>)
        – <p>List of Tuples with the column name and number of unique values per
categorical columns. e.g. <em>[('education', 11), ...]</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings, if any. <em>'tanh'</em>,
<em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>full_embed_dropout</b>
            (<code>bool</code>)
        – <p>Boolean indicating if an entire embedding (i.e. the representation of
one column) will be dropped in the batch. See:
<code>pytorch_widedeep.models.embeddings_layers.FullEmbeddingDropout</code>.
If <code>full_embed_dropout = True</code>, <code>cat_embed_dropout</code> is ignored.</p>
      </li>
      <li class="field-body">
        <b>shared_embed</b>
            (<code>bool</code>)
        – <p>The idea behind sharing part of the embeddings per column is to let
the model learn which column is embedded at the time.</p>
      </li>
      <li class="field-body">
        <b>add_shared_embed</b>
            (<code>bool</code>)
        – <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code></p>
      </li>
      <li class="field-body">
        <b>frac_shared_embed</b>
            (<code>float</code>)
        – <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column.</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: <em>'layernorm'</em>, <em>'batchnorm'</em> or None.</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Continuous embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function to be applied to the continuous embeddings, if
any. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>input_dim</b>
            (<code>int</code>)
        – <p>The so-called <em>dimension of the model</em>. Is the number of embeddings
used to encode the categorical and/or continuous columns</p>
      </li>
      <li class="field-body">
        <b>attn_dropout</b>
            (<code>float</code>)
        – <p>Dropout for each attention block</p>
      </li>
      <li class="field-body">
        <b>with_addnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating if residual connections will be used in the
attention blocks</p>
      </li>
      <li class="field-body">
        <b>attn_activation</b>
            (<code>str</code>)
        – <p>String indicating the activation function to be applied to the dense
layer in each attention encoder. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em>
and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>n_blocks</b>
            (<code>int</code>)
        – <p>Number of attention blocks</p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Sequence of attention encoders.</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">ContextAttentionMLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ContextAttentionMLP</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/context_attention_mlp.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ContextAttentionMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_activation</span> <span class="o">=</span> <span class="n">attn_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Attention Blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;attention_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">ContextAttentionEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">with_addnorm</span><span class="p">,</span>
                <span class="n">attn_activation</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/context_attention_mlp.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span>
        <span class="k">else</span> <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.attention_weights" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.context_attention_mlp.ContextAttentionMLP.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">attention_weights</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(F\)</span> is the number of features/columns in the dataset</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/context_attention_mlp.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">    The shape of the attention weights is $(N, F)$, where $N$ is the batch</span>
<span class="sd">    size and $F$ is the number of features/columns in the dataset</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">SelfAttentionMLP</span>


<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">SelfAttentionMLP</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="o">=</span><span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines a <code>SelfAttentionMLP</code> model that can be used as the
deeptabular component of a Wide &amp; Deep model or independently by
itself.</p>
<p>This class combines embedding representations of the categorical features
with numerical (aka continuous) features that are also embedded. These
are then passed through a series of attention blocks. Each attention
block is comprised by what we would refer as a simplified
<code>SelfAttentionEncoder</code>. See
<code>pytorch_widedeep.models.tabular.mlp._attention_layers</code> for details. The
reason to use a simplified version of self attention is because we
observed that the '<em>standard</em>' attention mechanism used in the
TabTransformer has a notable tendency to overfit.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the model. Required to slice the tensors. e.g.
<em>{'education': 0, 'relationship': 1, 'workclass': 2, ...}</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>)
        – <p>List of Tuples with the column name and number of unique values per
categorical column e.g. <em>[(education, 11), ...]</em>.</p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings, if any. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>full_embed_dropout</b>
            (<code>bool</code>)
        – <p>Boolean indicating if an entire embedding (i.e. the representation of
one column) will be dropped in the batch. See:
<code>pytorch_widedeep.models.embeddings_layers.FullEmbeddingDropout</code>.
If full_embed_dropout = True, <code>cat_embed_dropout</code> is ignored.</p>
      </li>
      <li class="field-body">
        <b>shared_embed</b>
            (<code>bool</code>)
        – <p>The of sharing part of the embeddings per column is to enable the
model to distinguish the classes in one column from those in the
other columns. In other words, the idea is to let the model learn
which column is embedded at the time.</p>
      </li>
      <li class="field-body">
        <b>add_shared_embed</b>
            (<code>bool</code>)
        – <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
frac_shared_embed with the shared embeddings.
See <code>pytorch_widedeep.models.embeddings_layers.SharedEmbeddings</code></p>
      </li>
      <li class="field-body">
        <b>frac_shared_embed</b>
            (<code>float</code>)
        – <p>The fraction of embeddings that will be shared (if add_shared_embed
= False) by all the different categories for one particular
column.</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: <em>'layernorm'</em>, <em>'batchnorm'</em> or None.</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Continuous embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function to be applied to the continuous embeddings, if
any. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>input_dim</b>
            (<code>int</code>)
        – <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
      </li>
      <li class="field-body">
        <b>attn_dropout</b>
            (<code>float</code>)
        – <p>Dropout for each attention block</p>
      </li>
      <li class="field-body">
        <b>n_heads</b>
            (<code>int</code>)
        – <p>Number of attention heads per attention block.</p>
      </li>
      <li class="field-body">
        <b>use_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to use bias in the Q, K projection
layers.</p>
      </li>
      <li class="field-body">
        <b>with_addnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating if residual connections will be used in the attention blocks</p>
      </li>
      <li class="field-body">
        <b>attn_activation</b>
            (<code>str</code>)
        – <p>String indicating the activation function to be applied to the dense
layer in each attention encoder. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em>
and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>n_blocks</b>
            (<code>int</code>)
        – <p>Number of attention blocks</p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Sequence of attention encoders.</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">SelfAttentionMLP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SelfAttentionMLP</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/self_attention_mlp.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttentionMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_activation</span> <span class="o">=</span> <span class="n">attn_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Attention Blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;attention_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">SelfAttentionEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">with_addnorm</span><span class="p">,</span>
                <span class="n">attn_activation</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the WideDeep class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/self_attention_mlp.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the WideDeep class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span>
        <span class="k">else</span> <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.attention_weights" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.mlp.self_attention_mlp.SelfAttentionMLP.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">attention_weights</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/mlp/self_attention_mlp.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">    The shape of the attention weights is $(N, H, F, F)$, where $N$ is the</span>
<span class="sd">    batch size, $H$ is the number of attention heads and $F$ is the</span>
<span class="sd">    number of features/columns in the dataset</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">TabTransformer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">TabTransformer</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines a <a href="https://arxiv.org/abs/2012.06678">TabTransformer model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>:
This is an enhanced adaptation of the model described in the paper,
containing a series of additional features.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the model. Required to slice the tensors. e.g.
<em>{'education': 0, 'relationship': 1, 'workclass': 2, ...}</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>)
        – <p>List of Tuples with the column name and number of unique values for
each categorical component e.g. <em>[(education, 11), ...]</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings, if any. <em>'tanh'</em>,
<em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>full_embed_dropout</b>
            (<code>bool</code>)
        – <p>Boolean indicating if an entire embedding (i.e. the representation of
one column) will be dropped in the batch. See:
<code>pytorch_widedeep.models.transformers._layers.FullEmbeddingDropout</code>.
If <code>full_embed_dropout = True</code>, <code>cat_embed_dropout</code> is ignored.</p>
      </li>
      <li class="field-body">
        <b>shared_embed</b>
            (<code>bool</code>)
        – <p>The idea behind <code>shared_embed</code> is described in the Appendix A in the
<a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>: the
goal of having column embedding is to enable the model to distinguish
the classes in one column from those in the other columns. In other
words, the idea is to let the model learn which column is embedded at
the time.</p>
      </li>
      <li class="field-body">
        <b>add_shared_embed</b>
            (<code>bool</code>)
        – <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code></p>
      </li>
      <li class="field-body">
        <b>frac_shared_embed</b>
            (<code>float</code>)
        – <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column.</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: <em>'layernorm'</em>, <em>'batchnorm'</em> or None.</p>
      </li>
      <li class="field-body">
        <b>embed_continuous</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the continuous columns will be embedded
(i.e. passed each through a linear layer with or without activation)</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Continuous embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function to be applied to the continuous embeddings, if
any. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>input_dim</b>
            (<code>int</code>)
        – <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
      </li>
      <li class="field-body">
        <b>n_heads</b>
            (<code>int</code>)
        – <p>Number of attention heads per Transformer block</p>
      </li>
      <li class="field-body">
        <b>use_qkv_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers.</p>
      </li>
      <li class="field-body">
        <b>n_blocks</b>
            (<code>int</code>)
        – <p>Number of Transformer blocks</p>
      </li>
      <li class="field-body">
        <b>attn_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the Multi-Head Attention layers</p>
      </li>
      <li class="field-body">
        <b>ff_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the FeedForward network</p>
      </li>
      <li class="field-body">
        <b>transformer_activation</b>
            (<code>str</code>)
        – <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>MLP hidden dimensions. If not provided it will default to <span class="arithmatex">\([l,
4\times l, 2 \times l]\)</span> where <span class="arithmatex">\(l\)</span> is the MLP's input dimension</p>
      </li>
      <li class="field-body">
        <b>mlp_activation</b>
            (<code>str</code>)
        – <p>MLP activation function. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and
<em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the final MLP</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Sequence of Transformer blocks</p>
      </li>
      <li class="field-body">
        <b>mlp</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>MLP component in the model</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabTransformer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_transformer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_continuous</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="n">embed_continuous</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_continuous</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;If only continuous features are used &#39;embed_continuous&#39; must be set to &#39;True&#39;&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;transformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">TransformerEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">use_qkv_bias</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">transformer_activation</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_first_hidden_dim</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">mlp_activation</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
            <span class="n">mlp_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_transformer.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.attention_weights" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_transformer.TabTransformer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">attention_weights</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F, F)\)</span>, where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_transformer.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">    The shape of the attention weights is $(N, H, F, F)$, where $N$ is the</span>
<span class="sd">    batch size, $H$ is the number of attention heads and $F$ is the</span>
<span class="sd">    number of features/columns in the dataset</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">SAINT</span>


<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">SAINT</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines a <a href="https://arxiv.org/abs/2106.01342">SAINT model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: This is an slightly modified and enhanced
 version of the model described in the paper,</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the model. Required to slice the tensors. e.g.
<em>{'education': 0, 'relationship': 1, 'workclass': 2, ...}</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>)
        – <p>List of Tuples with the column name and number of unique values and
embedding dimension. e.g. <em>[(education, 11), ...]</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings, if any. <em>'tanh'</em>,
<em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>full_embed_dropout</b>
            (<code>bool</code>)
        – <p>Boolean indicating if an entire embedding (i.e. the representation of
one column) will be dropped in the batch. See:
<code>pytorch_widedeep.models.transformers._layers.FullEmbeddingDropout</code>.
If <code>full_embed_dropout = True</code>, <code>cat_embed_dropout</code> is ignored.</p>
      </li>
      <li class="field-body">
        <b>shared_embed</b>
            (<code>bool</code>)
        – <p>The idea behind <code>shared_embed</code> is described in the Appendix A in the
<a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>: the
goal of having column embedding is to enable the model to distinguish
the classes in one column from those in the other columns. In other
words, the idea is to let the model learn which column is embedded
at the time.</p>
      </li>
      <li class="field-body">
        <b>add_shared_embed</b>
            (<code>bool</code>)
        – <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code></p>
      </li>
      <li class="field-body">
        <b>frac_shared_embed</b>
            (<code>float</code>)
        – <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column.</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: <em>'layernorm'</em>, <em>'batchnorm'</em> or None.</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Continuous embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function to be applied to the continuous embeddings, if
any. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>input_dim</b>
            (<code>int</code>)
        – <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
      </li>
      <li class="field-body">
        <b>n_heads</b>
            (<code>int</code>)
        – <p>Number of attention heads per Transformer block</p>
      </li>
      <li class="field-body">
        <b>use_qkv_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
      </li>
      <li class="field-body">
        <b>n_blocks</b>
            (<code>int</code>)
        – <p>Number of SAINT-Transformer blocks.</p>
      </li>
      <li class="field-body">
        <b>attn_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the Multi-Head Attention column and
row layers</p>
      </li>
      <li class="field-body">
        <b>ff_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the FeedForward network</p>
      </li>
      <li class="field-body">
        <b>transformer_activation</b>
            (<code>str</code>)
        – <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>MLP hidden dimensions. If not provided it will default to <span class="arithmatex">\([l, 4
\times l, 2 \times l]\)</span> where <span class="arithmatex">\(l\)</span> is the MLP's input dimension</p>
      </li>
      <li class="field-body">
        <b>mlp_activation</b>
            (<code>str</code>)
        – <p>MLP activation function. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and
<em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the final MLP</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Sequence of SAINT-Transformer blocks</p>
      </li>
      <li class="field-body">
        <b>mlp</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>MLP component in the model</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">SAINT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SAINT</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/saint.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SAINT</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;saint_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">SaintEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">use_qkv_bias</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">mlp_activation</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
            <span class="n">mlp_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/saint.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.saint.SAINT.attention_weights" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.saint.SAINT.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">attention_weights</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights. Each element of the list is a tuple
where the first and the second elements are the column and row
attention weights respectively</p>
<p>The shape of the attention weights is:</p>
<ul>
<li>
<p>column attention: <span class="arithmatex">\((N, H, F, F)\)</span></p>
</li>
<li>
<p>row attention: <span class="arithmatex">\((1, H, N, N)\)</span></p>
</li>
</ul>
<p>where <span class="arithmatex">\(N\)</span> is the batch size, <span class="arithmatex">\(H\)</span> is the number of heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/saint.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights. Each element of the list is a tuple</span>
<span class="sd">    where the first and the second elements are the column and row</span>
<span class="sd">    attention weights respectively</span>

<span class="sd">    The shape of the attention weights is:</span>

<span class="sd">    - column attention: $(N, H, F, F)$</span>

<span class="sd">    - row attention: $(1, H, N, N)$</span>

<span class="sd">    where $N$ is the batch size, $H$ is the number of heads and $F$ is the</span>
<span class="sd">    number of features/columns in the dataset</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
        <span class="n">attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">(</span><span class="n">blk</span><span class="o">.</span><span class="n">col_attn</span><span class="o">.</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">blk</span><span class="o">.</span><span class="n">row_attn</span><span class="o">.</span><span class="n">attn_weights</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_weights</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">FTTransformer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">FTTransformer</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">kv_compression_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">kv_sharing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;reglu&quot;</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="o">=</span><span class="mf">1.33</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines a <a href="https://arxiv.org/abs/2106.11959">FTTransformer model</a> that
can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model or
independently by itself.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the model. Required to slice the tensors. e.g.
<em>{'education': 0, 'relationship': 1, 'workclass': 2, ...}</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>)
        – <p>List of Tuples with the column name and number of unique values for
each categorical component e.g. <em>[(education, 11), ...]</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings, if any. <em>'tanh'</em>,
<em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>full_embed_dropout</b>
            (<code>bool</code>)
        – <p>Boolean indicating if an entire embedding (i.e. the representation of
one column) will be dropped in the batch. See:
<code>pytorch_widedeep.models.transformers._layers.FullEmbeddingDropout</code>.
If <code>full_embed_dropout = True</code>, <code>cat_embed_dropout</code> is ignored.</p>
      </li>
      <li class="field-body">
        <b>shared_embed</b>
            (<code>bool</code>)
        – <p>The idea behind <code>shared_embed</code> is described in the Appendix A in the
<a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>: the
goal of having column embedding is to enable the model to distinguish
the classes in one column from those in the other columns. In other
words, the idea is to let the model learn which column is embedded
at the time.</p>
      </li>
      <li class="field-body">
        <b>add_shared_embed</b>
            (<code>bool</code>)
        – <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code></p>
      </li>
      <li class="field-body">
        <b>frac_shared_embed</b>
            (<code>float</code>)
        – <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column.</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: 'layernorm', 'batchnorm' or None.</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Continuous embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function to be applied to the continuous embeddings, if
any. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>input_dim</b>
            (<code>int</code>)
        – <p>The so-called <em>dimension of the model</em>. Is the number of embeddings used to encode
the categorical and/or continuous columns.</p>
      </li>
      <li class="field-body">
        <b>kv_compression_factor</b>
            (<code>float</code>)
        – <p>By default, the FTTransformer uses Linear Attention
(See <a href="https://arxiv.org/abs/2006.04768&gt;">Linformer: Self-Attention with Linear Complexity</a> ).
The compression factor that will be used to reduce the input sequence
length. If we denote the resulting sequence length as
<span class="arithmatex">\(k = int(kv_{compression \space factor} \times s)\)</span>
where <span class="arithmatex">\(s\)</span> is the input sequence length.</p>
      </li>
      <li class="field-body">
        <b>kv_sharing</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the <span class="arithmatex">\(E\)</span> and <span class="arithmatex">\(F\)</span> projection matrices
will share weights.  See <a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear
Complexity</a> for details</p>
      </li>
      <li class="field-body">
        <b>n_heads</b>
            (<code>int</code>)
        – <p>Number of attention heads per FTTransformer block</p>
      </li>
      <li class="field-body">
        <b>use_qkv_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
      </li>
      <li class="field-body">
        <b>n_blocks</b>
            (<code>int</code>)
        – <p>Number of FTTransformer blocks</p>
      </li>
      <li class="field-body">
        <b>attn_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the Linear-Attention layers</p>
      </li>
      <li class="field-body">
        <b>ff_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the FeedForward network</p>
      </li>
      <li class="field-body">
        <b>transformer_activation</b>
            (<code>str</code>)
        – <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>ff_factor</b>
            (<code>float</code>)
        – <p>Multiplicative factor applied to the first layer of the FF network in
each Transformer block, This is normally set to 4, but they use 4/3
in the paper.</p>
      </li>
      <li class="field-body">
        <b>mlp_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>MLP hidden dimensions. If not provided no MLP on top of the final
FTTransformer block will be used</p>
      </li>
      <li class="field-body">
        <b>mlp_activation</b>
            (<code>str</code>)
        – <p>MLP activation function. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and
<em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the final MLP</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Sequence of FTTransformer blocks</p>
      </li>
      <li class="field-body">
        <b>mlp</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>MLP component in the model</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">FTTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FTTransformer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/ft_transformer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">kv_compression_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">kv_sharing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reglu&quot;</span><span class="p">,</span>
    <span class="n">ff_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.33</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FTTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">kv_compression_factor</span> <span class="o">=</span> <span class="n">kv_compression_factor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kv_sharing</span> <span class="o">=</span> <span class="n">kv_sharing</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_qkv_bias</span> <span class="o">=</span> <span class="n">use_qkv_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_factor</span> <span class="o">=</span> <span class="n">ff_factor</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="n">is_first</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
            <span class="s2">&quot;fttransformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="n">FTTransformerEncoder</span><span class="p">(</span>
                <span class="n">input_dim</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="p">,</span>
                <span class="n">use_qkv_bias</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">ff_dropout</span><span class="p">,</span>
                <span class="n">kv_compression_factor</span><span class="p">,</span>
                <span class="n">kv_sharing</span><span class="p">,</span>
                <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="n">ff_factor</span><span class="p">,</span>
                <span class="n">is_first</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">is_first</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Mlp</span>
    <span class="k">if</span> <span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">mlp_activation</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
            <span class="n">mlp_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/ft_transformer.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.attention_weights" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.ft_transformer.FTTransformer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">attention_weights</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is: <span class="arithmatex">\((N, H, F, k)\)</span>, where <span class="arithmatex">\(N\)</span> is
the batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads, <span class="arithmatex">\(F\)</span> is the
number of features/columns and <span class="arithmatex">\(k\)</span> is the reduced sequence length or
dimension, i.e. <span class="arithmatex">\(k = int(kv_{compression \space factor} \times s)\)</span></p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/ft_transformer.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">    The shape of the attention weights is: $(N, H, F, k)$, where $N$ is</span>
<span class="sd">    the batch size, $H$ is the number of attention heads, $F$ is the</span>
<span class="sd">    number of features/columns and $k$ is the reduced sequence length or</span>
<span class="sd">    dimension, i.e. $k = int(kv_{compression \space factor} \times s)$</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">TabPerceiver</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">TabPerceiver</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">n_cross_attns</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_cross_attn_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latents</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">n_latent_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latent_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_perceiver_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;geglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines an adaptation of a <a href="https://arxiv.org/abs/2103.03206">Perceiver</a>
 that can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model
 or independently by itself.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: while there are scientific publications for
 the <code>TabTransformer</code>, <code>SAINT</code> and <code>FTTransformer</code>, the <code>TabPerceiver</code>
 and the <code>TabFastFormer</code> are our own adaptations of the
 <a href="https://arxiv.org/abs/2103.03206">Perceiver</a> and the
 <a href="https://arxiv.org/abs/2108.09084">FastFormer</a> for tabular data.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the model. Required to slice the tensors. e.g.
<em>{'education': 0, 'relationship': 1, 'workclass': 2, ...}</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>)
        – <p>List of Tuples with the column name and number of unique values for
each categorical component e.g. <em>[(education, 11), ...]</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings, if any. <em>'tanh'</em>,
<em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>full_embed_dropout</b>
            (<code>bool</code>)
        – <p>Boolean indicating if an entire embedding (i.e. the representation of
one column) will be dropped in the batch. See:
<code>pytorch_widedeep.models.transformers._layers.FullEmbeddingDropout</code>.
If <code>full_embed_dropout = True</code>, <code>cat_embed_dropout</code> is ignored.</p>
      </li>
      <li class="field-body">
        <b>shared_embed</b>
            (<code>bool</code>)
        – <p>The idea behind <code>shared_embed</code> is described in the Appendix A in the
<a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>: the
goal of having column embedding is to enable the model to distinguish
the classes in one column from those in the other columns. In other
words, the idea is to let the model learn which column is embedded
at the time.</p>
      </li>
      <li class="field-body">
        <b>add_shared_embed</b>
            (<code>bool</code>)
        – <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code></p>
      </li>
      <li class="field-body">
        <b>frac_shared_embed</b>
            (<code>float</code>)
        – <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column.</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: <em>'layernorm'</em>, <em>'batchnorm'</em> or None.</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Continuous embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function to be applied to the continuous embeddings, if
any. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>input_dim</b>
            (<code>int</code>)
        – <p>The so-called <em>dimension of the model</em>. Is the number of embeddings
used to encode the categorical and/or continuous columns.</p>
      </li>
      <li class="field-body">
        <b>n_cross_attns</b>
            (<code>int</code>)
        – <p>Number of times each perceiver block will cross attend to the input
data (i.e. number of cross attention components per perceiver block).
This should normally be 1. However, in the paper they describe some
architectures (normally computer vision-related problems) where the
Perceiver attends multiple times to the input array. Therefore, maybe
multiple cross attention to the input array is also useful in some
cases for tabular data <img alt="🤷" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f937.png" title=":shrug:" /> .</p>
      </li>
      <li class="field-body">
        <b>n_cross_attn_heads</b>
            (<code>int</code>)
        – <p>Number of attention heads for the cross attention component</p>
      </li>
      <li class="field-body">
        <b>n_latents</b>
            (<code>int</code>)
        – <p>Number of latents. This is the <span class="arithmatex">\(N\)</span> parameter in the paper. As
indicated in the paper, this number should be significantly lower
than <span class="arithmatex">\(M\)</span> (the number of columns in the dataset). Setting <span class="arithmatex">\(N\)</span> closer
to <span class="arithmatex">\(M\)</span> defies the main purpose of the Perceiver, which is to overcome
the transformer quadratic bottleneck</p>
      </li>
      <li class="field-body">
        <b>latent_dim</b>
            (<code>int</code>)
        – <p>Latent dimension.</p>
      </li>
      <li class="field-body">
        <b>n_latent_heads</b>
            (<code>int</code>)
        – <p>Number of attention heads per Latent Transformer</p>
      </li>
      <li class="field-body">
        <b>n_latent_blocks</b>
            (<code>int</code>)
        – <p>Number of transformer encoder blocks (normalised MHA + normalised FF)
per Latent Transformer</p>
      </li>
      <li class="field-body">
        <b>n_perceiver_blocks</b>
            (<code>int</code>)
        – <p>Number of Perceiver blocks defined as [Cross Attention + Latent
Transformer]</p>
      </li>
      <li class="field-body">
        <b>share_weights</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the weights will be shared between Perceiver
blocks</p>
      </li>
      <li class="field-body">
        <b>attn_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the Multi-Head Attention layers</p>
      </li>
      <li class="field-body">
        <b>ff_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the FeedForward network</p>
      </li>
      <li class="field-body">
        <b>transformer_activation</b>
            (<code>str</code>)
        – <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>MLP hidden dimensions. If not provided it will default to <span class="arithmatex">\([l, 4
\times l, 2 \times l]\)</span> where <span class="arithmatex">\(l\)</span> is the MLP's input dimension</p>
      </li>
      <li class="field-body">
        <b>mlp_activation</b>
            (<code>str</code>)
        – <p>MLP activation function. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and
<em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the final MLP</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.ModuleDict">ModuleDict</span></code>)
        – <p>ModuleDict with the Perceiver blocks</p>
      </li>
      <li class="field-body">
        <b>latents</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Parameter">Parameter</span></code>)
        – <p>Latents that will be used for prediction</p>
      </li>
      <li class="field-body">
        <b>mlp</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>MLP component in the model</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabPerceiver</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabPerceiver</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
<span class="gp">... </span><span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span> <span class="n">n_latents</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span><span class="n">n_perceiver_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_perceiver.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">n_cross_attns</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n_cross_attn_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latents</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">n_latent_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_latent_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_perceiver_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;geglu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabPerceiver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attns</span> <span class="o">=</span> <span class="n">n_cross_attns</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cross_attn_heads</span> <span class="o">=</span> <span class="n">n_cross_attn_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_latents</span> <span class="o">=</span> <span class="n">n_latents</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="n">latent_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_latent_heads</span> <span class="o">=</span> <span class="n">n_latent_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_latent_blocks</span> <span class="o">=</span> <span class="n">n_latent_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_perceiver_blocks</span> <span class="o">=</span> <span class="n">n_perceiver_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span> <span class="o">=</span> <span class="n">share_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">latents</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">trunc_normal_</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_latents</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
    <span class="n">first_perceiver_block</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_perceiver_block</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block0&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">first_perceiver_block</span>

    <span class="k">if</span> <span class="n">share_weights</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_perceiver_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="n">first_perceiver_block</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_perceiver_blocks</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_perceiver_block</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span>

    <span class="c1"># Mlp</span>
    <span class="k">if</span> <span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">mlp_activation</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
            <span class="n">mlp_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_perceiver.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.attention_weights" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_perceiver.TabPerceiver.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">attention_weights</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights. If the weights are not shared
between perceiver blocks each element of the list will be a list
itself containing the Cross Attention and Latent Transformer
attention weights respectively</p>
<p>The shape of the attention weights is:</p>
<ul>
<li>
<p>Cross Attention: <span class="arithmatex">\((N, C, L, F)\)</span></p>
</li>
<li>
<p>Latent Attention: <span class="arithmatex">\((N, T, L, L)\)</span></p>
</li>
</ul>
<p>WHere <span class="arithmatex">\(N\)</span> is the batch size, <span class="arithmatex">\(C\)</span> is the number of Cross Attention
heads, <span class="arithmatex">\(L\)</span> is the number of Latents, <span class="arithmatex">\(F\)</span> is the number of
features/columns in the dataset and <span class="arithmatex">\(T\)</span> is the number of Latent
Attention heads</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_perceiver.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights. If the weights are not shared</span>
<span class="sd">    between perceiver blocks each element of the list will be a list</span>
<span class="sd">    itself containing the Cross Attention and Latent Transformer</span>
<span class="sd">    attention weights respectively</span>

<span class="sd">    The shape of the attention weights is:</span>

<span class="sd">    - Cross Attention: $(N, C, L, F)$</span>

<span class="sd">    - Latent Attention: $(N, T, L, L)$</span>

<span class="sd">    WHere $N$ is the batch size, $C$ is the number of Cross Attention</span>
<span class="sd">    heads, $L$ is the number of Latents, $F$ is the number of</span>
<span class="sd">    features/columns in the dataset and $T$ is the number of Latent</span>
<span class="sd">    Attention heads</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span><span class="p">:</span>
        <span class="n">cross_attns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block0&quot;</span><span class="p">][</span><span class="s2">&quot;cross_attns&quot;</span><span class="p">]</span>
        <span class="n">latent_transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block0&quot;</span><span class="p">][</span><span class="s2">&quot;latent_transformer&quot;</span><span class="p">]</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_attn_weights</span><span class="p">(</span>
            <span class="n">cross_attns</span><span class="p">,</span> <span class="n">latent_transformer</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_perceiver_blocks</span><span class="p">):</span>
            <span class="n">cross_attns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)][</span><span class="s2">&quot;cross_attns&quot;</span><span class="p">]</span>
            <span class="n">latent_transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;perceiver_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)][</span>
                <span class="s2">&quot;latent_transformer&quot;</span>
            <span class="p">]</span>
            <span class="n">attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_extract_attn_weights</span><span class="p">(</span><span class="n">cross_attns</span><span class="p">,</span> <span class="n">latent_transformer</span><span class="p">)</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">attention_weights</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">TabFastFormer</span>


<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">TabFastFormer</span><span class="p">(</span>
    <span class="n">column_idx</span><span class="p">,</span>
    <span class="n">cat_embed_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">share_qv_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="pytorch_widedeep.models.tabular._base_tabular_model.BaseTabularModelWithAttention">BaseTabularModelWithAttention</span></code></p>

  
      <p>Defines an adaptation of a <a href="https://arxiv.org/abs/2108.09084">FastFormer</a>
that can be used as the <code>deeptabular</code> component of a Wide &amp; Deep model
or independently by itself.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: while there are scientific publications for
 the <code>TabTransformer</code>, <code>SAINT</code> and <code>FTTransformer</code>, the <code>TabPerceiver</code>
 and the <code>TabFastFormer</code> are our own adaptations of the
 <a href="https://arxiv.org/abs/2103.03206">Perceiver</a> and the
 <a href="https://arxiv.org/abs/2108.09084">FastFormer</a> for tabular data.</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>column_idx</b>
            (<code><span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, int]</code>)
        – <p>Dict containing the index of the columns that will be passed through
the <code>TabFastFormer</code> model. Required to slice the tensors. e.g. <em>{'education':
0, 'relationship': 1, 'workclass': 2, ...}</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_input</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[<span title="pytorch_widedeep.wdtypes.Tuple">Tuple</span>[str, int]]]</code>)
        – <p>List of Tuples with the column name, number of unique values and
embedding dimension. e.g. <em>[(education, 11, 32), ...]</em></p>
      </li>
      <li class="field-body">
        <b>cat_embed_dropout</b>
            (<code>float</code>)
        – <p>Categorical embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cat_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>cat_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>Activation function for the categorical embeddings</p>
      </li>
      <li class="field-body">
        <b>full_embed_dropout</b>
            (<code>bool</code>)
        – <p>Boolean indicating if an entire embedding (i.e. the representation of
one column) will be dropped in the batch. See:
<code>pytorch_widedeep.models.transformers._layers.FullEmbeddingDropout</code>.
If <code>full_embed_dropout = True</code>, <code>cat_embed_dropout</code> is ignored.</p>
      </li>
      <li class="field-body">
        <b>shared_embed</b>
            (<code>bool</code>)
        – <p>The idea behind <code>shared_embed</code> is described in the Appendix A in the
<a href="https://arxiv.org/abs/2012.06678">TabTransformer paper</a>: the goal of
having column embedding is to enable the model to distinguish the
classes in one column from those in the other columns. In other
words, the idea is to let the model learn which column is embedded at
the time.</p>
      </li>
      <li class="field-body">
        <b>add_shared_embed</b>
            (<code>bool</code>)
        – <p>The two embedding sharing strategies are: 1) add the shared embeddings
to the column embeddings or 2) to replace the first
<code>frac_shared_embed</code> with the shared embeddings.
See <code>pytorch_widedeep.models.transformers._layers.SharedEmbeddings</code></p>
      </li>
      <li class="field-body">
        <b>frac_shared_embed</b>
            (<code>float</code>)
        – <p>The fraction of embeddings that will be shared (if <code>add_shared_embed
= False</code>) by all the different categories for one particular
column.</p>
      </li>
      <li class="field-body">
        <b>continuous_cols</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List with the name of the numeric (aka continuous) columns</p>
      </li>
      <li class="field-body">
        <b>cont_norm_layer</b>
            (<code>str</code>)
        – <p>Type of normalization layer applied to the continuous features. Options
are: <em>'layernorm'</em>, <em>'batchnorm'</em> or None.</p>
      </li>
      <li class="field-body">
        <b>cont_embed_dropout</b>
            (<code>float</code>)
        – <p>Continuous embeddings dropout</p>
      </li>
      <li class="field-body">
        <b>use_cont_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating if bias will be used for the continuous embeddings</p>
      </li>
      <li class="field-body">
        <b>cont_embed_activation</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[str]</code>)
        – <p>String indicating the activation function to be applied to the
continuous embeddings, if any. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and
<em>'gelu'</em> are supported.</p>
      </li>
      <li class="field-body">
        <b>input_dim</b>
            (<code>int</code>)
        – <p>The so-called <em>dimension of the model</em>. Is the number of
embeddings used to encode the categorical and/or continuous columns</p>
      </li>
      <li class="field-body">
        <b>n_heads</b>
            (<code>int</code>)
        – <p>Number of attention heads per FastFormer block</p>
      </li>
      <li class="field-body">
        <b>use_bias</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to use bias in the Q, K, and V
projection layers</p>
      </li>
      <li class="field-body">
        <b>n_blocks</b>
            (<code>int</code>)
        – <p>Number of FastFormer blocks</p>
      </li>
      <li class="field-body">
        <b>attn_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the Additive Attention layers</p>
      </li>
      <li class="field-body">
        <b>ff_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the FeedForward network</p>
      </li>
      <li class="field-body">
        <b>share_qv_weights</b>
            (<code>bool</code>)
        – <p>Following the paper, this is a boolean indicating if the Value (<span class="arithmatex">\(V\)</span>) and
the Query (<span class="arithmatex">\(Q\)</span>) transformation parameters will be shared.</p>
      </li>
      <li class="field-body">
        <b>share_weights</b>
            (<code>bool</code>)
        – <p>In addition to sharing the <span class="arithmatex">\(V\)</span> and <span class="arithmatex">\(Q\)</span> transformation parameters, the
parameters across different Fastformer layers can also be shared.
Please, see
<code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code> for
details</p>
      </li>
      <li class="field-body">
        <b>transformer_activation</b>
            (<code>str</code>)
        – <p>Transformer Encoder activation function. <em>'tanh'</em>, <em>'relu'</em>,
<em>'leaky_relu'</em>, <em>'gelu'</em>, <em>'geglu'</em> and <em>'reglu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>MLP hidden dimensions. If not provided it will default to <span class="arithmatex">\([l, 4
\times l, 2 \times l]\)</span> where <span class="arithmatex">\(l\)</span> is the MLP's input dimension</p>
      </li>
      <li class="field-body">
        <b>mlp_activation</b>
            (<code>str</code>)
        – <p>MLP activation function. <em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and
<em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>mlp_dropout</b>
            (<code>float</code>)
        – <p>Dropout that will be applied to the final MLP</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>mlp_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>cat_and_cont_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>This is the module that processes the categorical and continuous columns</p>
      </li>
      <li class="field-body">
        <b>encoder</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Sequence of FasFormer blocks.</p>
      </li>
      <li class="field-body">
        <b>mlp</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>MLP component in the model</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabFastFormer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_tab</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cat_embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colnames</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colnames</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TabFastFormer</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tab</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">column_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">cat_embed_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cat_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cat_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cat_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_embed_dropout</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">add_shared_embed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">frac_shared_embed</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_norm_layer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cont_embed_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_cont_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">cont_embed_activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">share_qv_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">share_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">transformer_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mlp_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TabFastFormer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span>
        <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">cat_embed_input</span><span class="p">,</span>
        <span class="n">cat_embed_dropout</span><span class="o">=</span><span class="n">cat_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cat_bias</span><span class="o">=</span><span class="n">use_cat_bias</span><span class="p">,</span>
        <span class="n">cat_embed_activation</span><span class="o">=</span><span class="n">cat_embed_activation</span><span class="p">,</span>
        <span class="n">full_embed_dropout</span><span class="o">=</span><span class="n">full_embed_dropout</span><span class="p">,</span>
        <span class="n">shared_embed</span><span class="o">=</span><span class="n">shared_embed</span><span class="p">,</span>
        <span class="n">add_shared_embed</span><span class="o">=</span><span class="n">add_shared_embed</span><span class="p">,</span>
        <span class="n">frac_shared_embed</span><span class="o">=</span><span class="n">frac_shared_embed</span><span class="p">,</span>
        <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">,</span>
        <span class="n">cont_norm_layer</span><span class="o">=</span><span class="n">cont_norm_layer</span><span class="p">,</span>
        <span class="n">embed_continuous</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">cont_embed_dropout</span><span class="o">=</span><span class="n">cont_embed_dropout</span><span class="p">,</span>
        <span class="n">use_cont_bias</span><span class="o">=</span><span class="n">use_cont_bias</span><span class="p">,</span>
        <span class="n">cont_embed_activation</span><span class="o">=</span><span class="n">cont_embed_activation</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_dropout</span> <span class="o">=</span> <span class="n">ff_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">share_qv_weights</span> <span class="o">=</span> <span class="n">share_qv_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span> <span class="o">=</span> <span class="n">share_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer_activation</span> <span class="o">=</span> <span class="n">transformer_activation</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="o">=</span> <span class="n">mlp_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_activation</span> <span class="o">=</span> <span class="n">mlp_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dropout</span> <span class="o">=</span> <span class="n">mlp_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="n">mlp_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_batchnorm_last</span> <span class="o">=</span> <span class="n">mlp_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="n">mlp_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="o">=</span> <span class="s2">&quot;cls_token&quot;</span> <span class="ow">in</span> <span class="n">column_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_embed_input</span><span class="p">)</span> <span class="k">if</span> <span class="n">cat_embed_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_cols</span><span class="p">)</span> <span class="k">if</span> <span class="n">continuous_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_cont</span>

    <span class="c1"># Embeddings are instantiated at the base model</span>
    <span class="c1"># Transformer blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">first_fastformer_block</span> <span class="o">=</span> <span class="n">FastFormerEncoder</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">,</span>
        <span class="n">ff_dropout</span><span class="p">,</span>
        <span class="n">share_qv_weights</span><span class="p">,</span>
        <span class="n">transformer_activation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;fastformer_block0&quot;</span><span class="p">,</span> <span class="n">first_fastformer_block</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_blocks</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">share_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;fastformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">first_fastformer_block</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;fastformer_block&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="n">FastFormerEncoder</span><span class="p">(</span>
                    <span class="n">input_dim</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="p">,</span>
                    <span class="n">use_bias</span><span class="p">,</span>
                    <span class="n">attn_dropout</span><span class="p">,</span>
                    <span class="n">ff_dropout</span><span class="p">,</span>
                    <span class="n">share_qv_weights</span><span class="p">,</span>
                    <span class="n">transformer_activation</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_cls_token</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feats</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Mlp</span>
    <span class="k">if</span> <span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">mlp_hidden_dims</span><span class="p">,</span>
            <span class="n">mlp_activation</span><span class="p">,</span>
            <span class="n">mlp_dropout</span><span class="p">,</span>
            <span class="n">mlp_batchnorm</span><span class="p">,</span>
            <span class="n">mlp_batchnorm_last</span><span class="p">,</span>
            <span class="n">mlp_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_first_hidden_dim</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.attention_weights" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.tabular.transformers.tab_fastformer.TabFastFormer.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">attention_weights</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights. Each element of the list is a
tuple where the first and second elements are the <span class="arithmatex">\(\alpha\)</span>
and <span class="arithmatex">\(\beta\)</span> attention weights in the paper.</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, H, F)\)</span> where <span class="arithmatex">\(N\)</span> is the
batch size, <span class="arithmatex">\(H\)</span> is the number of attention heads and <span class="arithmatex">\(F\)</span> is the
number of features/columns in the dataset</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/tabular/transformers/tab_fastformer.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights. Each element of the list is a</span>
<span class="sd">    tuple where the first and second elements are the $\alpha$</span>
<span class="sd">    and $\beta$ attention weights in the paper.</span>

<span class="sd">    The shape of the attention weights is $(N, H, F)$ where $N$ is the</span>
<span class="sd">    batch size, $H$ is the number of attention heads and $F$ is the</span>
<span class="sd">    number of features/columns in the dataset</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">share_weights</span><span class="p">:</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weight</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">attention_weights</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div><p><br/></p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: when we started developing the library we
 thought that combining Deep Learning architectures for tabular data, with
 CNN-based architectures (pretrained or not) for images and Transformer-based
 architectures for text would be an <em>'overkill'</em>  (also, pretrained
 transformer-based models were not as readily available as they are today).
 Therefore, at that time we made the decision of including in the library
 simple RNN-based architectures for the text dataset. A lot has passed since
 then and it is our intention to integrate this library with the
 <a href="https://huggingface.co/docs/transformers/main/en/index">Hugginface's Transformers library</a>
 in the near future. Nonetheless, note that it is still possible to use any
 custom model as the <code>deeptext</code> component using this library. Please, see the
 example section in this documentation for details</p>


<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.attentive_rnn.BasicRNN" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">BasicRNN</span>


<a href="#pytorch_widedeep.models.text.attentive_rnn.BasicRNN" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">BasicRNN</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Standard text classifier/regressor comprised by a stack of RNNs
(LSTMs or GRUs) that can be used as the <code>deeptext</code> component of a Wide &amp;
Deep model or independently by itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of
dense layers on top of the stack of RNNs</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>vocab_size</b>
            (<code>int</code>)
        – <p>Number of words in the vocabulary</p>
      </li>
      <li class="field-body">
        <b>embed_dim</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>)
        – <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
      </li>
      <li class="field-body">
        <b>embed_matrix</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>]</code>)
        – <p>Pretrained word embeddings</p>
      </li>
      <li class="field-body">
        <b>embed_trainable</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the pretrained embeddings are trainable</p>
      </li>
      <li class="field-body">
        <b>rnn_type</b>
            (<code>str</code>)
        – <p>String indicating the type of RNN to use. One of <em>'lstm'</em> or <em>'gru'</em></p>
      </li>
      <li class="field-body">
        <b>hidden_dim</b>
            (<code>int</code>)
        – <p>Hidden dim of the RNN</p>
      </li>
      <li class="field-body">
        <b>n_layers</b>
            (<code>int</code>)
        – <p>Number of recurrent layers</p>
      </li>
      <li class="field-body">
        <b>rnn_dropout</b>
            (<code>float</code>)
        – <p>Dropout for each RNN layer except the last layer</p>
      </li>
      <li class="field-body">
        <b>bidirectional</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the staked RNNs are bidirectional</p>
      </li>
      <li class="field-body">
        <b>use_hidden_state</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether to use the final hidden state or the RNN's
output as predicting features. Typically the former is used.</p>
      </li>
      <li class="field-body">
        <b>padding_idx</b>
            (<code>int</code>)
        – <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's tokenizer
where the token index 0 is reserved for the <em>'unknown'</em> word token.
Therefore, the default value is set to 1.</p>
      </li>
      <li class="field-body">
        <b>head_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
      </li>
      <li class="field-body">
        <b>head_activation</b>
            (<code>str</code>)
        – <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>head_dropout</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>)
        – <p>Dropout of the dense layers in the head</p>
      </li>
      <li class="field-body">
        <b>head_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
      </li>
      <li class="field-body">
        <b>head_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
      </li>
      <li class="field-body">
        <b>head_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>word_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>word embedding matrix</p>
      </li>
      <li class="field-body">
        <b>rnn</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Stack of RNNs</p>
      </li>
      <li class="field-body">
        <b>rnn_mlp</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not None</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">BasicRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/text/basic_rnn.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BasicRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;the input embedding dimension </span><span class="si">{}</span><span class="s2"> and the dimension of the &quot;</span>
            <span class="s2">&quot;pretrained embeddings </span><span class="si">{}</span><span class="s2"> do not match. The pretrained embeddings &quot;</span>
            <span class="s2">&quot;dimension (</span><span class="si">{}</span><span class="s2">) will be used&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">),</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span> <span class="o">=</span> <span class="n">embed_trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_dropout</span> <span class="o">=</span> <span class="n">rnn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_hidden_state</span> <span class="o">=</span> <span class="n">use_hidden_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

    <span class="c1"># Embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_embeddings</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">)</span>

    <span class="c1"># RNN</span>
    <span class="n">rnn_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="s2">&quot;num_layers&quot;</span><span class="p">:</span> <span class="n">n_layers</span><span class="p">,</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">,</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">rnn_dropout</span><span class="p">,</span>
        <span class="s2">&quot;batch_first&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bidirectional</span> <span class="k">else</span> <span class="n">hidden_dim</span>

    <span class="c1"># FC-Head (Mlp)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MLP</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># simple hack to add readability in the forward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.text.basic_rnn.BasicRNN.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.basic_rnn.BasicRNN.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/text/basic_rnn.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.attentive_rnn.AttentiveRNN" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">AttentiveRNN</span>


<a href="#pytorch_widedeep.models.text.attentive_rnn.AttentiveRNN" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">AttentiveRNN</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><a class="autorefs autorefs-internal" title="pytorch_widedeep.models.text.basic_rnn.BasicRNN" href="#pytorch_widedeep.models.text.attentive_rnn.BasicRNN">BasicRNN</a></code></p>

  
      <p>Text classifier/regressor comprised by a stack of RNNs
(LSTMs or GRUs) plus an attention layer. This model can be used as the
<code>deeptext</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of dense
layers on top of attention layer</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>vocab_size</b>
            (<code>int</code>)
        – <p>Number of words in the vocabulary</p>
      </li>
      <li class="field-body">
        <b>embed_dim</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>)
        – <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
      </li>
      <li class="field-body">
        <b>embed_matrix</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>]</code>)
        – <p>Pretrained word embeddings</p>
      </li>
      <li class="field-body">
        <b>embed_trainable</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the pretrained embeddings are trainable</p>
      </li>
      <li class="field-body">
        <b>rnn_type</b>
            (<code>str</code>)
        – <p>String indicating the type of RNN to use. One of <em>'lstm'</em> or <em>'gru'</em></p>
      </li>
      <li class="field-body">
        <b>hidden_dim</b>
            (<code>int</code>)
        – <p>Hidden dim of the RNN</p>
      </li>
      <li class="field-body">
        <b>n_layers</b>
            (<code>int</code>)
        – <p>Number of recurrent layers</p>
      </li>
      <li class="field-body">
        <b>rnn_dropout</b>
            (<code>float</code>)
        – <p>Dropout for each RNN layer except the last layer</p>
      </li>
      <li class="field-body">
        <b>bidirectional</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the staked RNNs are bidirectional</p>
      </li>
      <li class="field-body">
        <b>use_hidden_state</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether to use the final hidden state or the RNN's
output as predicting features. Typically the former is used.</p>
      </li>
      <li class="field-body">
        <b>padding_idx</b>
            (<code>int</code>)
        – <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's
tokenizer where the token index 0 is reserved for the <em>'unknown'</em>
word token. Therefore, the default value is set to 1.</p>
      </li>
      <li class="field-body">
        <b>attn_concatenate</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the input to the attention mechanism will be the
output of the RNN or the output of the RNN concatenated with the last
hidden state.</p>
      </li>
      <li class="field-body">
        <b>attn_dropout</b>
            (<code>float</code>)
        – <p>Internal dropout for the attention mechanism</p>
      </li>
      <li class="field-body">
        <b>head_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>List with the sizes of the dense layers in the head e.g: <em>[128, 64]</em></p>
      </li>
      <li class="field-body">
        <b>head_activation</b>
            (<code>str</code>)
        – <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>head_dropout</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>)
        – <p>Dropout of the dense layers in the head</p>
      </li>
      <li class="field-body">
        <b>head_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
      </li>
      <li class="field-body">
        <b>head_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
      </li>
      <li class="field-body">
        <b>head_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>word_embed</b>
            (<code>nn.<span title="nn.Module">Module</span></code>)
        – <p>word embedding matrix</p>
      </li>
      <li class="field-body">
        <b>rnn</b>
            (<code>nn.<span title="nn.Module">Module</span></code>)
        – <p>Stack of RNNs</p>
      </li>
      <li class="field-body">
        <b>rnn_mlp</b>
            (<code>nn.<span title="nn.Module">Module</span></code>)
        – <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not <code>None</code></p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">AttentiveRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AttentiveRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/text/attentive_rnn.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_hidden_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">embed_matrix</span><span class="o">=</span><span class="n">embed_matrix</span><span class="p">,</span>
        <span class="n">embed_trainable</span><span class="o">=</span><span class="n">embed_trainable</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
        <span class="n">rnn_dropout</span><span class="o">=</span><span class="n">rnn_dropout</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">use_hidden_state</span><span class="o">=</span><span class="n">use_hidden_state</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="o">=</span><span class="n">head_hidden_dims</span><span class="p">,</span>
        <span class="n">head_activation</span><span class="o">=</span><span class="n">head_activation</span><span class="p">,</span>
        <span class="n">head_dropout</span><span class="o">=</span><span class="n">head_dropout</span><span class="p">,</span>
        <span class="n">head_batchnorm</span><span class="o">=</span><span class="n">head_batchnorm</span><span class="p">,</span>
        <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="n">head_batchnorm_last</span><span class="p">,</span>
        <span class="n">head_linear_first</span><span class="o">=</span><span class="n">head_linear_first</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Embeddings and RNN defined in the BasicRNN inherited class</span>

    <span class="c1"># Attention</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_concatenate</span> <span class="o">=</span> <span class="n">attn_concatenate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>

    <span class="k">if</span> <span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="k">elif</span> <span class="n">bidirectional</span> <span class="ow">or</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">ContextAttention</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="p">,</span> <span class="n">sum_along_seq</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># FC-Head (Mlp)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.text.attentive_rnn.AttentiveRNN.attention_weights" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.attentive_rnn.AttentiveRNN.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">attention_weights</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, S)\)</span>, where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(S\)</span> is the length of the sequence</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/text/attentive_rnn.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights</span>

<span class="sd">    The shape of the attention weights is $(N, S)$, where $N$ is the batch</span>
<span class="sd">    size and $S$ is the length of the sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.text.stacked_attentive_rnn.StackedAttentiveRNN" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">StackedAttentiveRNN</span>


<a href="#pytorch_widedeep.models.text.stacked_attentive_rnn.StackedAttentiveRNN" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">StackedAttentiveRNN</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Text classifier/regressor comprised by a stack of blocks:
<code>[RNN + Attention]</code>. This can be used as the <code>deeptext</code> component of a
Wide &amp; Deep model or independently by itself.</p>
<p>In addition, there is the option to add a Fully Connected (FC) set of
dense layers on top of the attentiob blocks</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>vocab_size</b>
            (<code>int</code>)
        – <p>Number of words in the vocabulary</p>
      </li>
      <li class="field-body">
        <b>embed_dim</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>)
        – <p>Dimension of the word embeddings if non-pretained word vectors are
used</p>
      </li>
      <li class="field-body">
        <b>embed_matrix</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="numpy">np</span>.<span title="numpy.ndarray">ndarray</span>]</code>)
        – <p>Pretrained word embeddings</p>
      </li>
      <li class="field-body">
        <b>embed_trainable</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the pretrained embeddings are trainable</p>
      </li>
      <li class="field-body">
        <b>rnn_type</b>
            (<code>str</code>)
        – <p>String indicating the type of RNN to use. One of 'lstm' or 'gru'</p>
      </li>
      <li class="field-body">
        <b>hidden_dim</b>
            (<code>int</code>)
        – <p>Hidden dim of the RNN</p>
      </li>
      <li class="field-body">
        <b>bidirectional</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the staked RNNs are bidirectional</p>
      </li>
      <li class="field-body">
        <b>padding_idx</b>
            (<code>int</code>)
        – <p>index of the padding token in the padded-tokenised sequences. The
<code>TextPreprocessor</code> class within this library uses fastai's
tokenizer where the token index 0 is reserved for the <em>'unknown'</em>
word token. Therefore, the default value is set to 1.</p>
      </li>
      <li class="field-body">
        <b>n_blocks</b>
            (<code>int</code>)
        – <p>Number of attention blocks. Each block is comprised by an RNN and a
Context Attention Encoder</p>
      </li>
      <li class="field-body">
        <b>attn_concatenate</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the input to the attention mechanism will be the
output of the RNN or the output of the RNN concatenated with the last
hidden state or simply</p>
      </li>
      <li class="field-body">
        <b>attn_dropout</b>
            (<code>float</code>)
        – <p>Internal dropout for the attention mechanism</p>
      </li>
      <li class="field-body">
        <b>with_addnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the output of each block will be added to the
input and normalised</p>
      </li>
      <li class="field-body">
        <b>head_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>List with the sizes of the dense layers in the head e.g: [128, 64]</p>
      </li>
      <li class="field-body">
        <b>head_activation</b>
            (<code>str</code>)
        – <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>head_dropout</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>)
        – <p>Dropout of the dense layers in the head</p>
      </li>
      <li class="field-body">
        <b>head_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <em>'rnn_mlp'</em></p>
      </li>
      <li class="field-body">
        <b>head_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
      </li>
      <li class="field-body">
        <b>head_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>word_embed</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>word embedding matrix</p>
      </li>
      <li class="field-body">
        <b>rnn</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Stack of RNNs</p>
      </li>
      <li class="field-body">
        <b>rnn_mlp</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>Stack of dense layers on top of the RNN. This will only exists if
<code>head_layers_dim</code> is not <code>None</code></p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">StackedAttentiveRNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">StackedAttentiveRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_text</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/text/stacked_attentive_rnn.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_matrix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embed_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">attn_concatenate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">with_addnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">StackedAttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">embed_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">embed_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;the input embedding dimension </span><span class="si">{}</span><span class="s2"> and the dimension of the &quot;</span>
            <span class="s2">&quot;pretrained embeddings </span><span class="si">{}</span><span class="s2"> do not match. The pretrained embeddings &quot;</span>
            <span class="s2">&quot;dimension (</span><span class="si">{}</span><span class="s2">) will be used&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embed_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">),</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="s2">&quot;gru&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;rnn_type&#39; must be &#39;lstm&#39; or &#39;gru&#39;, got </span><span class="si">{</span><span class="n">rnn_type</span><span class="si">}</span><span class="s2"> instead&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_trainable</span> <span class="o">=</span> <span class="n">embed_trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_concatenate</span> <span class="o">=</span> <span class="n">attn_concatenate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_addnorm</span> <span class="o">=</span> <span class="n">with_addnorm</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

    <span class="c1"># Embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_embeddings</span><span class="p">(</span><span class="n">embed_matrix</span><span class="p">)</span>

    <span class="c1"># Linear Projection: if embed_dim is different that the input of the</span>
    <span class="c1"># attention blocks we add a linear projection</span>
    <span class="k">if</span> <span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="k">elif</span> <span class="n">bidirectional</span> <span class="ow">or</span> <span class="n">attn_concatenate</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_proj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="c1"># RNN</span>
    <span class="n">rnn_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">,</span>
        <span class="s2">&quot;batch_first&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="o">**</span><span class="n">rnn_params</span><span class="p">)</span>

    <span class="c1"># FC-Head (Mlp)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ContextAttentionEncoder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">,</span>
                <span class="n">attn_dropout</span><span class="p">,</span>
                <span class="n">attn_concatenate</span><span class="p">,</span>
                <span class="n">with_addnorm</span><span class="o">=</span><span class="n">with_addnorm</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">sum_along_seq</span><span class="o">=</span><span class="n">i</span> <span class="o">==</span> <span class="n">n_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Mlp</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MLP</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">]</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># simple hack to add readability in the forward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.text.stacked_attentive_rnn.StackedAttentiveRNN.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>


<a href="#pytorch_widedeep.models.text.stacked_attentive_rnn.StackedAttentiveRNN.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/text/stacked_attentive_rnn.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_output_dim</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.text.stacked_attentive_rnn.StackedAttentiveRNN.attention_weights" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">attention_weights</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.text.stacked_attentive_rnn.StackedAttentiveRNN.attention_weights" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">attention_weights</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>List with the attention weights per block</p>
<p>The shape of the attention weights is <span class="arithmatex">\((N, S)\)</span> Where <span class="arithmatex">\(N\)</span> is the batch
size and <span class="arithmatex">\(S\)</span> is the length of the sequence</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/text/stacked_attentive_rnn.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;List with the attention weights per block</span>

<span class="sd">    The shape of the attention weights is $(N, S)$ Where $N$ is the batch</span>
<span class="sd">    size and $S$ is the length of the sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">blk</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_weights</span> <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_blks</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.image.vision.Vision" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">Vision</span>


<a href="#pytorch_widedeep.models.image.vision.Vision" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">Vision</span><span class="p">(</span>
    <span class="n">pretrained_model_setup</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">channel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">kernel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Defines a standard image classifier/regressor using a pretrained
network or a sequence of convolution layers that can be used as the
<code>deepimage</code> component of a Wide &amp; Deep model or independently by
itself.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: this class represents the integration
 between <code>pytorch-widedeep</code> and <code>torchvision</code>. New architectures will be
 available as they are added to <code>torchvision</code>. In a distant future we aim
 to bring transformer-based architectures as well. However, simple
 CNN-based architectures (and even MLP-based) seem to produce SoTA
 results. For the time being, we describe below the options available
 through this class</p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>pretrained_model_setup</b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[str, <span title="pytorch_widedeep.wdtypes.Dict">Dict</span>[str, <span title="pytorch_widedeep.wdtypes.Union">Union</span>[str, <span title="pytorch_widedeep.wdtypes.WeightsEnum">WeightsEnum</span>]]]</code>)
        – <p>Name of the pretrained model. Should be a variant of the following
architectures: <em>'resnet'</em>, <em>'shufflenet'</em>, <em>'resnext'</em>,
<em>'wide_resnet'</em>, <em>'regnet'</em>, <em>'densenet'</em>, <em>'mobilenetv3'</em>,
<em>'mobilenetv2'</em>, <em>'mnasnet'</em>, <em>'efficientnet'</em> and <em>'squeezenet'</em>. if
<code>pretrained_model_setup = None</code> a basic, fully trainable CNN will be
used. Alternatively, since Torchvision 0.13 one can use pretrained
models with different weigths. Therefore, <code>pretrained_model_setup</code> can
also be dictionary with the name of the model and the weights (e.g.
<code>{'resnet50': ResNet50_Weights.DEFAULT}</code> or
<code>{'resnet50': "IMAGENET1K_V2"}</code>). <br/> Aliased as <code>pretrained_model_name</code>.</p>
      </li>
      <li class="field-body">
        <b>n_trainable</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[int]</code>)
        – <p>Number of trainable layers starting from the layer closer to the
output neuron(s). Note that this number DOES NOT take into account
the so-called <em>'head'</em> which is ALWAYS trainable. If
<code>trainable_params</code> is not None this parameter will be ignored</p>
      </li>
      <li class="field-body">
        <b>trainable_params</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[str]]</code>)
        – <p>List of strings containing the names (or substring within the name) of
the parameters that will be trained. For example, if we use a
<em>'resnet18'</em> pretrained model and we set <code>trainable_params =
['layer4']</code> only the parameters of <em>'layer4'</em> of the network
(and the head, as mentioned before) will be trained. Note that
setting this or the previous parameter involves some knowledge of
the architecture used.</p>
      </li>
      <li class="field-body">
        <b>channel_sizes</b>
            (<code><span title="pytorch_widedeep.wdtypes.List">List</span>[int]</code>)
        – <p>List of integers with the channel sizes of a CNN in case we choose not
to use a pretrained model</p>
      </li>
      <li class="field-body">
        <b>kernel_sizes</b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>List of integers with the kernel sizes of a CNN in case we choose not
to use a pretrained model. Must be of length equal to <code>len(channel_sizes) - 1</code>.</p>
      </li>
      <li class="field-body">
        <b>strides</b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[int, <span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>List of integers with the stride sizes of a CNN in case we choose not
to use a pretrained model. Must be of length equal to <code>len(channel_sizes) - 1</code>.</p>
      </li>
      <li class="field-body">
        <b>head_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>List with the number of neurons per dense layer in the head. e.g: <em>[64,32]</em></p>
      </li>
      <li class="field-body">
        <b>head_activation</b>
            (<code>str</code>)
        – <p>Activation function for the dense layers in the head. Currently
<em>'tanh'</em>, <em>'relu'</em>, <em>'leaky_relu'</em> and <em>'gelu'</em> are supported</p>
      </li>
      <li class="field-body">
        <b>head_dropout</b>
            (<code><span title="pytorch_widedeep.wdtypes.Union">Union</span>[float, <span title="pytorch_widedeep.wdtypes.List">List</span>[float]]</code>)
        – <p>float indicating the dropout between the dense layers.</p>
      </li>
      <li class="field-body">
        <b>head_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the dense layers</p>
      </li>
      <li class="field-body">
        <b>head_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not batch normalization will be applied
to the last of the dense layers</p>
      </li>
      <li class="field-body">
        <b>head_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
  </ul>

  <p>Attributes:</p>
  <ul>
      <li class="field-body">
        <b>features</b>
            (<code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code>)
        – <p>The pretrained model or Standard CNN plus the optional head</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Vision</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Vision</span><span class="p">(</span><span class="n">channel_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">kernel_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">head_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_img</span><span class="p">)</span>
</code></pre></div>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/image/vision.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@Alias</span><span class="p">(</span><span class="s2">&quot;pretrained_model_setup&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;pretrained_model_name&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_setup</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">WeightsEnum</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_trainable</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">channel_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">kernel_sizes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">strides</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Vision</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_check_pretrained_model_setup</span><span class="p">(</span>
        <span class="n">pretrained_model_setup</span><span class="p">,</span> <span class="n">n_trainable</span><span class="p">,</span> <span class="n">trainable_params</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_model_setup</span> <span class="o">=</span> <span class="n">pretrained_model_setup</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_trainable</span> <span class="o">=</span> <span class="n">n_trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">trainable_params</span> <span class="o">=</span> <span class="n">trainable_params</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">channel_sizes</span> <span class="o">=</span> <span class="n">channel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_sizes</span> <span class="o">=</span> <span class="n">kernel_sizes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">strides</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="n">head_hidden_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span> <span class="o">=</span> <span class="n">head_activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span> <span class="o">=</span> <span class="n">head_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span> <span class="o">=</span> <span class="n">head_batchnorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span> <span class="o">=</span> <span class="n">head_batchnorm_last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span> <span class="o">=</span> <span class="n">head_linear_first</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_features</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">pretrained_model_setup</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">head_hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vision_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_activation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_dropout</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">









<div class="doc doc-object doc-function">



<h3 id="pytorch_widedeep.models.image.vision.Vision.output_dim" class="doc doc-heading">
        <span class="doc doc-object-name doc-function-name">output_dim</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pytorch_widedeep.models.image.vision.Vision.output_dim" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">output_dim</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>The output dimension of the model. This is a required property
neccesary to build the <code>WideDeep</code> class</p>

      <details class="quote">
        <summary>Source code in <code>pytorch_widedeep/models/image/vision.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">output_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The output dimension of the model. This is a required property</span>
<span class="sd">    neccesary to build the `WideDeep` class</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone_output_dim</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.wide_deep.WideDeep" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">WideDeep</span>


<a href="#pytorch_widedeep.models.wide_deep.WideDeep" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">WideDeep</span><span class="p">(</span>
    <span class="n">wide</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptabular</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptext</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deepimage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deephead</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">enforce_positive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">enforce_positive_activation</span><span class="o">=</span><span class="s2">&quot;softplus&quot;</span><span class="p">,</span>
    <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">with_fds</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">fds_config</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>

  
      <p>Main collector class that combines all <code>wide</code>, <code>deeptabular</code>
<code>deeptext</code> and <code>deepimage</code> models.</p>
<p>Note that all models described so far in this library must be passed to
the <code>WideDeep</code> class once constructed. This is because the models output
the last layer before the prediction layer. Such prediction layer is
added by the <code>WideDeep</code> class as it collects the components for every
data mode.</p>
<p>There are two options to combine these models that correspond to the
two main architectures that <code>pytorch-widedeep</code> can build.</p>
<ul>
<li>
<p>Directly connecting the output of the model components to an ouput neuron(s).</p>
</li>
<li>
<p>Adding a <code>Fully-Connected Head</code> (FC-Head) on top of the deep models.
  This FC-Head will combine the output form the <code>deeptabular</code>, <code>deeptext</code> and
  <code>deepimage</code> and will be then connected to the output neuron(s).</p>
</li>
</ul>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>wide</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span>]</code>)
        – <p><code>Wide</code> model. This is a linear model where the non-linearities are
captured via crossed-columns.</p>
      </li>
      <li class="field-body">
        <b>deeptabular</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span>]</code>)
        – <p>Currently this library implements a number of possible architectures
for the <code>deeptabular</code> component. See the documenation of the
package.</p>
      </li>
      <li class="field-body">
        <b>deeptext</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span>]</code>)
        – <p>Currently this library implements a number of possible architectures
for the <code>deeptext</code> component. See the documenation of the
package.</p>
      </li>
      <li class="field-body">
        <b>deepimage</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span>]</code>)
        – <p>Currently this library uses <code>torchvision</code> and implements a number of
possible architectures for the <code>deepimage</code> component. See the
documenation of the package.</p>
      </li>
      <li class="field-body">
        <b>head_hidden_dims</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="pytorch_widedeep.wdtypes.List">List</span>[int]]</code>)
        – <p>List with the sizes of the dense layers in the head e.g: [128, 64]</p>
      </li>
      <li class="field-body">
        <b>head_activation</b>
            (<code>str</code>)
        – <p>Activation function for the dense layers in the head. Currently
<code>'tanh'</code>, <code>'relu'</code>, <code>'leaky_relu'</code> and <code>'gelu'</code> are supported</p>
      </li>
      <li class="field-body">
        <b>head_dropout</b>
            (<code>float</code>)
        – <p>Dropout of the dense layers in the head</p>
      </li>
      <li class="field-body">
        <b>head_batchnorm</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to include batch normalization in
the dense layers that form the <code>'rnn_mlp'</code></p>
      </li>
      <li class="field-body">
        <b>head_batchnorm_last</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether or not to apply batch normalization to the
last of the dense layers in the head</p>
      </li>
      <li class="field-body">
        <b>head_linear_first</b>
            (<code>bool</code>)
        – <p>Boolean indicating whether the order of the operations in the dense
layer. If <code>True: [LIN -&gt; ACT -&gt; BN -&gt; DP]</code>. If <code>False: [BN -&gt; DP -&gt;
LIN -&gt; ACT]</code></p>
      </li>
      <li class="field-body">
        <b>deephead</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span>]</code>)
        – <p>Alternatively, the user can pass a custom model that will receive the
output of the deep component. If <code>deephead</code> is not None all the
previous fc-head parameters will be ignored</p>
      </li>
      <li class="field-body">
        <b>enforce_positive</b>
            (<code>bool</code>)
        – <p>Boolean indicating if the output from the final layer must be
positive. This is important if you are using loss functions with
non-negative input restrictions, e.g. RMSLE, or if you know your
predictions are bounded in between 0 and inf</p>
      </li>
      <li class="field-body">
        <b>enforce_positive_activation</b>
            (<code>str</code>)
        – <p>Activation function to enforce that the final layer has a positive
output. <code>'softplus'</code> or <code>'relu'</code> are supported.</p>
      </li>
      <li class="field-body">
        <b>pred_dim</b>
            (<code>int</code>)
        – <p>Size of the final wide and deep output layer containing the
predictions. <code>1</code> for regression and binary classification or number
of classes for multiclass classification.</p>
      </li>
      <li class="field-body">
        <b>with_fds</b>
            (<code>bool</code>)
        – <p>Boolean indicating if Feature Distribution Smoothing (FDS) will be
applied before the final prediction layer. Only available for
regression problems.
See <a href="https://arxiv.org/abs/2102.09554">Delving into Deep Imbalanced Regression</a> for details.</p>
      </li>
  </ul>

  <p>Other Parameters:</p>
  <ul>
      <li class="field-body">
        <b>**fds_config</b>
        – <p>Dictionary with the parameters to be used when using Feature
Distribution Smoothing. Please, see the docs for the <code>FDSLayer</code>.
<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: Feature Distribution Smoothing
is available when using <strong>ONLY</strong> a <code>deeptabular</code> component
<br/>
<img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: We consider this feature absolutely
experimental and we recommend the user to not use it unless the
corresponding <a href="https://arxiv.org/abs/2102.09554">publication</a> is
well understood</p>
      </li>
  </ul>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">TabResnet</span><span class="p">,</span> <span class="n">Vision</span><span class="p">,</span> <span class="n">BasicRNN</span><span class="p">,</span> <span class="n">Wide</span><span class="p">,</span> <span class="n">WideDeep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embed_input</span> <span class="o">=</span> <span class="p">[(</span><span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">][:</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">column_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deeptabular</span> <span class="o">=</span> <span class="n">TabResnet</span><span class="p">(</span><span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">column_idx</span><span class="o">=</span><span class="n">column_idx</span><span class="p">,</span> <span class="n">cat_embed_input</span><span class="o">=</span><span class="n">embed_input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deeptext</span> <span class="o">=</span> <span class="n">BasicRNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deepimage</span> <span class="o">=</span> <span class="n">Vision</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">WideDeep</span><span class="p">(</span><span class="n">wide</span><span class="o">=</span><span class="n">wide</span><span class="p">,</span> <span class="n">deeptabular</span><span class="o">=</span><span class="n">deeptabular</span><span class="p">,</span> <span class="n">deeptext</span><span class="o">=</span><span class="n">deeptext</span><span class="p">,</span> <span class="n">deepimage</span><span class="o">=</span><span class="n">deepimage</span><span class="p">)</span>
</code></pre></div>
    <p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: It is possible to use custom components to
 build Wide &amp; Deep models. Simply, build them and pass them as the
 corresponding parameters. Note that the custom models MUST return a last
 layer of activations(i.e. not the final prediction) so that  these
 activations are collected by <code>WideDeep</code> and combined accordingly. In
 addition, the models MUST also contain an attribute <code>output_dim</code> with
 the size of these last layers of activations. See for example
 <code>pytorch_widedeep.models.tab_mlp.TabMlp</code></p>


          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/wide_deep.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">wide</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptabular</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deeptext</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deepimage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deephead</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_hidden_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="n">head_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_batchnorm_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">head_linear_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">enforce_positive</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">enforce_positive_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;softplus&quot;</span><span class="p">,</span>
    <span class="n">pred_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">with_fds</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">fds_config</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">WideDeep</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span>
        <span class="n">wide</span><span class="p">,</span>
        <span class="n">deeptabular</span><span class="p">,</span>
        <span class="n">deeptext</span><span class="p">,</span>
        <span class="n">deepimage</span><span class="p">,</span>
        <span class="n">deephead</span><span class="p">,</span>
        <span class="n">head_hidden_dims</span><span class="p">,</span>
        <span class="n">pred_dim</span><span class="p">,</span>
        <span class="n">with_fds</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># this attribute will be eventually over-written by the Trainer&#39;s</span>
    <span class="c1"># device. Acts here as a &#39;placeholder&#39;.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wd_device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># required as attribute just in case we pass a deephead</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_dim</span> <span class="o">=</span> <span class="n">pred_dim</span>

    <span class="c1"># The main 5 components of the wide and deep assemble</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wide</span> <span class="o">=</span> <span class="n">wide</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span> <span class="o">=</span> <span class="n">deeptabular</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">deeptext</span> <span class="o">=</span> <span class="n">deeptext</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">deepimage</span> <span class="o">=</span> <span class="n">deepimage</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="o">=</span> <span class="n">deephead</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">enforce_positive</span> <span class="o">=</span> <span class="n">enforce_positive</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">with_fds</span> <span class="o">=</span> <span class="n">with_fds</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span> <span class="o">=</span> <span class="n">deeptabular</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;TabNet&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tabnet</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">head_hidden_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_deephead</span><span class="p">(</span>
            <span class="n">head_hidden_dims</span><span class="p">,</span>
            <span class="n">head_activation</span><span class="p">,</span>
            <span class="n">head_dropout</span><span class="p">,</span>
            <span class="n">head_batchnorm</span><span class="p">,</span>
            <span class="n">head_batchnorm_last</span><span class="p">,</span>
            <span class="n">head_linear_first</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">deephead</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_add_pred_layer</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_fds</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fds_layer</span> <span class="o">=</span> <span class="n">FDSLayer</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">deeptabular</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">fds_config</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enforce_positive</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enf_pos</span> <span class="o">=</span> <span class="n">get_activation_fn</span><span class="p">(</span><span class="n">enforce_positive_activation</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_widedeep.models.fds_layer.FDSLayer" class="doc doc-heading">
        <span class="doc doc-object-name doc-class-name">FDSLayer</span>


<a href="#pytorch_widedeep.models.fds_layer.FDSLayer" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">FDSLayer</span><span class="p">(</span>
    <span class="n">feature_dim</span><span class="p">,</span>
    <span class="n">granularity</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">y_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">y_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">start_update</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">start_smooth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="n">ks</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">clip_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents first">
      <p class="doc doc-class-bases">
        Bases: <code><span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>


  
      <p>Feature Distribution Smoothing layer. Please, see
<a href="https:/arxiv.org/abs/2102.09554">Delving into Deep Imbalanced Regression</a>
for details.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: this is NOT an available model per se,
 but more a utility that can be used as we run a <code>WideDeep</code> model.
 The parameters of this extra layers can be set as the class
 <code>WideDeep</code> is instantiated via the keyword arguments <code>fds_config</code>.</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: Feature Distribution Smoothing is
 available when using ONLY a <code>deeptabular</code> component</p>
<p><img alt="ℹ️" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/2139.png" title=":information_source:" /> <strong>NOTE</strong>: We consider this feature absolutely
experimental and we recommend the user to not use it unless the
corresponding <a href="https://arxiv.org/abs/2102.09554">publication</a> is
well understood</p>
<p>The code here is based on the code at the
<a href="https://github.com/YyzHarry/imbalanced-regression">official repo</a></p>

  <p>Parameters:</p>
  <ul>
      <li class="field-body">
        <b>feature_dim</b>
            (<code>int</code>)
        – <p>input dimension size, i.e. output size of previous layer. This
will be the dimension of the output from the <code>deeptabular</code>
component</p>
      </li>
      <li class="field-body">
        <b>granularity</b>
            (<code>int</code>)
        – <p>number of bins that the target <span class="arithmatex">\(y\)</span> is divided into and that will
be used to compute the features' statistics (mean and variance)</p>
      </li>
      <li class="field-body">
        <b>y_max</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>)
        – <p><span class="arithmatex">\(y\)</span> upper limit to be considered when binning</p>
      </li>
      <li class="field-body">
        <b>y_min</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>)
        – <p><span class="arithmatex">\(y\)</span> lower limit to be considered when binning</p>
      </li>
      <li class="field-body">
        <b>start_update</b>
            (<code>int</code>)
        – <p>number of _'waiting epochs' after which the FDS layer will start
to update its statistics</p>
      </li>
      <li class="field-body">
        <b>start_smooth</b>
            (<code>int</code>)
        – <p>number of _'waiting epochs' after which the FDS layer will start
smoothing the feature distributions</p>
      </li>
      <li class="field-body">
        <b>kernel</b>
            (<code><span title="pytorch_widedeep.wdtypes.Literal">Literal</span>[gaussian, triang, laplace]</code>)
        – <p>choice of smoothing kernel</p>
      </li>
      <li class="field-body">
        <b>ks</b>
            (<code>int</code>)
        – <p>kernel window size</p>
      </li>
      <li class="field-body">
        <b>sigma</b>
            (<code>float</code>)
        – <p>if a <em>'gaussian'</em> or <em>'laplace'</em> kernels are used, this is the
corresponding standard deviation</p>
      </li>
      <li class="field-body">
        <b>momentum</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>)
        – <p>to train the layer the authors used a momentum update of the running
statistics across each epoch. Set to 0.9 in the paper.</p>
      </li>
      <li class="field-body">
        <b>clip_min</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>)
        – <p>this parameter is used to clip the ratio between the so called
running variance and the smoothed variance, and is introduced for
numerical stability. We leave it as optional as we did not find a
notable improvement in our experiments. The authors used a value
of 0.1</p>
      </li>
      <li class="field-body">
        <b>clip_max</b>
            (<code><span title="pytorch_widedeep.wdtypes.Optional">Optional</span>[float]</code>)
        – <p>same as <code>clip_min</code> but for the upper limit.We leave it as optional
as we did not find a notable improvement in our experiments. The
authors used a value of 10.</p>
      </li>
  </ul>

          <details class="quote">
            <summary>Source code in <code>pytorch_widedeep/models/fds_layer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">granularity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">y_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">y_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">start_update</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">start_smooth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s2">&quot;triang&quot;</span><span class="p">,</span> <span class="s2">&quot;laplace&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="n">ks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">momentum</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
    <span class="n">clip_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clip_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Feature Distribution Smoothing layer. Please, see</span>
<span class="sd">    [Delving into Deep Imbalanced Regression](https:/arxiv.org/abs/2102.09554)</span>
<span class="sd">    for details.</span>

<span class="sd">    :information_source: **NOTE**: this is NOT an available model per se,</span>
<span class="sd">     but more a utility that can be used as we run a `WideDeep` model.</span>
<span class="sd">     The parameters of this extra layers can be set as the class</span>
<span class="sd">     `WideDeep` is instantiated via the keyword arguments `fds_config`.</span>

<span class="sd">    :information_source: **NOTE**: Feature Distribution Smoothing is</span>
<span class="sd">     available when using ONLY a `deeptabular` component</span>

<span class="sd">    :information_source: **NOTE**: We consider this feature absolutely</span>
<span class="sd">    experimental and we recommend the user to not use it unless the</span>
<span class="sd">    corresponding [publication](https://arxiv.org/abs/2102.09554) is</span>
<span class="sd">    well understood</span>

<span class="sd">    The code here is based on the code at the</span>
<span class="sd">    [official repo](https://github.com/YyzHarry/imbalanced-regression)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    feature_dim: int,</span>
<span class="sd">        input dimension size, i.e. output size of previous layer. This</span>
<span class="sd">        will be the dimension of the output from the `deeptabular`</span>
<span class="sd">        component</span>
<span class="sd">    granularity: int = 100,</span>
<span class="sd">        number of bins that the target $y$ is divided into and that will</span>
<span class="sd">        be used to compute the features&#39; statistics (mean and variance)</span>
<span class="sd">    y_max: Optional[float] = None,</span>
<span class="sd">        $y$ upper limit to be considered when binning</span>
<span class="sd">    y_min: Optional[float] = None,</span>
<span class="sd">        $y$ lower limit to be considered when binning</span>
<span class="sd">    start_update: int = 0,</span>
<span class="sd">        number of _&#39;waiting epochs&#39; after which the FDS layer will start</span>
<span class="sd">        to update its statistics</span>
<span class="sd">    start_smooth: int = 1,</span>
<span class="sd">        number of _&#39;waiting epochs&#39; after which the FDS layer will start</span>
<span class="sd">        smoothing the feature distributions</span>
<span class="sd">    kernel: Literal[&quot;gaussian&quot;, &quot;triang&quot;, &quot;laplace&quot;, None] = &quot;gaussian&quot;,</span>
<span class="sd">        choice of smoothing kernel</span>
<span class="sd">    ks: int = 5,</span>
<span class="sd">        kernel window size</span>
<span class="sd">    sigma: Union[int, float] = 2,</span>
<span class="sd">        if a _&#39;gaussian&#39;_ or _&#39;laplace&#39;_ kernels are used, this is the</span>
<span class="sd">        corresponding standard deviation</span>
<span class="sd">    momentum: float = 0.9,</span>
<span class="sd">        to train the layer the authors used a momentum update of the running</span>
<span class="sd">        statistics across each epoch. Set to 0.9 in the paper.</span>
<span class="sd">    clip_min: Optional[float] = None,</span>
<span class="sd">        this parameter is used to clip the ratio between the so called</span>
<span class="sd">        running variance and the smoothed variance, and is introduced for</span>
<span class="sd">        numerical stability. We leave it as optional as we did not find a</span>
<span class="sd">        notable improvement in our experiments. The authors used a value</span>
<span class="sd">        of 0.1</span>
<span class="sd">    clip_max: Optional[float] = None,</span>
<span class="sd">        same as `clip_min` but for the upper limit.We leave it as optional</span>
<span class="sd">        as we did not find a notable improvement in our experiments. The</span>
<span class="sd">        authors used a value of 10.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FDSLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">start_update</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">start_smooth</span>
    <span class="p">),</span> <span class="s2">&quot;initial update must start at least 2 epoch before smoothing&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">feature_dim</span> <span class="o">=</span> <span class="n">feature_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">granularity</span> <span class="o">=</span> <span class="n">granularity</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">y_max</span> <span class="o">=</span> <span class="n">y_max</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">y_min</span> <span class="o">=</span> <span class="n">y_min</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_window</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">get_kernel_window</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">ks</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">half_ks</span> <span class="o">=</span> <span class="p">(</span><span class="n">ks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">start_update</span> <span class="o">=</span> <span class="n">start_update</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">start_smooth</span> <span class="o">=</span> <span class="n">start_smooth</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clip_min</span> <span class="o">=</span> <span class="n">clip_min</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clip_max</span> <span class="o">=</span> <span class="n">clip_max</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pred_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_register_buffers</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>




              
            </article>
            
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="preprocessing.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: Preprocessing" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Preprocessing
            </div>
          </div>
        </a>
      
      
        
        <a href="bayesian_models.html" class="md-footer__link md-footer__link--next" aria-label="Next: Bayesian models" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Bayesian models
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Javier Zaurin and Pavol Mulinka
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://jrzaurin.medium.com/" target="_blank" rel="noopener" title="jrzaurin.medium.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.expand", "toc.integrate"], "search": "../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.9c69f0bc.min.js"></script>
      
        <script src="../stylesheets/extra.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>