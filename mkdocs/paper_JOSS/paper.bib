@inproceedings{cheng2016wide,
  title={Wide \& deep learning for recommender systems},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st workshop on deep learning for recommender systems},
  pages={7--10},
  year={2016}
}

@inproceedings{arik2021tabnet,
  title={Tabnet: Attentive interpretable tabular learning},
  author={Arik, Sercan {\"O} and Pfister, Tomas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={8},
  pages={6679--6687},
  year={2021}
}

@inproceedings{yang2016hierarchical,
  title={Hierarchical attention networks for document classification},
  author={Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  booktitle={Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies},
  pages={1480--1489},
  year={2016}
}

@article{huang2020tabtransformer,
  title={Tabtransformer: Tabular data modeling using contextual embeddings},
  author={Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
  journal={arXiv preprint arXiv:2012.06678},
  year={2020}
}

@article{somepalli2021saint,
  title={Saint: Improved neural networks for tabular data via row attention and contrastive pre-training},
  author={Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C Bayan and Goldstein, Tom},
  journal={arXiv preprint arXiv:2106.01342},
  year={2021}
}

@article{gorishniy2021revisiting,
  title={Revisiting deep learning models for tabular data},
  author={Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18932--18943},
  year={2021}
}

@article{kim2020fastformers,
  title={Fastformers: Highly efficient transformer models for natural language understanding},
  author={Kim, Young Jin and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2010.13382},
  year={2020}
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General perception with iterative attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle={International conference on machine learning},
  pages={4651--4664},
  year={2021},
  organization={PMLR}
}

@inproceedings{blundell2015weight,
  title={Weight uncertainty in neural network},
  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  booktitle={International conference on machine learning},
  pages={1613--1622},
  year={2015},
  organization={PMLR}
}

@inproceedings{yang2021delving,
  title={Delving into deep imbalanced regression},
  author={Yang, Yuzhe and Zha, Kaiwen and Chen, Yingcong and Wang, Hao and Katabi, Dina},
  booktitle={International Conference on Machine Learning},
  pages={11842--11851},
  year={2021},
  organization={PMLR}
}

@article{wang2019deep,
  title={A deep probabilistic model for customer lifetime value prediction},
  author={Wang, Xiaojing and Liu, Tianqi and Miao, Jingang},
  journal={arXiv preprint arXiv:1912.07753},
  year={2019}
}

@article{gorishniy2022embeddings,
  title={On Embeddings for Numerical Features in Tabular Deep Learning},
  author={Gorishniy, Yura and Rubachev, Ivan and Babenko, Artem},
  journal={arXiv preprint arXiv:2203.05556},
  year={2022}
}

@manual{pytorch_widedeep_examples,
  title={pytorch-widedeep examples},
  note="[Online]. Available at: https://github.com/jrzaurin/pytorch-widedeep/tree/master/examples"
}

@manual{torchvision_models,
  title={torchvision models},
  note="[Online]. Available at: https://pytorch.org/vision/stable/models.html"
}

@manual{torchvision_weight,
  title={torchvision multi weight support},
  note="[Online]. Available at: https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/"
}

@manual{torch_sample,
  title={torch sample},
  note="[Online]. Available at: https://github.com/ncullen93/torchsample"
}

@manual{keras,
  title={keras},
  note="[Online]. Available at: https://keras.io/"
}

@manual{fastai_tokenizer,
  title={fastai tokenizer},
  note="[Online]. Available at: https://docs.fast.ai/text.transform.html#BaseTokenizer.tokenizer"
}

@manual{dl4cv,
  title={dl4cv},
  note="[Online]. Available at: https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/"
}

@manual{pytorch_widedeep_slack,
  title={pytorch-widedeep slack},
  note="[Online]. Available at: https://join.slack.com/t/pytorch-widedeep/shared_invite/zt-soss7stf-iXpVuLeKZz8lGTnxxtHtTw"
}

@misc{resnet,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015}, 
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{shufflenet,
  doi = {10.48550/ARXIV.1807.11164},
  url = {https://arxiv.org/abs/1807.11164},
  author = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{resnext,
  doi = {10.48550/ARXIV.1611.05431},
  url = {https://arxiv.org/abs/1611.05431},
  author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Aggregated Residual Transformations for Deep Neural Networks},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{wide_resnet,
  doi = {10.48550/ARXIV.1605.07146},
  url = {https://arxiv.org/abs/1605.07146},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Wide Residual Networks},
  publisher = {arXiv},
  year = {2016}, 
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{regnet,
  doi = {10.48550/ARXIV.2003.13678},
  url = {https://arxiv.org/abs/2003.13678},
  author = {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Designing Network Design Spaces},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{densenet,
  doi = {10.48550/ARXIV.1608.06993},
  url = {https://arxiv.org/abs/1608.06993},
  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Densely Connected Convolutional Networks},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{mobilenetv2,
  doi = {10.48550/ARXIV.1801.04381},
  url = {https://arxiv.org/abs/1801.04381},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  publisher = {arXiv},
  year = {2018},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{mobilenetv3,
  doi = {10.48550/ARXIV.1905.02244},
  url = {https://arxiv.org/abs/1905.02244},
  author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Searching for MobileNetV3},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{mnasnet,
  doi = {10.48550/ARXIV.1807.11626},
  url = {https://arxiv.org/abs/1807.11626},
  author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MnasNet: Platform-Aware Neural Architecture Search for Mobile},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{efficientnet,
  doi = {10.48550/ARXIV.1905.11946},
  url = {https://arxiv.org/abs/1905.11946},
  author = {Tan, Mingxing and Le, Quoc V.},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  publisher = {arXiv},
  year = {2019},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{efficientnetv2,
  doi = {10.48550/ARXIV.2104.00298},
  url = {https://arxiv.org/abs/2104.00298},
  author = {Tan, Mingxing and Le, Quoc V.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {EfficientNetV2: Smaller Models and Faster Training},
  publisher = {arXiv},
  year = {2021}, 
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{squeezenet,
  doi = {10.48550/ARXIV.1602.07360},
  url = {https://arxiv.org/abs/1602.07360},
  author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
