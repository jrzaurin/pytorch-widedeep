{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification with different optimizers, schedulers, etc.\n",
    "\n",
    "In this notebook we will use the Adult Census dataset. Download the data from [here](https://www.kaggle.com/wenruliu/adult-income-dataset/downloads/adult.csv/2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javier/.pyenv/versions/3.7.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/adult/adult.csv.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational_num      marital_status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital_gain  capital_loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours_per_week native_country  income_label  \n",
       "0              40  United-States             0  \n",
       "1              50  United-States             0  \n",
       "2              40  United-States             1  \n",
       "3              40  United-States             1  \n",
       "4              30  United-States             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For convenience, we'll replace '-' with '_'\n",
    "df.columns = [c.replace(\"-\", \"_\") for c in df.columns]\n",
    "#binary target\n",
    "df['income_label'] = (df[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "df.drop('income', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Have a look to notebooks one and two if you want to get a good understanding of the next few lines of code (although there is no need to use the package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_cols = ['education', 'relationship','workclass','occupation','native_country','gender']\n",
    "crossed_cols = [('education', 'occupation'), ('native_country', 'occupation')]\n",
    "cat_embed_cols = [('education',16), ('relationship',8), ('workclass',16), ('occupation',16),('native_country',16)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target_col = 'income_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET\n",
    "target = df[target_col].values\n",
    "\n",
    "# WIDE\n",
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "X_wide = wide_preprocessor.fit_transform(df)\n",
    "\n",
    "# DEEP\n",
    "tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols)\n",
    "X_tab = tab_preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1  17  23 ...  89  91 316]\n",
      " [  2  18  23 ...  89  92 317]\n",
      " [  3  18  24 ...  89  93 318]\n",
      " ...\n",
      " [  2  20  23 ...  90 103 323]\n",
      " [  2  17  23 ...  89 103 323]\n",
      " [  2  21  29 ...  90 115 324]]\n",
      "(48842, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_wide)\n",
    "print(X_wide.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.          1.         ...  1.         -0.99512893\n",
      "  -0.03408696]\n",
      " [ 2.          2.          1.         ...  1.         -0.04694151\n",
      "   0.77292975]\n",
      " [ 3.          2.          2.         ...  1.         -0.77631645\n",
      "  -0.03408696]\n",
      " ...\n",
      " [ 2.          4.          1.         ...  1.          1.41180837\n",
      "  -0.03408696]\n",
      " [ 2.          1.          1.         ...  1.         -1.21394141\n",
      "  -1.64812038]\n",
      " [ 2.          5.          7.         ...  1.          0.97418341\n",
      "  -0.03408696]]\n",
      "(48842, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_tab)\n",
    "print(X_tab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you can run a wide and deep model in just a few lines of code\n",
    "\n",
    "Let's now see how to use `WideDeep` with varying parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 Dropout and Batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TabMlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "# We can add dropout and batchnorm to the dense layers, as well as chose the order of the operations\n",
    "deeptabular = TabMlp(column_idx=tab_preprocessor.column_idx,\n",
    "                   mlp_hidden_dims=[64,32], \n",
    "                   mlp_dropout=[0.5, 0.5], \n",
    "                   mlp_batchnorm=True, \n",
    "                   mlp_linear_first = True,\n",
    "                   embed_input=tab_preprocessor.embeddings_input,\n",
    "                   continuous_cols=continuous_cols)\n",
    "model = WideDeep(wide=wide, deeptabular=deeptabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wide_linear): Embedding(797, 1, padding_idx=0)\n",
       "  )\n",
       "  (deeptabular): Sequential(\n",
       "    (0): TabMlp(\n",
       "      (cat_embed_and_cont): CatEmbeddingsAndCont(\n",
       "        (embed_layers): ModuleDict(\n",
       "          (emb_layer_education): Embedding(17, 16, padding_idx=0)\n",
       "          (emb_layer_native_country): Embedding(43, 16, padding_idx=0)\n",
       "          (emb_layer_occupation): Embedding(16, 16, padding_idx=0)\n",
       "          (emb_layer_relationship): Embedding(7, 8, padding_idx=0)\n",
       "          (emb_layer_workclass): Embedding(10, 16, padding_idx=0)\n",
       "        )\n",
       "        (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (cont_norm): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (tab_mlp): MLP(\n",
       "        (mlp): Sequential(\n",
       "          (dense_layer_0): Sequential(\n",
       "            (0): Linear(in_features=74, out_features=64, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (dense_layer_1): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use different initializers, optimizers and learning rate schedulers for each `branch` of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optimizers, LR schedulers, Initializers and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep.initializers import KaimingNormal, XavierNormal\n",
    "from pytorch_widedeep.callbacks import ModelCheckpoint, LRHistory, EarlyStopping\n",
    "from pytorch_widedeep.optim import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "wide_opt = torch.optim.Adam(model.wide.parameters(), lr=0.03)\n",
    "deep_opt = RAdam(model.deeptabular.parameters(), lr=0.01)\n",
    "# LR Schedulers\n",
    "wide_sch = torch.optim.lr_scheduler.StepLR(wide_opt, step_size=3)\n",
    "deep_sch = torch.optim.lr_scheduler.StepLR(deep_opt, step_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the component-dependent settings must be passed as dictionaries, while general settings are simply lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component-dependent settings as Dict\n",
    "optimizers = {'wide': wide_opt, 'deeptabular':deep_opt}\n",
    "schedulers = {'wide': wide_sch, 'deeptabular':deep_sch}\n",
    "initializers = {'wide': KaimingNormal, 'deeptabular':XavierNormal}\n",
    "# General settings as List\n",
    "callbacks = [LRHistory(n_epochs=10), EarlyStopping, ModelCheckpoint(filepath='model_weights/wd_out')]\n",
    "metrics = [Accuracy, Recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, \n",
    "                  objective='binary', \n",
    "                  optimizers=optimizers, \n",
    "                  lr_schedulers=schedulers,\n",
    "                  initializers=initializers,\n",
    "                  callbacks=callbacks,\n",
    "                  metrics=metrics\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 153/153 [00:03<00:00, 42.78it/s, loss=0.562, metrics={'acc': 0.7779, 'rec': 0.488}] \n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 54.81it/s, loss=0.374, metrics={'acc': 0.8363, 'rec': 0.5684}]\n",
      "epoch 2: 100%|██████████| 153/153 [00:03<00:00, 44.03it/s, loss=0.373, metrics={'acc': 0.8277, 'rec': 0.5535}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 108.54it/s, loss=0.359, metrics={'acc': 0.8361, 'rec': 0.5915}]\n",
      "epoch 3: 100%|██████████| 153/153 [00:03<00:00, 41.40it/s, loss=0.354, metrics={'acc': 0.8354, 'rec': 0.5686}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 100.84it/s, loss=0.355, metrics={'acc': 0.8378, 'rec': 0.5346}]\n",
      "epoch 4: 100%|██████████| 153/153 [00:03<00:00, 43.49it/s, loss=0.346, metrics={'acc': 0.8381, 'rec': 0.5653}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 117.29it/s, loss=0.352, metrics={'acc': 0.8388, 'rec': 0.5633}]\n",
      "epoch 5: 100%|██████████| 153/153 [00:03<00:00, 39.83it/s, loss=0.343, metrics={'acc': 0.8396, 'rec': 0.5669}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 115.86it/s, loss=0.351, metrics={'acc': 0.8388, 'rec': 0.6074}]\n",
      "epoch 6: 100%|██████████| 153/153 [00:03<00:00, 41.32it/s, loss=0.342, metrics={'acc': 0.8406, 'rec': 0.5758}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 110.53it/s, loss=0.35, metrics={'acc': 0.84, 'rec': 0.5834}]   \n",
      "epoch 7: 100%|██████████| 153/153 [00:03<00:00, 40.08it/s, loss=0.341, metrics={'acc': 0.8407, 'rec': 0.5664}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 108.04it/s, loss=0.35, metrics={'acc': 0.8399, 'rec': 0.5924}] \n",
      "epoch 8: 100%|██████████| 153/153 [00:03<00:00, 40.74it/s, loss=0.341, metrics={'acc': 0.8397, 'rec': 0.573}] \n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 103.97it/s, loss=0.35, metrics={'acc': 0.8404, 'rec': 0.5881}] \n",
      "epoch 9: 100%|██████████| 153/153 [00:03<00:00, 41.83it/s, loss=0.341, metrics={'acc': 0.8407, 'rec': 0.571}] \n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 112.66it/s, loss=0.35, metrics={'acc': 0.8398, 'rec': 0.595}]  \n",
      "epoch 10: 100%|██████████| 153/153 [00:03<00:00, 41.73it/s, loss=0.341, metrics={'acc': 0.8404, 'rec': 0.5751}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 111.89it/s, loss=0.35, metrics={'acc': 0.8389, 'rec': 0.5787}] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights after training corresponds to the those of the final epoch which might not be the best performing weights. Usethe 'ModelCheckpoint' Callback to restore the best epoch weights.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(X_wide=X_wide, X_tab=X_tab, target=target, n_epochs=10, batch_size=256, val_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that, among many methods and attributes we have the `history` and `lr_history` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [0.5623695554296955, 0.3727661143330967, 0.3543393321676192, 0.3463186333382052, 0.34326155766162997, 0.34202106482063244, 0.34081082270036334, 0.34090089836930915, 0.3412071953411975, 0.3405635129002964], 'train_acc': [0.7778517134594221, 0.8277071123282062, 0.8353594553783943, 0.8381235123998669, 0.83960791339288, 0.8405804519745093, 0.8406572313362168, 0.8396590996340184, 0.840682824456786, 0.8404268932510941], 'train_rec': [0.4879666268825531, 0.5535351634025574, 0.5686169862747192, 0.5653011202812195, 0.5669055581092834, 0.5757834911346436, 0.5663707256317139, 0.5730024576187134, 0.5709701776504517, 0.5751417279243469], 'val_loss': [0.374390076368283, 0.35924087579433733, 0.354536472986906, 0.35208039711683226, 0.35081761387678295, 0.3504261534947615, 0.350106044457509, 0.34991710613935423, 0.35027056473952073, 0.34997811913490295], 'val_acc': [0.8363189681646023, 0.8361142389190296, 0.8377520728836114, 0.8387757191114751, 0.8387757191114751, 0.8400040945849114, 0.8399017299621251, 0.8404135530760569, 0.8397993653393387, 0.8388780837342614], 'val_rec': [0.5684345364570618, 0.5915312170982361, 0.5346450209617615, 0.5633019804954529, 0.6073567271232605, 0.5834046006202698, 0.5923866629600525, 0.5881094932556152, 0.5949529409408569, 0.5786997675895691]}\n"
     ]
    }
   ],
   "source": [
    "print(trainer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr_wide_0': [0.03, 0.03, 0.03, 0.003, 0.003, 0.003, 0.00030000000000000003, 0.00030000000000000003, 0.00030000000000000003, 3.0000000000000004e-05], 'lr_deeptabular_0': [0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001]}\n"
     ]
    }
   ],
   "source": [
    "print(trainer.lr_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the learning rate effectively decreases by a factor of 0.1 (the default) after the corresponding `step_size`. Note that the keys of the dictionary have a suffix `_0`. This is because if you pass different parameter groups to the torch optimizers, these will also be recorded. We'll see this in the `Regression` notebook. \n",
    "\n",
    "And I guess one has a good idea of how to use the package. \n",
    "\n",
    "Before we leave this notebook just mentioning that the `WideDeep` class comes with a what is perhaps a useful method that I intend to deprecate in favor of `Tab2Vec`. This method, called `get_embeddings` is designed to \"rescue\" the learned embeddings. For example, let's say I want to use the embeddings learned for the different levels of the categorical feature `education`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javier/Projects/pytorch-widedeep/pytorch_widedeep/training/trainer.py:794: DeprecationWarning: 'get_embeddings' will be deprecated in the next release. Please consider using 'Tab2vec' instead\n",
      "  DeprecationWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'11th': array([-0.3475832 ,  0.34912273, -0.11974874,  0.14691196, -0.22545682,\n",
       "        -0.3613695 , -0.00136127, -0.0563265 ,  0.3466888 ,  0.11706785,\n",
       "        -0.01166581, -0.01369573, -0.17875178,  0.18713965,  0.2914308 ,\n",
       "        -0.198182  ], dtype=float32),\n",
       " 'HS-grad': array([ 0.09942148, -0.33260158,  0.2164713 , -0.2940495 ,  0.22636804,\n",
       "         0.12042803, -0.07338171,  0.17187971, -0.12905738,  0.3129245 ,\n",
       "        -0.31488863, -0.17345233,  0.32477817,  0.00439972,  0.39258945,\n",
       "        -0.14481816], dtype=float32),\n",
       " 'Assoc-acdm': array([-0.00751864, -0.1771137 ,  0.06895561, -0.21083945,  0.23953192,\n",
       "        -0.6551445 ,  0.01284237, -0.0050387 , -0.07738334,  0.00540992,\n",
       "         0.0681937 ,  0.05531053, -0.4259041 , -0.1871334 , -0.04381247,\n",
       "         0.32671115], dtype=float32),\n",
       " 'Some-college': array([ 0.01929094,  0.10994322,  0.36765632, -0.23809849,  0.10644584,\n",
       "        -0.19297272, -0.39444843,  0.32810718, -0.05060181,  0.4375799 ,\n",
       "         0.34009618, -0.30499312,  0.06079052, -0.36158556,  0.16431686,\n",
       "        -0.02064201], dtype=float32),\n",
       " '10th': array([ 0.32986915,  0.20145807, -0.46201912,  0.15131666,  0.39709982,\n",
       "         0.69238126,  0.20381889,  0.10686771, -0.00311412,  0.40032774,\n",
       "        -0.25356117,  0.05119215, -0.5510974 , -0.3487673 , -0.05308707,\n",
       "        -0.15400933], dtype=float32),\n",
       " 'Prof-school': array([-0.08333081, -0.1471433 ,  0.0884981 ,  0.7094311 , -0.22927387,\n",
       "        -0.07990997,  0.09308612,  0.13682584,  0.31950092,  0.0993206 ,\n",
       "         0.31872186, -0.05731025, -0.02362061,  0.2931348 ,  0.22886205,\n",
       "         0.07881528], dtype=float32),\n",
       " '7th-8th': array([-0.17651281,  0.14389877, -0.51749426, -0.36477914, -0.24834834,\n",
       "        -0.35161367, -0.38828874, -0.36244267,  0.16288954, -0.2656618 ,\n",
       "        -0.42065242, -0.16790003, -0.04955713, -0.4936896 , -0.07479241,\n",
       "        -0.15467522], dtype=float32),\n",
       " 'Bachelors': array([-1.7109926e-01, -1.5259677e-01,  2.1966804e-02,  2.4115700e-01,\n",
       "        -6.3872856e-01, -6.5369323e-02, -2.5777605e-01,  1.3853328e-01,\n",
       "        -1.3078525e-04,  3.5386881e-01,  3.6984026e-02,  1.3007362e-01,\n",
       "        -3.4332672e-01,  1.4918861e-01, -2.7776187e-02,  1.8584514e-02],\n",
       "       dtype=float32),\n",
       " 'Masters': array([-0.0607332 ,  0.03891989, -0.12000098, -0.26994392,  0.1360479 ,\n",
       "        -0.12739065, -0.42029268,  0.4281459 , -0.2187682 , -0.10016173,\n",
       "         0.21315324,  0.06292748, -0.17620797,  0.06142575, -0.16202934,\n",
       "         0.07813183], dtype=float32),\n",
       " 'Doctorate': array([ 0.07975567, -0.2995707 , -0.1297475 , -0.23506498, -0.07601811,\n",
       "         0.21119696, -0.4014182 ,  0.3409825 ,  0.00557449,  0.17662002,\n",
       "         0.01124496,  0.01987186,  0.00463357,  0.05345817,  0.28748044,\n",
       "        -0.24112043], dtype=float32),\n",
       " '5th-6th': array([ 0.5512265 , -0.1908678 , -0.27131537,  0.36982986,  0.26176104,\n",
       "         0.36645773, -0.23311335, -0.12837252,  0.24260557, -0.01326179,\n",
       "         0.14636081, -0.0393713 ,  0.12896451, -0.14971113,  0.01964791,\n",
       "        -0.04153565], dtype=float32),\n",
       " 'Assoc-voc': array([ 0.00294167, -0.39827642, -0.02495229,  0.13957082, -0.13182898,\n",
       "        -0.27178332,  0.14512709, -0.29180354, -0.39801288, -0.15011302,\n",
       "        -0.19905967, -0.19827461,  0.1912367 ,  0.1386391 ,  0.13930447,\n",
       "         0.05284905], dtype=float32),\n",
       " '9th': array([ 0.10768763, -0.06806335, -0.18458003,  0.07836349,  0.3678258 ,\n",
       "        -0.03671409, -0.02125892, -0.22644126,  0.24890126, -0.01134706,\n",
       "        -0.35545322, -0.26837015, -0.22845276, -0.00784048, -0.01379843,\n",
       "         0.07515417], dtype=float32),\n",
       " '12th': array([ 0.3593276 ,  0.4534212 , -0.17996144,  0.288639  ,  0.03528969,\n",
       "         0.01434682,  0.33964154, -0.19378136, -0.09871213,  0.073057  ,\n",
       "        -0.09627059, -0.1055373 , -0.24785268,  0.4939406 ,  0.11959701,\n",
       "        -0.10218817], dtype=float32),\n",
       " '1st-4th': array([ 0.1668331 ,  0.12820388, -0.19806872, -0.07793977,  0.10481353,\n",
       "        -0.2746754 ,  0.33344626,  0.09796116,  0.4184359 , -0.06698637,\n",
       "        -0.49304193, -0.44370967, -0.07711301, -0.24175553,  0.42256072,\n",
       "         0.3274595 ], dtype=float32),\n",
       " 'Preschool': array([ 0.5568573 , -0.02487507, -0.21234682, -0.19250521, -0.26240364,\n",
       "         0.11080477,  0.41791028, -0.10821233, -0.04813243, -0.19189784,\n",
       "        -0.03009004,  0.28244784,  0.44653463,  0.2691065 ,  0.4888137 ,\n",
       "        -0.01453342], dtype=float32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.get_embeddings(col_name='education', cat_encoding_dict=tab_preprocessor.label_encoder.encoding_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
